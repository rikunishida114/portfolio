{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SB3 を用いた 2 リンクアームの躍度最小化軌道の学習(https://github.com/DLR-RM/stable-baselines3)\n",
    "### この jupyternotebook は以下のような構造になっています\n",
    "1. モジュール管理\n",
    "2. パラメータ管理\n",
    "3. 環境設定\n",
    "4. 環境が正しく動くかの確認\n",
    "5. エピソードごとの報酬などの学習内部指標を定期的に保存するためのコールバック\n",
    "6. 評価タイミングごとの評価結果とその時点までに学習したエピソード数をCSV に記録するためのコールバック\n",
    "7. 学習\n",
    "8. 学習済みデータを用いたシミュレーション\n",
    "9. ベストモデルを用いたシミュレーション\n",
    "10. エピソードごとの様々な報酬の推移などの多指標をプロット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュール管理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 基本的なライブラリ =====\n",
    "import os             # ファイルパスやディレクトリの操作（例：ログ保存先の作成など）\n",
    "import json           # 設定ファイルや学習結果をJSON形式で保存・読み込むために使用\n",
    "import csv            # 結果やログをCSV形式で出力するために使用\n",
    "import time           # 実行時間の計測やスリープ処理などに利用\n",
    "import glob           # 特定パターンのファイル探索（例：\"*.csv\" など）\n",
    "from typing import Optional, List  # 型ヒントのため（関数の引数・戻り値の明示）\n",
    "from datetime import datetime      # 実行開始時刻やログのタイムスタンプ記録に利用\n",
    "import numpy as np    # 数値計算用ライブラリ（ベクトル・行列演算、乱数生成など）\n",
    "import pandas as pd   # データ処理やCSVの読み書き・集計・可視化に便利\n",
    "\n",
    "# ===== 可視化関連 =====\n",
    "import matplotlib.pyplot as plt  # 学習曲線、報酬推移、動作ログなどの可視化に使用\n",
    "\n",
    "# ===== 強化学習関連（Stable-Baselines3） =====\n",
    "from stable_baselines3 import PPO, SAC, TD3, DDPG, A2C  \n",
    "# → 代表的な強化学習アルゴリズム（方策勾配系やアクタークリティック系）をインポート\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback  \n",
    "# → 学習中に評価や早期終了などを行うためのコールバック機能を利用するため\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env  \n",
    "# → 独自実装したGym環境がSB3に適合しているかを検証するための関数\n",
    "\n",
    "from collections import deque  # 一定長の履歴（移動平均など）を保持するのに便利\n",
    "\n",
    "# ===== Gym関連 =====\n",
    "import gymnasium as gym  # 強化学習環境の作成・管理（GymnasiumはGymの後継ライブラリ）\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== シミュレーションパラメータ =====\n",
    "DT = 0.01           # シミュレーションの時間刻み (秒)\n",
    "L = 1.0             # アームの全長 (m)（ここでは正規化された長さとして 1.0）\n",
    "STEPS_MAX = 200     # 1エピソードあたりの最大ステップ数\n",
    "THRESHOLD = 0.03    # ゴール判定の許容距離 (m)\n",
    "\n",
    "# ===== アクションと状態の範囲 =====\n",
    "# ★すでに「トルク [N·m]」として使っているのでコメントもそれに合わせておく\n",
    "ACTION_MIN, ACTION_MAX = -20.0, 20.0  # アクション範囲（各関節トルクの下限・上限）[N·m]\n",
    "THETA_MIN, THETA_MAX = 0.0, 180.0     # 関節角度の下限・上限 [deg]\n",
    "THETA_INIT = [40.0, 100.0]            # 初期角度 [deg]\n",
    "GOAL_POS = [0.0, 0.9]                 # ゴール位置 (x, y) [m]\n",
    "\n",
    "# --- 物理パラメータ（ヒト上肢オーダーの近似値） ---\n",
    "# 上腕 ~2 kg, 前腕+手 ~1.5 kg 程度のオーダー\n",
    "M1 = 1.0   # 上腕リンクの質量 [kg]\n",
    "M2 = 1.0   # 前腕リンクの質量 [kg]\n",
    "\n",
    "# 関節粘性（ダンピング）。0.05〜0.2 の範囲で試した中から、\n",
    "# 学習・軌道の安定性が良かった値として 0.1 を採用。\n",
    "JOINT_DAMPING = 0.1  # [N·m·s/rad]\n",
    "\n",
    "# --- 報酬関連（終端 + shaping） ---\n",
    "REWARD_P_V = 15.0         # 終端速度ペナルティ係数\n",
    "REWARD_J = 1e-2           # 終端躍度係数（累積にかける）\n",
    "REWARD_JE_LIM = 1.0       # 比率表示用（infoに出すだけ）\n",
    "\n",
    "# --- 目標時間と時間軸の扱い（ソフト制約） ---\n",
    "T_TARGET = 0.45           # 目標所要時間 [s]\n",
    "SIGMA_T = 2.0             # 時間ボーナスの許容幅 [s]\n",
    "R_GOAL = 50.0             # 成功時の基礎ボーナス（十分大きくする）\n",
    "TIME_COST = 1e-3          # 毎ステップの時間コスト（小）\n",
    "\n",
    "# --- shaping（毎ステップの小さなガイド） ---\n",
    "SHAPING_DIST_COEFF = 10.0   # 距離改善に対するステップ報酬倍率\n",
    "SHAPING_JERK_COEFF = 1e-7   # 瞬時躍度ペナルティ係数（小さく）\n",
    "\n",
    "# --- 停滞検出（進捗が無いと早めに打ち切る） ---\n",
    "STALL_WINDOW_S = 0.30        # 停滞ウィンドウ長 [s]\n",
    "STALL_WINDOW = max(1, int(STALL_WINDOW_S / DT))  # ステップ単位\n",
    "MIN_PROGRESS_PER_WINDOW = 1e-4   # この期間の進捗がこれ未満なら停滞と判定（m）\n",
    "\n",
    "# ===== 学習設定 =====\n",
    "TOTAL_TIMESTEPS = 4_000_000   # 学習全体での総ステップ数\n",
    "LEARNING_RATE = 0.0003        # 学習率\n",
    "TAU = 0.01                    # SACを使う際のαの学習率（今はPPOなら未使用でもOK）\n",
    "HID_LAY = 64                  # 隠れ層のノード数\n",
    "BUFFER_SIZE = 100000          # リプレイバッファのサイズ\n",
    "BATCH_SIZE = 256              # ミニバッチサイズ\n",
    "EVAL_FREQ = 100               # 何ステップごとに評価するか\n",
    "SAVE_INTERVAL = 1             # 何エピソードごとにモデルを保存するか\n",
    "RUNNNIG_WINDOW = 10           # 直近何エピソードの平均を計算するか\n",
    "STATES_ARM = 4                # armに関する状態数\n",
    "SAME_ACTION_TOL = 1e-8        # 同一アクション判定の許容誤差\n",
    "\n",
    "SUCCESS_WINDOW_SIZE = 100             # sigma_T更新用のウィンドウサイズ\n",
    "SIGMA_UPDATE_INTERVAL = 10            # sigma_T更新のインターバル\n",
    "SIGMA_SHARPEN_THRESHOLD = 0.80        # sigmaを小さくする成功率しきい値\n",
    "SIGMA_BLUNT_THRESHOLD = 0.40          # sigmaを大きくする成功率しきい値\n",
    "SIGMA_SHARPEN_FACTOR = 0.90           # sigma先鋭化倍率 (<1)\n",
    "SIGMA_BLUNT_FACTOR = 1.10             # sigma鈍化倍率 (>1)\n",
    "SIGMA_MIN = 0.2                       # sigmaの下限\n",
    "SIGMA_MAX_FACTOR = 10.0               # sigmaの上限計算に使う倍率\n",
    "\n",
    "# ===== stage2（shaping フェーズアウト）関連パラメータ =====\n",
    "# 成功率と sigma_T が十分に良くなったら stage2 に入り、\n",
    "# shaping をエピソード単位でフェードアウトさせるためのしきい値\n",
    "STAGE2_SUCCESS_THRESHOLD = 0.80   # 直近成功率がこの値以上\n",
    "STAGE2_SIGMA_THRESHOLD   = 0.20   # sigma_T がこの値以下\n",
    "STAGE2_FADE_EPISODES     = 4000   # stage2 移行後、このエピソード数かけて線形にshapingを 1→0 へ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定（2リンクアームのリーチング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoJointReachingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    2関節アームのリーチング環境（ダイナミクス込み + 2段階報酬版）\n",
    "\n",
    "    - stage1:\n",
    "        距離shaping, jerk shaping, 時間コスト を使って学習をガイド\n",
    "    - stage2:\n",
    "        直近 success-rate と sigma_T を見て条件を満たしたら遷移\n",
    "        → shaping係数をエピソード単位で徐々に 0 までフェードアウト\n",
    "        → 終端の jerk / velocity / time bonus が主役の目的関数に移行\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- 基本パラメータ ----------\n",
    "        self.dt = DT\n",
    "        self.l1 = float(L) * 0.5\n",
    "        self.l2 = float(L) * 0.5\n",
    "\n",
    "        # --- 物理パラメータ（ダイナミクス用） ---\n",
    "        # ★ globals() 依存をやめて、上で定義した M1, M2, JOINT_DAMPING をそのまま使う\n",
    "        self.m1 = M1\n",
    "        self.m2 = M2\n",
    "        # COM を人の上肢データに近い 0.43L 付近に設定\n",
    "        self.lc1 = self.l1 * 0.5\n",
    "        self.lc2 = self.l2 * 0.5\n",
    "        # 慣性モーメント（細い棒の中央まわり近似）\n",
    "        self.I1 = (1.0 / 12.0) * self.m1 * (self.l1 ** 2)\n",
    "        self.I2 = (1.0 / 12.0) * self.m2 * (self.l2 ** 2)\n",
    "        self.g = 9.81\n",
    "        self.b = JOINT_DAMPING  # [N·m·s/rad]\n",
    "\n",
    "        # エピソード最大ステップ、ゴール閾値、ゴール位置\n",
    "        self.max_steps = STEPS_MAX\n",
    "        self.goal_threshold = THRESHOLD\n",
    "        self.goal_pos = np.array(GOAL_POS, dtype=np.float32)\n",
    "\n",
    "        # --- 報酬 / タスクパラメータ ---\n",
    "        self.T_target = T_TARGET\n",
    "        self.sigma_T = SIGMA_T\n",
    "        self.R_goal = R_GOAL\n",
    "\n",
    "        # ★shaping の「元の値」を保持しておく（stage2 フェード用）\n",
    "        self.base_shaping_dist_coeff = SHAPING_DIST_COEFF\n",
    "        self.base_shaping_jerk_coeff = SHAPING_JERK_COEFF\n",
    "        self.base_time_cost = TIME_COST\n",
    "\n",
    "        # 実際に step で使う係数（毎ステップ更新）\n",
    "        self.shaping_dist_coeff = float(self.base_shaping_dist_coeff)\n",
    "        self.shaping_jerk_coeff = float(self.base_shaping_jerk_coeff)\n",
    "        self.time_cost = float(self.base_time_cost)\n",
    "\n",
    "        # 終端ペナルティの係数\n",
    "        self.terminal_jerk_coeff = REWARD_J\n",
    "        self.terminal_vel_coeff = REWARD_P_V\n",
    "\n",
    "        # 停滞判定\n",
    "        self.stall_window = STALL_WINDOW\n",
    "        self.min_progress_per_window = MIN_PROGRESS_PER_WINDOW\n",
    "\n",
    "        # ---------- 観測空間 ----------\n",
    "        obs_low = np.array([-1.0] * STATES_ARM + [0.0], dtype=np.float32)\n",
    "        obs_high = np.array([1.0] * STATES_ARM + [1.0], dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "\n",
    "        # ---------- アクション空間（トルク） ----------\n",
    "        self.torque_min = ACTION_MIN\n",
    "        self.torque_max = ACTION_MAX\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([self.torque_min, self.torque_min], dtype=np.float32),\n",
    "            high=np.array([self.torque_max, self.torque_max], dtype=np.float32),\n",
    "            shape=(2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self._same_action_counter = 0\n",
    "        self._same_action_tol = SAME_ACTION_TOL\n",
    "\n",
    "        # ---------- sigma 自動更新 ----------\n",
    "        self._success_window_size = SUCCESS_WINDOW_SIZE\n",
    "        self._success_window = deque(maxlen=self._success_window_size)\n",
    "\n",
    "        self.episode_count = 0\n",
    "        self.sigma_update_interval = SIGMA_UPDATE_INTERVAL\n",
    "        self._sigma_update_counter = 0\n",
    "\n",
    "        self.sigma_sharpen_threshold = SIGMA_SHARPEN_THRESHOLD\n",
    "        self.sigma_blunt_threshold = SIGMA_BLUNT_THRESHOLD\n",
    "        self.sigma_sharpen_factor = SIGMA_SHARPEN_FACTOR\n",
    "        self.sigma_blunt_factor = SIGMA_BLUNT_FACTOR\n",
    "        self.sigma_min = SIGMA_MIN\n",
    "        self.sigma_max = max(1e-3, SIGMA_T * SIGMA_MAX_FACTOR)\n",
    "\n",
    "        self._last_sigma_update = {\n",
    "            \"episode\": None,\n",
    "            \"old_sigma\": None,\n",
    "            \"new_sigma\": None,\n",
    "            \"success_rate\": None,\n",
    "            \"action\": None,\n",
    "        }\n",
    "\n",
    "        # ---------- ★stage2（shaping フェーズアウト）関連 ----------\n",
    "        # ★ここも globals() の分岐をやめて、上で定義した値をそのまま使う\n",
    "        self.stage2_success_threshold = STAGE2_SUCCESS_THRESHOLD\n",
    "        self.stage2_sigma_threshold = STAGE2_SIGMA_THRESHOLD\n",
    "        self.stage2_fade_episodes = STAGE2_FADE_EPISODES\n",
    "\n",
    "        self.stage2_active = False          # stage2 に入ったかどうか\n",
    "        self.stage2_start_episode = None    # stage2 開始エピソード\n",
    "        self.stage2_last_success_rate = None  # 判定に使った成功率の記録\n",
    "\n",
    "        # 環境初期化\n",
    "        self.reset()\n",
    "\n",
    "    # ---------- ヘルパー ----------\n",
    "    def set_penalty_weight(self, new_weight):\n",
    "        self.terminal_jerk_coeff = float(new_weight)\n",
    "\n",
    "    def reached_goal(self, dist_to_goal):\n",
    "        return float(dist_to_goal) <= float(self.goal_threshold)\n",
    "\n",
    "    def forward_kinematics(self, thetas):\n",
    "        th1, th2 = float(thetas[0]), float(thetas[1])\n",
    "        x = self.l1 * np.cos(th1) + self.l2 * np.cos(th1 + th2)\n",
    "        y = self.l1 * np.sin(th1) + self.l2 * np.sin(th1 + th2)\n",
    "        return np.array([x, y], dtype=np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        pos_scale = (self.l1 + self.l2)\n",
    "        hand_pos_scaled = np.asarray(self.hand_pos / max(1e-8, pos_scale)).ravel()\n",
    "        hand_pos_scaled = np.clip(hand_pos_scaled, -1.0, 1.0)\n",
    "\n",
    "        # （便宜上）トルクレンジから最大角速度スケールを近似\n",
    "        max_joint_speed_rad = np.radians(max(abs(self.torque_min), abs(self.torque_max)))\n",
    "        max_hand_speed = max_joint_speed_rad * pos_scale\n",
    "        hand_vel_scaled = np.asarray(self.hand_vel / max(1e-8, max_hand_speed)).ravel()\n",
    "        hand_vel_scaled = np.clip(hand_vel_scaled, -1.0, 1.0)\n",
    "\n",
    "        time_scaled = np.array(\n",
    "            [np.clip(float(self.steps) / max(1, int(self.max_steps)), 0.0, 1.0)],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        obs = np.concatenate([\n",
    "            hand_pos_scaled.astype(np.float32),\n",
    "            hand_vel_scaled.astype(np.float32),\n",
    "            time_scaled.astype(np.float32),\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    # ---------- Gymnasium API: reset ----------\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.episode_count += 1\n",
    "\n",
    "        try:\n",
    "            th_init_arr = np.asarray(THETA_INIT)\n",
    "            if th_init_arr.size == 1:\n",
    "                th_init_arr = np.array([th_init_arr.item(), th_init_arr.item()], dtype=np.float32)\n",
    "            else:\n",
    "                th_init_arr = th_init_arr.reshape(-1)[:2].astype(np.float32)\n",
    "        except Exception:\n",
    "            th_init_arr = np.array([THETA_INIT, THETA_INIT], dtype=np.float32)\n",
    "\n",
    "        self.theta = np.radians(th_init_arr).astype(np.float32)\n",
    "        self.steps = 0\n",
    "        self.t = 0.0\n",
    "\n",
    "        self.theta_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.theta_acc = np.zeros(2, dtype=np.float32)\n",
    "        self.theta_jerk = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "        self.hand_pos = self.forward_kinematics(self.theta)\n",
    "        self.hand_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.hand_acc = np.zeros(2, dtype=np.float32)\n",
    "        self.hand_jerk = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "        self.jerk_sum = 0.0\n",
    "\n",
    "        self.prev_dist = np.linalg.norm(self.hand_pos - self.goal_pos)\n",
    "        self.dist_window = [self.prev_dist]\n",
    "\n",
    "        self._same_action_counter = 0\n",
    "\n",
    "        # ★reset のたびに、stage2 のフェードアウト係数に応じて shaping 係数を更新\n",
    "        self._update_shaping_coeffs_for_current_episode()\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    # ---------- ★stage2 用の shaping 更新ロジック ----------\n",
    "    def _get_stage2_shaping_scale(self) -> float:\n",
    "        \"\"\"\n",
    "        stage2 中であれば、エピソード数に応じて [1 → 0] へ線形に減衰させるスケールを返す。\n",
    "        stage2 でなければ 1.0 を返す。\n",
    "        \"\"\"\n",
    "        if not self.stage2_active or self.stage2_start_episode is None:\n",
    "            return 1.0\n",
    "\n",
    "        elapsed = max(0, self.episode_count - self.stage2_start_episode)\n",
    "        k = elapsed / max(1, self.stage2_fade_episodes)  # 0〜1 以上\n",
    "        scale = float(np.clip(1.0 - k, 0.0, 1.0))\n",
    "        return scale\n",
    "\n",
    "    def _update_shaping_coeffs_for_current_episode(self):\n",
    "        \"\"\"\n",
    "        現在の episode_count / stage2 状態に応じて\n",
    "        shaping_dist_coeff / shaping_jerk_coeff / time_cost を更新する。\n",
    "        \"\"\"\n",
    "        scale = self._get_stage2_shaping_scale()\n",
    "        self.shaping_dist_coeff = float(self.base_shaping_dist_coeff * scale)\n",
    "        self.shaping_jerk_coeff = float(self.base_shaping_jerk_coeff * scale)\n",
    "        self.time_cost = float(self.base_time_cost * scale)\n",
    "\n",
    "    # ---------- 2リンクアームの前進ダイナミクス ----------\n",
    "    def compute_forward_dynamics(self, theta, theta_vel, tau):\n",
    "        th1, th2 = float(theta[0]), float(theta[1])\n",
    "        dth1, dth2 = float(theta_vel[0]), float(theta_vel[1])\n",
    "\n",
    "        c2 = np.cos(th2)\n",
    "        s2 = np.sin(th2)\n",
    "\n",
    "        m1, m2 = self.m1, self.m2\n",
    "        l1, lc1, lc2 = self.l1, self.lc1, self.lc2\n",
    "        I1, I2 = self.I1, self.I2\n",
    "\n",
    "        M11 = I1 + I2 + m1 * (lc1 ** 2) + m2 * (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * c2)\n",
    "        M22 = I2 + m2 * (lc2 ** 2)\n",
    "        M12 = I2 + m2 * (lc2 ** 2 + l1 * lc2 * c2)\n",
    "        M21 = M12\n",
    "        M = np.array([[M11, M12],\n",
    "                      [M21, M22]], dtype=np.float32)\n",
    "\n",
    "        h = - m2 * l1 * lc2 * s2\n",
    "        C1 = h * dth2 * (2.0 * dth1 + dth2)\n",
    "        C2 = h * dth1 * dth1\n",
    "        C_vec = np.array([C1, C2], dtype=np.float32)\n",
    "\n",
    "        g = self.g\n",
    "        G1 = (m1 * lc1 + m2 * l1) * g * np.cos(th1) + m2 * lc2 * g * np.cos(th1 + th2)\n",
    "        G2 = m2 * lc2 * g * np.cos(th1 + th2)\n",
    "        G_vec = np.array([G1, G2], dtype=np.float32)\n",
    "\n",
    "        B_vec = self.b * np.asarray(theta_vel, dtype=np.float32)\n",
    "\n",
    "        rhs = np.asarray(tau, dtype=np.float32) - C_vec - G_vec - B_vec\n",
    "        theta_acc = np.linalg.solve(M, rhs)\n",
    "        return theta_acc.astype(np.float32)\n",
    "\n",
    "    # ---------- Sigma 更新 + ★stage2 判定 ----------\n",
    "    def update_sigma_by_success_rate(self):\n",
    "        \"\"\"\n",
    "        sigma_T 更新に加えて、\n",
    "        - success-rate\n",
    "        - sigma_T\n",
    "        を見て stage2 開始条件を満たしたら stage2_active を True にする。\n",
    "        \"\"\"\n",
    "        if len(self._success_window) < self._success_window_size:\n",
    "            return False\n",
    "\n",
    "        success_rate = float(sum(self._success_window)) / float(self._success_window_size)\n",
    "        old_sigma = float(self.sigma_T)\n",
    "        update_action = None\n",
    "\n",
    "        if success_rate >= self.sigma_sharpen_threshold:\n",
    "            new_sigma = old_sigma * float(self.sigma_sharpen_factor)\n",
    "            update_action = \"sharpen\"\n",
    "        elif success_rate <= self.sigma_blunt_threshold:\n",
    "            new_sigma = old_sigma * float(self.sigma_blunt_factor)\n",
    "            update_action = \"blunt\"\n",
    "        else:\n",
    "            new_sigma = old_sigma\n",
    "\n",
    "        new_sigma = float(np.clip(new_sigma, self.sigma_min, self.sigma_max))\n",
    "        self.sigma_T = new_sigma\n",
    "\n",
    "        self._last_sigma_update = {\n",
    "            \"episode\": int(self.episode_count),\n",
    "            \"old_sigma\": float(old_sigma),\n",
    "            \"new_sigma\": float(new_sigma),\n",
    "            \"success_rate\": float(success_rate),\n",
    "            \"action\": update_action,\n",
    "        }\n",
    "\n",
    "        # ★ここで stage2 開始判定\n",
    "        self.stage2_last_success_rate = float(success_rate)\n",
    "        if (not self.stage2_active\n",
    "            and success_rate >= self.stage2_success_threshold\n",
    "            and self.sigma_T <= self.stage2_sigma_threshold):\n",
    "            self.stage2_active = True\n",
    "            self.stage2_start_episode = int(self.episode_count)\n",
    "            # 次の reset から shaping 係数がフェードアウトを開始\n",
    "\n",
    "        return True\n",
    "\n",
    "    # ---------- Gymnasium API: step ----------\n",
    "    def step(self, action):\n",
    "        prev_theta = self.theta.copy()\n",
    "        prev_vel = self.theta_vel.copy()\n",
    "        prev_acc = self.theta_acc.copy()\n",
    "\n",
    "        tau = np.asarray(action, dtype=np.float32).reshape(-1)\n",
    "        if tau.shape[0] != 2:\n",
    "            raise ValueError(f\"action must have shape (2,), got {tau.shape}\")\n",
    "\n",
    "        tau_clipped = np.clip(tau, self.torque_min, self.torque_max)\n",
    "\n",
    "        action_same = bool(np.allclose(tau_clipped[0], tau_clipped[1], atol=self._same_action_tol))\n",
    "        if action_same:\n",
    "            self._same_action_counter += 1\n",
    "        else:\n",
    "            self._same_action_counter = 0\n",
    "\n",
    "        # --- ダイナミクス更新 ---\n",
    "        self.theta_acc = self.compute_forward_dynamics(prev_theta, prev_vel, tau_clipped)\n",
    "        self.theta_vel = prev_vel + self.theta_acc * self.dt\n",
    "        self.theta = prev_theta + self.theta_vel * self.dt\n",
    "\n",
    "        theta_min = np.radians(THETA_MIN)\n",
    "        theta_max = np.radians(THETA_MAX)\n",
    "        self.theta = np.clip(self.theta, theta_min, theta_max)\n",
    "\n",
    "        self.theta_jerk = (self.theta_acc - prev_acc) / self.dt\n",
    "\n",
    "        prev_hand_pos = self.hand_pos.copy()\n",
    "        prev_hand_vel = self.hand_vel.copy()\n",
    "        prev_hand_acc = self.hand_acc.copy()\n",
    "\n",
    "        self.hand_pos = self.forward_kinematics(self.theta)\n",
    "        self.hand_vel = (self.hand_pos - prev_hand_pos) / self.dt\n",
    "        self.hand_acc = (self.hand_vel - prev_hand_vel) / self.dt\n",
    "        self.hand_jerk = (self.hand_acc - prev_hand_acc) / self.dt\n",
    "\n",
    "        hand_jerk_norm_sq = float(np.dot(self.hand_jerk, self.hand_jerk))\n",
    "        self.jerk_sum += hand_jerk_norm_sq * self.dt\n",
    "\n",
    "        self.steps += 1\n",
    "        self.t += self.dt\n",
    "\n",
    "        dist_to_goal = float(np.linalg.norm(self.hand_pos - self.goal_pos))\n",
    "\n",
    "        # ---------- 報酬計算 ----------\n",
    "        reward_dist_step = self.shaping_dist_coeff * (self.prev_dist - dist_to_goal)\n",
    "        reward_jerk_step = - self.shaping_jerk_coeff * hand_jerk_norm_sq * self.dt\n",
    "        reward_time_step = - self.time_cost\n",
    "        reward = reward_dist_step + reward_jerk_step + reward_time_step\n",
    "\n",
    "        self.dist_window.append(dist_to_goal)\n",
    "        if len(self.dist_window) > self.stall_window:\n",
    "            self.dist_window.pop(0)\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        terminal_jerk_penalty = 0.0\n",
    "        terminal_vel_penalty = 0.0\n",
    "        time_bonus = 0.0\n",
    "\n",
    "        if self.reached_goal(dist_to_goal):\n",
    "            time_bonus = self.R_goal * np.exp(\n",
    "                - (self.t - self.T_target) ** 2 / (2 * max(1e-12, (self.sigma_T ** 2)))\n",
    "            )\n",
    "            terminal_jerk_penalty = - self.terminal_jerk_coeff * (self.jerk_sum / max(1, self.steps))\n",
    "            hand_vel_norm_sq = float(np.dot(self.hand_vel, self.hand_vel))\n",
    "            terminal_vel_penalty = - self.terminal_vel_coeff * hand_vel_norm_sq\n",
    "\n",
    "            reward += time_bonus + terminal_jerk_penalty + terminal_vel_penalty\n",
    "            terminated = True\n",
    "\n",
    "        elif len(self.dist_window) >= self.stall_window:\n",
    "            prog = self.dist_window[0] - self.dist_window[-1]\n",
    "            if prog < self.min_progress_per_window:\n",
    "                truncated = True\n",
    "                reward += -10.0\n",
    "\n",
    "        if not terminated and self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "            reward += - self.terminal_jerk_coeff * (self.jerk_sum / max(1, self.steps))\n",
    "\n",
    "        reward_components = {\n",
    "            \"reward_dist_step\": float(reward_dist_step),\n",
    "            \"reward_jerk_step\": float(reward_jerk_step),\n",
    "            \"reward_time_step\": float(reward_time_step),\n",
    "            \"terminal_jerk_penalty\": float(terminal_jerk_penalty),\n",
    "            \"terminal_vel_penalty\": float(terminal_vel_penalty),\n",
    "            \"time_bonus\": float(time_bonus),\n",
    "            \"jerk_sum\": float(self.jerk_sum),\n",
    "            \"hand_jerk_norm_sq\": hand_jerk_norm_sq,\n",
    "        }\n",
    "\n",
    "        info = {\n",
    "            \"hand_pos\": self.hand_pos,\n",
    "            \"hand_vel\": self.hand_vel,\n",
    "            \"hand_acc\": self.hand_acc,\n",
    "            \"hand_jerk\": self.hand_jerk,\n",
    "            \"theta\": self.theta.copy(),\n",
    "            \"theta_vel\": self.theta_vel.copy(),\n",
    "            \"theta_acc\": self.theta_acc.copy(),\n",
    "            \"theta_jerk\": self.theta_jerk.copy(),\n",
    "            \"dist_to_goal\": dist_to_goal,\n",
    "            \"t\": float(self.t),\n",
    "            \"action\": tau_clipped.copy(),\n",
    "            \"action_same\": action_same,\n",
    "            \"same_action_counter\": int(self._same_action_counter),\n",
    "            \"reward_total\": float(reward),\n",
    "            \"sigma_T\": float(self.sigma_T),\n",
    "            \"sigma_update\": dict(self._last_sigma_update),\n",
    "            \"stage2_active\": bool(self.stage2_active),\n",
    "            \"stage2_start_episode\": self.stage2_start_episode,\n",
    "            \"stage2_success_rate\": self.stage2_last_success_rate,\n",
    "            \"shaping_scale\": self._get_stage2_shaping_scale(),\n",
    "            **reward_components,\n",
    "        }\n",
    "\n",
    "        # --- 終端時の success-window & sigma 更新 ---\n",
    "        if terminated or truncated:\n",
    "            success_flag = 1 if terminated else 0\n",
    "            self._success_window.append(success_flag)\n",
    "\n",
    "            if len(self._success_window) == self._success_window_size:\n",
    "                self._sigma_update_counter += 1\n",
    "                if (self._sigma_update_counter % self.sigma_update_interval) == 0:\n",
    "                    _ = self.update_sigma_by_success_rate()\n",
    "                    # sigma / success_rate が変わったら次エピソードから shaping 更新される\n",
    "\n",
    "        self.prev_dist = dist_to_goal\n",
    "        return self._get_obs(), float(reward), bool(terminated), bool(truncated), info\n",
    "\n",
    "    # ---------- 描画 ----------\n",
    "    def render(self):\n",
    "        print(\n",
    "            f\"t={self.t:.3f}s step={self.steps}, \"\n",
    "            f\"theta1={np.degrees(self.theta[0]):.2f} deg, \"\n",
    "            f\"theta2={np.degrees(self.theta[1]):.2f} deg, \"\n",
    "            f\"hand={self.hand_pos}, sigma_T={self.sigma_T:.6f}, \"\n",
    "            f\"stage2={self.stage2_active}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境が動くかの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoJointReachingEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エピソードごとの報酬などの学習内部指標を定期的に保存するためのコールバック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FullMonitorCallback\n",
    "====================\n",
    "\n",
    "役割:\n",
    "  - Stable-Baselines3（PPO/SAC など）での学習中に、\n",
    "    「エピソード単位の指標」を CSV に記録するためのコールバック。\n",
    "\n",
    "設計方針:\n",
    "  - 環境の `info` に入ってくる「報酬の内訳」や `sigma_T` など、\n",
    "    アルゴリズム非依存の情報だけを集約する。\n",
    "  - 1 エピソードぶんの情報をバッファに貯めておき、\n",
    "    エピソード終了時に 1 行追記することで、後から解析しやすいログを作る。\n",
    "\n",
    "関連コンポーネント:\n",
    "  - TwoJointReachingEnv: `info` に報酬内訳 / sigma_T / shaping_scale などを詰めている環境クラス。\n",
    "  - EvalWithTrainEpisodeCallback: 評価タイミングと学習エピソード数を紐づける EvalCallback 拡張。\n",
    "\"\"\"\n",
    "\n",
    "class FullMonitorCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    エピソードごとの指標を CSV に保存するコールバック（SB3 用）。\n",
    "\n",
    "    記録する列:\n",
    "      - episode                    : エピソード番号（1, 2, 3, ...）\n",
    "      - episode_length             : そのエピソードのステップ数\n",
    "      - total_reward               : エピソード内報酬の合計\n",
    "      - sum_reward_dist_step       : 距離 shaping 報酬のステップ和\n",
    "      - sum_reward_jerk_step       : 躍度 shaping ペナルティのステップ和\n",
    "      - sum_reward_time_step       : 毎ステップの時間コストのステップ和\n",
    "      - sum_terminal_jerk_penalty  : 終端躍度ペナルティの合計\n",
    "      - sum_terminal_vel_penalty   : 終端速度ペナルティの合計\n",
    "      - sum_time_bonus             : 時間ボーナスの合計\n",
    "      - jerk_sum                   : エピソード全体の躍度積分（環境側で計算）\n",
    "      - running_mean_total_reward_N: 直近 N エピソードの total_reward の移動平均\n",
    "      - episode_wall_time          : 1 エピソードにかかった実時間（wall clock）\n",
    "      - sigma_T                    : エピソード終端時点の sigma_T\n",
    "      - shaping_scale              : stage2 フェーズアウト用スケール（0〜1）\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # コンストラクタ: ログ先ディレクトリ / 保存間隔 / 窓幅などを受け取る\n",
    "    # ------------------------------------------------------------\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_dir: str,\n",
    "        save_interval: int = 1,\n",
    "        running_window: int = 10,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_dir : str\n",
    "            CSV を出力するルートディレクトリ（run_id ごとなど）。\n",
    "        save_interval : int\n",
    "            何エピソードごとに 1 行書き込むか。\n",
    "            - 1 の場合: 毎エピソード追記（デバッグ向き）\n",
    "            - >1 の場合: 間引いて書き込む（大規模学習向き）\n",
    "        running_window : int\n",
    "            直近何エピソードで「平均 total_reward」を取るか（移動平均）。\n",
    "        verbose : int\n",
    "            1 の場合、保存タイミングなどを print する。\n",
    "        \"\"\"\n",
    "        super().__init__(verbose)\n",
    "\n",
    "        # --- ログディレクトリを確実に作成 ---\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "        # --- 保存間隔 / 移動平均窓幅 ---\n",
    "        self.save_interval = int(save_interval)\n",
    "        self.running_window = int(running_window)\n",
    "\n",
    "        # --- 出力 CSV のパス ---\n",
    "        self.csv_path = os.path.join(self.log_dir, \"episode_full_metrics.csv\")\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # CSV ヘッダ定義\n",
    "        #   解析で使う列だけに絞ることで、ログの見通しを良くしている。\n",
    "        # --------------------------------------------------------\n",
    "        header = [\n",
    "            \"episode\",\n",
    "            \"episode_length\",\n",
    "            \"total_reward\",\n",
    "            \"sum_reward_dist_step\",\n",
    "            \"sum_reward_jerk_step\",\n",
    "            \"sum_reward_time_step\",\n",
    "            \"sum_terminal_jerk_penalty\",\n",
    "            \"sum_terminal_vel_penalty\",\n",
    "            \"sum_time_bonus\",\n",
    "            \"jerk_sum\",\n",
    "            f\"running_mean_total_reward_{self.running_window}\",\n",
    "            \"episode_wall_time\",\n",
    "            \"sigma_T\",\n",
    "            \"shaping_scale\",\n",
    "        ]\n",
    "\n",
    "        # run ごとにヘッダを書き込む（以降は追記モードで行を追加）\n",
    "        with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # エピソード内の一時バッファ\n",
    "        #   SB3 の callback は step 単位で呼ばれるので、\n",
    "        #   1 エピソードぶんをここに貯めてから終端で 1 行にまとめる。\n",
    "        # --------------------------------------------------------\n",
    "        self._step_rewards = []   # 各ステップの報酬\n",
    "        self._step_infos = []     # 各ステップの info（shallow copy）\n",
    "\n",
    "        # エピソードカウンタ（1, 2, 3, ...）\n",
    "        self.episode_count = 0\n",
    "\n",
    "        # 直近 N エピソード分の集計用（移動平均 total_reward 用）\n",
    "        self.episode_records = []\n",
    "\n",
    "        # エピソード開始時刻（wall clock）\n",
    "        self._ep_start_time = None\n",
    "\n",
    "        # 学習全体の開始時刻（必要に応じて time/系を増やせるよう保持）\n",
    "        self._train_start_time = None\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # helper: 学習開始時に呼ばれる\n",
    "    # ------------------------------------------------------------\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        学習開始時に呼ばれるフック。\n",
    "        ここでは「学習開始時刻（wall clock）」を記録する。\n",
    "        \"\"\"\n",
    "        self._train_start_time = time.time()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # helper: info 内の特定キーの値をエピソード内で合計する\n",
    "    # ------------------------------------------------------------\n",
    "    def _sum_info_key(self, key: str) -> float:\n",
    "        \"\"\"\n",
    "        環境側の info に入っている `key` の値を、\n",
    "        1 エピソード分すべてのステップで合計して返す。\n",
    "\n",
    "        例:\n",
    "          key=\"reward_dist_step\" → shaping 距離報酬のステップ和。\n",
    "        \"\"\"\n",
    "        return float(sum([it.get(key, 0.0) for it in self._step_infos]))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # main: 各 step ごとに呼ばれる\n",
    "    #   - ここでは「バッファリング」と「エピソード終了検出」を行う。\n",
    "    # ------------------------------------------------------------\n",
    "    def _on_step(self) -> bool:\n",
    "        # --------------------------------------------------------\n",
    "        # 1. SB3 の locals から reward / info / done を取り出す\n",
    "        #    VecEnv 前提で list/ndarray になるため 0 番だけを見る（1 env 想定）。\n",
    "        # --------------------------------------------------------\n",
    "        infos = self.locals.get(\"infos\")\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "\n",
    "        # info の取り出し\n",
    "        if isinstance(infos, (list, tuple, np.ndarray)):\n",
    "            info = infos[0] if len(infos) > 0 else {}\n",
    "        else:\n",
    "            info = infos or {}\n",
    "\n",
    "        # reward の取り出し\n",
    "        if isinstance(rewards, (list, tuple, np.ndarray)):\n",
    "            reward = float(rewards[0])\n",
    "        else:\n",
    "            reward = float(rewards or 0.0)\n",
    "\n",
    "        # ステップごとの値をバッファに積む\n",
    "        self._step_rewards.append(reward)\n",
    "        self._step_infos.append(dict(info))  # shallow copy にしておく\n",
    "\n",
    "        # done 判定の取得（1 env 前提で 0 番だけ見る）\n",
    "        dones = self.locals.get(\"dones\")\n",
    "        if isinstance(dones, (list, tuple, np.ndarray)):\n",
    "            done_flag = bool(dones[0])\n",
    "        else:\n",
    "            done_flag = bool(dones)\n",
    "\n",
    "        # エピソード開始時刻の初期化\n",
    "        if self._ep_start_time is None:\n",
    "            self._ep_start_time = time.time()\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # 2. エピソード終了時の処理\n",
    "        #    - エピソード内バッファを集約して 1 行の CSV レコードを作る。\n",
    "        # --------------------------------------------------------\n",
    "        if done_flag:\n",
    "            # ---- 基本情報（エピソード番号 / 長さ / 総報酬）----\n",
    "            self.episode_count += 1\n",
    "            ep_len = len(self._step_rewards)\n",
    "            total_reward = float(np.sum(self._step_rewards)) if self._step_rewards else 0.0\n",
    "\n",
    "            # VecEnv で wrap されている TwoJointReachingEnv を取得（envs[0] を想定）\n",
    "            base_env = None\n",
    "            try:\n",
    "                base_env = getattr(self.training_env, \"envs\", [None])[0]\n",
    "            except Exception:\n",
    "                base_env = None\n",
    "\n",
    "            # ---- 報酬内訳の集約 ----\n",
    "            sum_reward_dist_step = self._sum_info_key(\"reward_dist_step\")\n",
    "            sum_reward_jerk_step = self._sum_info_key(\"reward_jerk_step\")\n",
    "            sum_reward_time_step = self._sum_info_key(\"reward_time_step\")\n",
    "            sum_terminal_jerk_penalty = self._sum_info_key(\"terminal_jerk_penalty\")\n",
    "            sum_terminal_vel_penalty = self._sum_info_key(\"terminal_vel_penalty\")\n",
    "            sum_time_bonus = self._sum_info_key(\"time_bonus\")\n",
    "\n",
    "            # jerk_sum（基本的には終端 info に入っている想定）\n",
    "            if self._step_infos and \"jerk_sum\" in self._step_infos[-1]:\n",
    "                jerk_sum = float(self._step_infos[-1].get(\"jerk_sum\", 0.0))\n",
    "            else:\n",
    "                jerk_sum = float(\"nan\")\n",
    "\n",
    "            # 成功フラグ:\n",
    "            #  - 今後成功率を集計したくなったときのために episode_records 内には保持しておく。\n",
    "            success = bool(self._step_infos[-1].get(\"success\", False)) if self._step_infos else False\n",
    "            if not success:\n",
    "                last_dist = float(self._step_infos[-1].get(\"dist_to_goal\", np.inf)) if self._step_infos else np.inf\n",
    "                goal_thresh = None\n",
    "                if base_env is not None:\n",
    "                    goal_thresh = getattr(base_env, \"goal_threshold\", None)\n",
    "                    if goal_thresh is None:\n",
    "                        goal_thresh = getattr(base_env, \"THRESHOLD\", None)\n",
    "                if goal_thresh is not None:\n",
    "                    success = last_dist <= float(goal_thresh)\n",
    "\n",
    "            # ---- エピソードにかかった実時間（wall clock）----\n",
    "            ep_wall_time = time.time() - (self._ep_start_time or time.time())\n",
    "            self._ep_start_time = None\n",
    "\n",
    "            # ---- 直近 N エピソード分の total_reward を使った移動平均 ----\n",
    "            self.episode_records.append({\n",
    "                \"episode\": self.episode_count,\n",
    "                \"episode_length\": ep_len,\n",
    "                \"total_reward\": total_reward,\n",
    "                \"success\": int(success),\n",
    "                \"jerk_sum\": jerk_sum,\n",
    "            })\n",
    "            last_N = self.episode_records[-self.running_window:]\n",
    "            if last_N:\n",
    "                running_mean_total_reward = float(\n",
    "                    np.mean([r[\"total_reward\"] for r in last_N])\n",
    "                )\n",
    "            else:\n",
    "                running_mean_total_reward = 0.0\n",
    "\n",
    "            # ---- sigma_T, shaping_scale の取得 ----\n",
    "            #      いつ sigma_T や shaping_scale がどう変化していたかを後から追えるように、\n",
    "            #      エピソード終端時点の値を記録する。\n",
    "            if self._step_infos:\n",
    "                last_info = self._step_infos[-1]\n",
    "                sigma_T_val = float(last_info.get(\"sigma_T\", float(\"nan\")))\n",
    "                shaping_scale_val = float(last_info.get(\"shaping_scale\", float(\"nan\")))\n",
    "            else:\n",
    "                sigma_T_val = float(\"nan\")\n",
    "                shaping_scale_val = float(\"nan\")\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # 3. CSV 行の構築 & 書き込み\n",
    "            # ----------------------------------------------------\n",
    "            row = [\n",
    "                self.episode_count,\n",
    "                ep_len,\n",
    "                total_reward,\n",
    "                sum_reward_dist_step,\n",
    "                sum_reward_jerk_step,\n",
    "                sum_reward_time_step,\n",
    "                sum_terminal_jerk_penalty,\n",
    "                sum_terminal_vel_penalty,\n",
    "                sum_time_bonus,\n",
    "                jerk_sum,\n",
    "                running_mean_total_reward,\n",
    "                ep_wall_time,\n",
    "                sigma_T_val,\n",
    "                shaping_scale_val,\n",
    "            ]\n",
    "\n",
    "            # save_interval ごとに追記（毎エピソード書きたい場合は save_interval=1）\n",
    "            if (self.episode_count % self.save_interval) == 0:\n",
    "                with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "                if self.verbose:\n",
    "                    print(f\"[FullMonitor] Saved episode {self.episode_count} to {self.csv_path}\")\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # 4. SB3 logger にも簡単な指標を流す\n",
    "            #    - success / running_success_rate は記録しない。\n",
    "            # ----------------------------------------------------\n",
    "            try:\n",
    "                self.logger.record(\"episode/total_reward\", total_reward)\n",
    "                self.logger.record(\"episode/jerk_sum\", jerk_sum)\n",
    "                self.logger.record(\"episode/length\", ep_len)\n",
    "                self.logger.record(\"episode/running_mean_reward\", running_mean_total_reward)\n",
    "                self.logger.record(\"episode/sigma_T\", sigma_T_val)\n",
    "                self.logger.record(\"episode/shaping_scale\", shaping_scale_val)\n",
    "            except Exception:\n",
    "                # logger が存在しない状況でも学習は続けたいので握りつぶす\n",
    "                pass\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # 5. 次エピソードに備えてバッファをクリア\n",
    "            # ----------------------------------------------------\n",
    "            self._step_rewards.clear()\n",
    "            self._step_infos.clear()\n",
    "\n",
    "        # True を返すことで学習継続（False だと学習停止）\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価タイミングごとの評価結果とその時点までに学習したエピソード数をCSV に記録するためのコールバック\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EvalWithTrainEpisodeCallback\n",
    "============================\n",
    "\n",
    "役割:\n",
    "  - Stable-Baselines3 の EvalCallback を拡張し、\n",
    "    「評価タイミングごとの評価結果」と「その時点までに学習したエピソード数」を\n",
    "    CSV に記録するためのコールバック。\n",
    "\n",
    "設計方針:\n",
    "  - 評価ロジック自体は EvalCallback（親クラス）に任せる。\n",
    "    → 本クラスでは「評価が1回終わったタイミングを検知して、ログを整形して残す」\n",
    "      ことだけに責務を絞る。\n",
    "  - 学習の進行状況と評価結果をあとからマッチングしやすいように、\n",
    "    「評価インデックス」「総ステップ数」「mean_reward」「平均エピソード長」\n",
    "    「評価時点での学習エピソード数」「その時点までのベスト平均報酬」を CSV に残す。\n",
    "  - どのエピソードでベストが出たかの詳細（best_eval_index, best_train_episode_at_eval）は\n",
    "    外部で集計可能なため、クラス内では保持しない設計とする。\n",
    "\n",
    "関連コンポーネント:\n",
    "  - FullMonitorCallback:\n",
    "      episode_count を持っており、「学習済みエピソード数」を知るために利用する。\n",
    "  - TwoJointReachingEnv:\n",
    "      評価対象となる環境（ここでは env の中身には依存しない）。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class EvalWithTrainEpisodeCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    EvalCallback を拡張して、\n",
    "    - 各評価の結果（mean_reward / mean_ep_length）\n",
    "    - 評価実行時点で何エピソード学習していたか（FullMonitorCallback.episode_count）\n",
    "    - その時点までのベスト mean_reward（best_mean_reward）\n",
    "    を CSV に記録するためのクラス。\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # コンストラクタ: 評価環境 / 監視用コールバック / CSV パスなどを受け取る\n",
    "    # ------------------------------------------------------------\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env,\n",
    "        full_monitor_callback=None,\n",
    "        csv_path=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        eval_env :\n",
    "            評価に用いる環境（EvalCallback にそのまま渡す）。\n",
    "        full_monitor_callback : FullMonitorCallback | None\n",
    "            学習エピソード数を知るために参照するコールバック。\n",
    "            - None の場合は train_episode_at_eval が None になる。\n",
    "        csv_path : str | None\n",
    "            評価ログを書き出す CSV ファイルパス。\n",
    "            - None の場合はファイルには書き出さず、属性のみ更新。\n",
    "        kwargs :\n",
    "            EvalCallback に引き継ぐ追加の引数（n_eval_episodes, eval_freq など）。\n",
    "        \"\"\"\n",
    "        # 親クラス（EvalCallback）の初期化を先に呼ぶ\n",
    "        super().__init__(eval_env=eval_env, **kwargs)\n",
    "\n",
    "        # 学習側のエピソード数を参照するためのコールバック\n",
    "        self.full_monitor_callback = full_monitor_callback\n",
    "\n",
    "        # 評価結果を記録する CSV ファイルパス\n",
    "        self.csv_path = csv_path\n",
    "\n",
    "        # すでにログ済みの「評価回数」を記録しておく\n",
    "        # （EvalCallback 側は evaluations_results を蓄積していくので、\n",
    "        #   それが増えたかどうかで「新しい評価が終わった」ことを検知する）\n",
    "        self._n_evals_logged = 0\n",
    "\n",
    "        # これまでに観測した mean_reward の最大値（学習全体のベスト評価値）\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # CSV ファイルのヘッダ行を書き込む（csv_path が指定されている場合）\n",
    "        # --------------------------------------------------------\n",
    "        if self.csv_path is not None:\n",
    "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f, delimiter=\",\")\n",
    "                writer.writerow(\n",
    "                    [\n",
    "                        \"eval_index\",              # 1, 2, 3, ... 評価何回目か\n",
    "                        \"timesteps\",               # 評価実行時点の総学習ステップ数\n",
    "                        \"mean_reward\",             # 評価エピソード群の平均報酬\n",
    "                        \"mean_ep_length\",          # 評価エピソード群の平均エピソード長\n",
    "                        \"train_episode_at_eval\",   # 評価時点までに学習したエピソード数\n",
    "                        \"best_mean_reward_so_far\", # その時点までのベスト平均報酬\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # main: 学習ステップごとに呼ばれる\n",
    "    #   - 親クラスの処理で「必要なタイミングで評価」が走る。\n",
    "    #   - ここでは「評価が新たに1回終わったか？」を検知して CSV に追記する。\n",
    "    # ------------------------------------------------------------\n",
    "    def _on_step(self) -> bool:\n",
    "        # まず EvalCallback 側の標準処理を実行\n",
    "        # （指定された eval_freq ごとに評価が回り、evaluations_results 等が更新される）\n",
    "        continue_training = super()._on_step()\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # 1. 新しい評価が終わったかどうかをチェック\n",
    "        #    - evaluations_results は shape: (n_evals, n_eval_episodes)\n",
    "        #    - 要素数が増えていれば「評価1回分が増えた」とみなせる。\n",
    "        # --------------------------------------------------------\n",
    "        current_n_evals = len(self.evaluations_results)\n",
    "        if current_n_evals > self._n_evals_logged:\n",
    "            # 今回の評価インデックス（1,2,3,...）\n",
    "            eval_index = current_n_evals\n",
    "\n",
    "            # EvalCallback が最後に計算した mean_reward\n",
    "            mean_reward = float(self.last_mean_reward)\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # 2. 評価エピソード群の平均エピソード長を求める\n",
    "            #    - evaluations_length[-1] は「今回の評価で走らせた各エピソード長のリスト」\n",
    "            # ----------------------------------------------------\n",
    "            if len(self.evaluations_length) > 0:\n",
    "                mean_ep_length = float(np.mean(self.evaluations_length[-1]))\n",
    "            else:\n",
    "                mean_ep_length = float(\"nan\")\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # 3. 評価実行時点で何エピソード学習していたかを取得\n",
    "            #    - FullMonitorCallback が episode_count を持っている想定。\n",
    "            #    - 無ければ None のまま（学習エピソード数を追跡しないモード）。\n",
    "            # ----------------------------------------------------\n",
    "            train_episode_at_eval = None\n",
    "            if self.full_monitor_callback is not None:\n",
    "                train_episode_at_eval = self.full_monitor_callback.episode_count\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # 4. これまでのベスト mean_reward を更新\n",
    "            #    - 「評価タイミングごとのベスト値」を追うことで、\n",
    "            #      ログの中から最大値を探さなくても済むようにしている。\n",
    "            # ----------------------------------------------------\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # 5. CSV に 1 評価分のレコードを追記\n",
    "            # ----------------------------------------------------\n",
    "            if self.csv_path is not None:\n",
    "                with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f, delimiter=\",\")\n",
    "                    writer.writerow(\n",
    "                        [\n",
    "                            eval_index,\n",
    "                            self.num_timesteps,       # 学習済みの総ステップ数\n",
    "                            mean_reward,\n",
    "                            mean_ep_length,\n",
    "                            train_episode_at_eval,\n",
    "                            self.best_mean_reward,\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "            # この時点までの評価回数を更新し、\n",
    "            # 次の _on_step 呼び出しでは「新しい評価が終わっていない」と判断されるようにする。\n",
    "            self._n_evals_logged = current_n_evals\n",
    "\n",
    "        # True を返すことで学習継続（EvalCallback の判定も尊重）\n",
    "        return continue_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コード\n",
    "### PPO モデルと EvalCallback の設定\n",
    "\n",
    "ここでは、**PPO (Proximal Policy Optimization)** を用いて  \n",
    "2 関節アーム環境 `TwoJointReachingEnv` の到達動作を学習します。\n",
    "\n",
    "本研究では、  \n",
    "- **毎ステップでの躍度（jerk）ペナルティ**\n",
    "- **距離改善による shaping**\n",
    "- **目標時間 T に基づく時間ボーナス**\n",
    "- **stage1 → stage2 での shaping フェードアウト**\n",
    "を組み合わせた **人らしい運動軌道の獲得**を目的としています。\n",
    "\n",
    "PPO に渡している主な引数の意味は以下の通りです：\n",
    "\n",
    "- **total_timesteps**  \n",
    "  学習に使う総ステップ数（1 ステップ＝1 回の環境遷移）\n",
    "- **eval_freq**  \n",
    "  何ステップごとに評価するか（例：2000 → 2000 ステップごと）\n",
    "- **best_model_save_path**  \n",
    "  評価結果が最良のモデルを保存するディレクトリ\n",
    "- **deterministic=True**  \n",
    "  評価時は確率ではなく決定的に動作させ、ポリシーの純粋な性能を確認\n",
    "- **render=False**  \n",
    "  評価時の描画（速度低下するため通常は False）\n",
    "\n",
    "> 本研究では躍度（jerk）と時間・距離の複数コストを含むため、  \n",
    "> 学習過程そのものがどのように軌道を変化させるかを EvalCallback で追跡している。\n",
    "\n",
    "---\n",
    "\n",
    "## 学習ログの見方\n",
    "\n",
    "PPO 学習時は、`FullMonitorCallback` によりエピソード単位の詳細な指標が CSV に記録されます。  \n",
    "そのうち、現在重要なものは以下です：\n",
    "\n",
    "```plaintext\n",
    "| episode/                |\n",
    "|    jerk_sum             | １エピソードの手先躍度の総和（滑らかさの指標）\n",
    "|    length               | エピソードの総ステップ数\n",
    "|    running_mean_reward  | 直近 N エピソードの平均総報酬\n",
    "|    shaping_scale        | shaping が 1→0 に減衰する割合（stage2）\n",
    "|    sigma_T              | 時間ボーナスの幅（時間戦略の変化指標）\n",
    "|    total_reward         | １エピソードの総報酬\n",
    "|\n",
    "| eval/                   |\n",
    "|    mean_ep_length       | 評価時の平均ステップ数\n",
    "|    mean_reward          | 評価時の平均総報酬\n",
    "|\n",
    "| time/                   |\n",
    "|    total_timesteps      | 学習で実行した環境ステップの総数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. ログ用ディレクトリの作成 & run ID の生成\n",
    "# ============================================================\n",
    "\n",
    "# 実行ごとに一意な ID（YYYY_MM_DD_hh_mm_ss）を振る\n",
    "run_id = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "# ログ保存用のルートディレクトリ（この下に params / モデル / CSV などを置く）\n",
    "log_dir = f\"./logs/{run_id}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 2. すべてのハイパーパラメータを training_params に集約\n",
    "#    （コード②で定義した値 + 学習関係）\n",
    "# ============================================================\n",
    "\n",
    "training_params = {\n",
    "    # ----- シミュレーション / 環境パラメータ -----\n",
    "    \"DT\": DT,                            # 時間刻み [s]\n",
    "    \"L\": L,                              # アーム全長 [m]\n",
    "    \"STEPS_MAX\": STEPS_MAX,              # 1エピソードの最大ステップ数\n",
    "    \"THRESHOLD\": THRESHOLD,              # ゴール判定距離 [m]\n",
    "\n",
    "    # ----- アクション・状態空間関連 -----\n",
    "    \"ACTION_MIN\": ACTION_MIN,            # トルク最小値 [N·m]\n",
    "    \"ACTION_MAX\": ACTION_MAX,            # トルク最大値 [N·m]\n",
    "    \"THETA_MIN\": THETA_MIN,              # 関節角度の下限 [deg]\n",
    "    \"THETA_MAX\": THETA_MAX,              # 関節角度の上限 [deg]\n",
    "    \"THETA_INIT\": THETA_INIT,            # 初期関節角度 [deg]（2関節分）\n",
    "    \"GOAL_POS\": GOAL_POS,                # ゴール位置 (x, y) [m]\n",
    "\n",
    "    # ----- 物理パラメータ（質量・ダンピング） -----\n",
    "    \"M1\": M1,                            # 上腕リンク質量 [kg]\n",
    "    \"M2\": M2,                            # 前腕リンク質量 [kg]\n",
    "    \"JOINT_DAMPING\": JOINT_DAMPING,      # 関節粘性 [N·m·s/rad]\n",
    "\n",
    "    # ----- 終端報酬まわり -----\n",
    "    \"REWARD_P_V\": REWARD_P_V,            # 終端速度ペナルティ係数\n",
    "    \"REWARD_J\": REWARD_J,                # 終端躍度ペナルティ係数\n",
    "    \"REWARD_JE_LIM\": REWARD_JE_LIM,      # 参考用スケール（info に出すだけ）\n",
    "\n",
    "    # ----- 目標時間 / 時間ボーナス -----\n",
    "    \"T_TARGET\": T_TARGET,                # 目標到達時間 [s]\n",
    "    \"SIGMA_T\": SIGMA_T,                  # 時間ボーナスの幅 [s]\n",
    "    \"R_GOAL\": R_GOAL,                    # ゴールボーナスの基礎値\n",
    "    \"TIME_COST\": TIME_COST,              # 毎ステップの時間コスト\n",
    "\n",
    "    # ----- shaping 報酬（ステップごとのガイド） -----\n",
    "    \"SHAPING_DIST_COEFF\": SHAPING_DIST_COEFF,  # 距離改善の係数\n",
    "    \"SHAPING_JERK_COEFF\": SHAPING_JERK_COEFF,  # 瞬時躍度ペナルティ係数\n",
    "\n",
    "    # ----- 停滞検出（早期打ち切り） -----\n",
    "    \"STALL_WINDOW_S\": STALL_WINDOW_S,          # 停滞判定窓の長さ [s]\n",
    "    \"STALL_WINDOW\": STALL_WINDOW,              # 停滞判定窓（ステップ数）\n",
    "    \"MIN_PROGRESS_PER_WINDOW\": MIN_PROGRESS_PER_WINDOW,  # 停滞判定しきい値 [m]\n",
    "\n",
    "    # ----- 学習関連（PPO / SAC 共通で使えるように） -----\n",
    "    \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,  # 全学習ステップ数\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,      # 学習率\n",
    "    \"TAU\": TAU,                          # （SAC用）ターゲット更新係数 / α更新など\n",
    "    \"HID_LAY\": HID_LAY,                  # MLP の隠れ層ユニット数\n",
    "    \"BUFFER_SIZE\": BUFFER_SIZE,          # （SAC用）リプレイバッファサイズ\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,            # ミニバッチサイズ\n",
    "    \"EVAL_FREQ\": EVAL_FREQ,              # 何ステップごとに評価するか\n",
    "    \"SAVE_INTERVAL\": SAVE_INTERVAL,      # 何エピソードごとに保存 or ログを切るか\n",
    "    \"RUNNNIG_WINDOW\": RUNNNIG_WINDOW,    # 直近エピソード何本で移動平均を取るか\n",
    "    \"STATES_ARM\": STATES_ARM,            # アーム状態次元（hand pos/vel + time など）\n",
    "    \"SAME_ACTION_TOL\": SAME_ACTION_TOL,  # 同一アクション判定の許容差\n",
    "\n",
    "    # ----- sigma_T 自動調整まわり -----\n",
    "    \"SUCCESS_WINDOW_SIZE\": SUCCESS_WINDOW_SIZE,      # 成功率を見る窓サイズ\n",
    "    \"SIGMA_UPDATE_INTERVAL\": SIGMA_UPDATE_INTERVAL,  # 何回ごとに sigma を更新するか\n",
    "    \"SIGMA_SHARPEN_THRESHOLD\": SIGMA_SHARPEN_THRESHOLD,  # σ縮小の成功率しきい値\n",
    "    \"SIGMA_BLUNT_THRESHOLD\": SIGMA_BLUNT_THRESHOLD,      # σ拡大の成功率しきい値\n",
    "    \"SIGMA_SHARPEN_FACTOR\": SIGMA_SHARPEN_FACTOR,        # σ縮小倍率 (<1)\n",
    "    \"SIGMA_BLUNT_FACTOR\": SIGMA_BLUNT_FACTOR,            # σ拡大倍率 (>1)\n",
    "    \"SIGMA_MIN\": SIGMA_MIN,                        # σの下限\n",
    "    \"SIGMA_MAX_FACTOR\": SIGMA_MAX_FACTOR,          # σ上限の倍率（SIGMA_T * factor）\n",
    "\n",
    "    # ----- stage2（shaping フェードアウト）関連 -----\n",
    "    \"STAGE2_SUCCESS_THRESHOLD\": STAGE2_SUCCESS_THRESHOLD,  # stage2 へ入る成功率しきい値\n",
    "    \"STAGE2_SIGMA_THRESHOLD\": STAGE2_SIGMA_THRESHOLD,      # stage2 へ入る σ_T しきい値\n",
    "    \"STAGE2_FADE_EPISODES\": STAGE2_FADE_EPISODES,          # stage2 で shaping を 1→0 にするエピソード数\n",
    "}\n",
    "\n",
    "# params.json として保存しておくと、後で再現性が取りやすい\n",
    "with open(os.path.join(log_dir, \"params.json\"), \"w\") as f:\n",
    "    json.dump(training_params, f, indent=4)\n",
    "\n",
    "# ============================================================\n",
    "# 3. 環境の作成（学習用 env と評価用 eval_env）\n",
    "# ============================================================\n",
    "\n",
    "# 学習用の環境（PPO がサンプルを集める先）\n",
    "env = TwoJointReachingEnv()\n",
    "\n",
    "# 評価専用の環境（EvalCallback がポリシーをテストする先）\n",
    "eval_env = TwoJointReachingEnv()\n",
    "\n",
    "# ============================================================\n",
    "# 4. コールバックの準備\n",
    "#    - FullMonitorCallback: エピソードごとの詳細ログ\n",
    "#    - EvalWithTrainEpisodeCallback: 定期評価 + その時点のエピソード数\n",
    "# ============================================================\n",
    "\n",
    "# エピソードごとの報酬内訳・sigma_T などを CSV に落とすコールバック\n",
    "full_monitor_cb = FullMonitorCallback(\n",
    "    log_dir=log_dir,\n",
    "    verbose=1,  # 1: ある程度ログを表示 / 0: 静かに実行\n",
    ")\n",
    "\n",
    "# 評価時のメトリクスを保存する CSV のパス\n",
    "eval_metrics_path = os.path.join(log_dir, \"eval_metrics.csv\")\n",
    "\n",
    "# EvalCallback 拡張版：\n",
    "# - 定期的に eval_env で評価\n",
    "# - best_model を保存\n",
    "# - その評価が学習の何エピソード目だったかも一緒に CSV に出す\n",
    "eval_callback = EvalWithTrainEpisodeCallback(\n",
    "    eval_env=eval_env,\n",
    "    full_monitor_callback=full_monitor_cb,   # 学習済みエピソード数を参照するため\n",
    "    csv_path=eval_metrics_path,             # 評価ログの保存先\n",
    "    best_model_save_path=log_dir,           # ベストモデルの保存先\n",
    "    log_path=log_dir,                       # 評価ログ（SB3 標準形式）保存先\n",
    "    eval_freq=training_params[\"EVAL_FREQ\"], # 何ステップごとに評価するか\n",
    "    deterministic=True,                     # 評価時は確率ではなく平均行動で実行\n",
    "    render=False,                           # 評価中に描画しない（基本 False でOK）\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5. PPO モデルの作成\n",
    "# ============================================================\n",
    "\n",
    "# ポリシーネットワークの構造（hidden 層 2段の MLP）\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[training_params[\"HID_LAY\"], training_params[\"HID_LAY\"]]\n",
    ")\n",
    "\n",
    "# 今回は PPO を使用（SAC に切り替える場合はここを差し替え）\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",                          # MLP ベースのポリシー\n",
    "    env=env,                                     # 学習環境\n",
    "    verbose=1,                                   # ログ出力レベル（1: 情報, 0: 無し）\n",
    "    learning_rate=training_params[\"LEARNING_RATE\"],\n",
    "    batch_size=training_params[\"BATCH_SIZE\"],\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    # ここで ent_coef, vf_coef, gamma などを明示的に渡したければ追加する\n",
    "    # 例:\n",
    "    # ent_coef=0.0,\n",
    "    # vf_coef=0.5,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. 学習ループの実行\n",
    "#    EvalCallback + FullMonitorCallback を併用\n",
    "# ============================================================\n",
    "\n",
    "# SB3 は callback を list で渡すと、順番に呼んでくれる\n",
    "callback = [eval_callback, full_monitor_cb]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=training_params[\"TOTAL_TIMESTEPS\"],  # 総ステップ数\n",
    "    callback=callback,                                   # 評価 & ログ用コールバック\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 7. 学習済みモデルの保存 & ベスト評価情報の表示\n",
    "# ============================================================\n",
    "\n",
    "# 最終モデルを保存（ベストモデルとは別に、最後の状態を残しておきたい場合）\n",
    "final_model_path = os.path.join(log_dir, \"final_model.zip\")\n",
    "model.save(final_model_path)\n",
    "print(f\"✅ Final model saved: {final_model_path}\")\n",
    "\n",
    "# EvalWithTrainEpisodeCallback が持っている「ベスト評価」の情報を表示\n",
    "print(\"📌 Best eval mean_reward:\", eval_callback.best_mean_reward)\n",
    "print(\"📌 Best eval index (1,2,3,...):\", eval_callback.best_eval_index)\n",
    "print(\"📌 Best eval was at train episode:\", eval_callback.best_train_episode_at_eval)\n",
    "print(\"   → その時点までに学習したエピソード数（FullMonitorCallback.episode_count ベース）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習終了時点のモデルの活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2関節アーム：1エピソード分のロールアウトをログ＋可視化するスクリプト\n",
    "# =============================================================================\n",
    "# 役割:\n",
    "#   - 学習済みモデル model と環境 env を1エピソードだけ実行し、\n",
    "#     「時系列ログ（CSV）」と「可視化用の画像ファイル」を logs/ 以下に保存する。\n",
    "#\n",
    "# 設計方針:\n",
    "#   - 研究ノートや論文用にそのまま使えるよう、\n",
    "#       (1) 解析用: CSV (time, hand pos, joint, torque, norm など)\n",
    "#       (2) 図用: PNG (軌道＋アーム姿勢、関節動的変数、手先ノルム、トルク)\n",
    "#     の2種類を必ず出力する。\n",
    "#   - 可能な限り env / info から「真値」を取りに行き、フォールバックも用意しておく。\n",
    "#   - 角度は可読性を優先して deg で保存し、内部計算のみ rad を使う。\n",
    "#   - 既存のノートブックに貼り付けて動くよう、外から env / model / THETA_INIT / L が\n",
    "#     与えられている前提で、スクリプト部分だけを整理する。\n",
    "#\n",
    "# 関連コンポーネント:\n",
    "#   - env:   TwoJointReachingEnv など「2関節アームの環境」\n",
    "#   - model: Stable-Baselines3 の学習済みエージェント\n",
    "#   - ログ:  ./logs 配下に作られる各エピソードのログフォルダ（別箇所で作成済み）\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. ログ出力先フォルダ（最新ログフォルダ）を決定\n",
    "#    → すでに別のコードで env / model の実験を回して logs/XXX ができている前提。\n",
    "#    → もっとも更新日時が新しいフォルダを「今回の結果の保存先」とみなす。\n",
    "# -----------------------------------------------------------------------------\n",
    "logs_dir = \"./logs\"\n",
    "log_folders = [\n",
    "    os.path.join(logs_dir, d)\n",
    "    for d in os.listdir(logs_dir)\n",
    "    if os.path.isdir(os.path.join(logs_dir, d))\n",
    "]\n",
    "latest_log_folder = max(log_folders, key=os.path.getmtime)\n",
    "print(f\"[INFO] 最新ログフォルダを使用: {latest_log_folder}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. ログ用のリストを初期化\n",
    "#    - DataFrame 化しやすいように「列ごとのリスト」で保持する。\n",
    "#    - 2関節アーム＋手先ノルム＋トルク＋行動パターン(action_same) を記録。\n",
    "# -----------------------------------------------------------------------------\n",
    "# 時刻と手先位置\n",
    "time_log = []\n",
    "hand_x, hand_y = [], []\n",
    "\n",
    "# 関節ごとの角度・角速度・角加速度・角躍度 [deg, deg/s, deg/s^2, deg/s^3]\n",
    "theta1_log, theta2_log = [], []\n",
    "theta1_vel_log, theta2_vel_log = [], []\n",
    "theta1_acc_log, theta2_acc_log = [], []\n",
    "theta1_jerk_log, theta2_jerk_log = [], []\n",
    "\n",
    "# トルクと「2関節がほぼ同じトルクかどうか」のフラグ\n",
    "torque1_log, torque2_log = [], []  # 各ステップの関節トルク [N·m]\n",
    "action_same_log = []               # 2 関節トルクがほぼ同じかどうか（チェック用）\n",
    "\n",
    "# 手先の速度・加速度・躍度のノルム ||v||, ||a||, ||jerk||\n",
    "hand_speed_log = []       # ||v||\n",
    "hand_acc_norm_log = []    # ||a||\n",
    "hand_jerk_norm_log = []   # ||jerk||\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 初期観測を取得し、時刻 t=0 の状態をログに残す\n",
    "#    - 解析時に「0秒時点」を含めたいので、reset()直後の値も保存しておく。\n",
    "#    - env / info の仕様変更に耐えられるよう、try/except でフォールバックを用意。\n",
    "# -----------------------------------------------------------------------------\n",
    "obs, _ = env.reset()\n",
    "\n",
    "try:\n",
    "    # env 側で各種状態が用意されている場合はこちらを優先して使う\n",
    "    init_theta      = np.asarray(env.theta).reshape(-1)\n",
    "    init_theta_vel  = np.asarray(env.theta_vel).reshape(-1)\n",
    "    init_theta_acc  = np.asarray(env.theta_acc).reshape(-1)\n",
    "    init_theta_jerk = np.asarray(env.theta_jerk).reshape(-1)\n",
    "\n",
    "    # 手先位置・速度・加速度・躍度も、あれば env から取得\n",
    "    init_hand_pos  = np.asarray(getattr(env, \"hand_pos\", env.forward_kinematics(init_theta)))\n",
    "    init_hand_vel  = np.asarray(getattr(env, \"hand_vel\", np.zeros(2)))\n",
    "    init_hand_acc  = np.asarray(getattr(env, \"hand_acc\", np.zeros(2)))\n",
    "    init_hand_jerk = np.asarray(getattr(env, \"hand_jerk\", np.zeros(2)))\n",
    "except Exception:\n",
    "    # env に上記属性が存在しない or 形が崩れている場合のフォールバック\n",
    "    # → 最低限、初期角度 THETA_INIT とゼロ速度から始める。\n",
    "    init_theta      = np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)])\n",
    "    init_theta_vel  = np.zeros(2)\n",
    "    init_theta_acc  = np.zeros(2)\n",
    "    init_theta_jerk = np.zeros(2)\n",
    "\n",
    "    init_hand_pos  = np.asarray(getattr(env, \"hand_pos\", np.array([0.0, 0.0])))\n",
    "    init_hand_vel  = np.zeros(2)\n",
    "    init_hand_acc  = np.zeros(2)\n",
    "    init_hand_jerk = np.zeros(2)\n",
    "\n",
    "# --- t = 0 をログに追加（角度系は可読性のため deg に変換して保存） ---\n",
    "t = 0.0\n",
    "time_log.append(t)\n",
    "\n",
    "# 手先位置\n",
    "hand_x.append(float(init_hand_pos[0]))\n",
    "hand_y.append(float(init_hand_pos[1]))\n",
    "\n",
    "# 関節角度など（研究メモに貼りやすいよう deg 単位で保存）\n",
    "theta1_log.append(np.degrees(init_theta[0]))\n",
    "theta2_log.append(np.degrees(init_theta[1]))\n",
    "theta1_vel_log.append(np.degrees(init_theta_vel[0]))\n",
    "theta2_vel_log.append(np.degrees(init_theta_vel[1]))\n",
    "theta1_acc_log.append(np.degrees(init_theta_acc[0]))\n",
    "theta2_acc_log.append(np.degrees(init_theta_acc[1]))\n",
    "theta1_jerk_log.append(np.degrees(init_theta_jerk[0]))\n",
    "theta2_jerk_log.append(np.degrees(init_theta_jerk[1]))\n",
    "\n",
    "# 手先ノルム（速度・加速度・躍度）\n",
    "hand_speed_log.append(float(np.linalg.norm(init_hand_vel)))\n",
    "hand_acc_norm_log.append(float(np.linalg.norm(init_hand_acc)))\n",
    "hand_jerk_norm_log.append(float(np.linalg.norm(init_hand_jerk)))\n",
    "\n",
    "# t=0 ではまだトルクは発生していないので NaN を入れておく\n",
    "torque1_log.append(np.nan)\n",
    "torque2_log.append(np.nan)\n",
    "action_same_log.append(False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. ロールアウト本体ループ\n",
    "#    - 学習済みモデル model からトルクを出し、env.step(action) で1ステップ進める。\n",
    "#    - 各ステップごとに、手先・関節・トルク・ノルムをログに追記。\n",
    "# -----------------------------------------------------------------------------\n",
    "max_steps_to_run = 200  # 可視化用なので上限ステップ数を決めておく\n",
    "\n",
    "for _ in range(max_steps_to_run):\n",
    "    # ---------------------------------------------------------\n",
    "    # 4-1. 学習済みモデルから行動（ここでは2次元トルク）を取得\n",
    "    #      - deterministic=True: 評価時なので確定的な方策を使用。\n",
    "    #      - 形状が (2,) であることを明示的にチェックしておく。\n",
    "    # ---------------------------------------------------------\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    action = np.asarray(action).reshape(-1)\n",
    "    if action.shape[0] != 2:\n",
    "        raise RuntimeError(\n",
    "            f\"model.predict returned action with wrong shape {action.shape}. expected (2,)\"\n",
    "        )\n",
    "\n",
    "    # 環境を1ステップ進める\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4-2. 時刻の更新\n",
    "    #      - env.dt を持っている場合はそれを使い、なければ 0.01 [s] をデフォルトとする。\n",
    "    # ---------------------------------------------------------\n",
    "    t += getattr(env, \"dt\", 0.01)\n",
    "    time_log.append(t)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4-3. info / env から状態を取り出す\n",
    "    #      - まず info を優先（vecenv などでステップ毎に記録された「その時点の値」が入る想定）。\n",
    "    #      - なければ env 属性からフォールバック。\n",
    "    #      - こうしておくと、ログの定義を変えてもここだけ直せばよい。\n",
    "    # ---------------------------------------------------------\n",
    "    if info is not None:\n",
    "        # 手先\n",
    "        hand_pos = np.asarray(info.get(\"hand_pos\", getattr(env, \"hand_pos\", np.array([0.0, 0.0]))))\n",
    "        hand_vel = np.asarray(info.get(\"hand_vel\", getattr(env, \"hand_vel\", np.zeros(2))))\n",
    "        hand_acc = np.asarray(info.get(\"hand_acc\", getattr(env, \"hand_acc\", np.zeros(2))))\n",
    "        hand_jerk = np.asarray(info.get(\"hand_jerk\", getattr(env, \"hand_jerk\", np.zeros(2))))\n",
    "\n",
    "        # 関節角\n",
    "        theta      = np.asarray(info.get(\"theta\",      getattr(env, \"theta\",      np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)]))))\n",
    "        theta_vel  = np.asarray(info.get(\"theta_vel\",  getattr(env, \"theta_vel\",  np.zeros(2))))\n",
    "        theta_acc  = np.asarray(info.get(\"theta_acc\",  getattr(env, \"theta_acc\",  np.zeros(2))))\n",
    "        theta_jerk = np.asarray(info.get(\"theta_jerk\", getattr(env, \"theta_jerk\", np.zeros(2))))\n",
    "\n",
    "        # トルク（action）も info が持っていればそちらを優先\n",
    "        action_info = np.asarray(info.get(\"action\", action))\n",
    "\n",
    "        # action_same: 2関節のトルクがほぼ同じかどうかを bool で保存（解析用の簡単な指標）\n",
    "        action_same = bool(info.get(\"action_same\", np.allclose(action_info[0], action_info[1], atol=1e-8)))\n",
    "    else:\n",
    "        # info に何も入っていない場合のフォールバック\n",
    "        hand_pos = np.asarray(getattr(env, \"hand_pos\", np.array([0.0, 0.0])))\n",
    "        hand_vel = np.asarray(getattr(env, \"hand_vel\", np.zeros(2)))\n",
    "        hand_acc = np.asarray(getattr(env, \"hand_acc\", np.zeros(2)))\n",
    "        hand_jerk = np.asarray(getattr(env, \"hand_jerk\", np.zeros(2)))\n",
    "\n",
    "        theta      = np.asarray(getattr(env, \"theta\",      np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)])))\n",
    "        theta_vel  = np.asarray(getattr(env, \"theta_vel\",  np.zeros(2)))\n",
    "        theta_acc  = np.asarray(getattr(env, \"theta_acc\",  np.zeros(2)))\n",
    "        theta_jerk = np.asarray(getattr(env, \"theta_jerk\", np.zeros(2)))\n",
    "\n",
    "        action_info = np.asarray(action)\n",
    "        action_same = bool(np.allclose(action_info[0], action_info[1], atol=1e-8))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4-4. 手先位置をログ（軌道図用）\n",
    "    # ---------------------------------------------------------\n",
    "    hand_x.append(float(hand_pos[0]))\n",
    "    hand_y.append(float(hand_pos[1]))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4-5. 関節状態をログ（deg に変換して保存）\n",
    "    # ---------------------------------------------------------\n",
    "    theta1_log.append(np.degrees(theta[0]))\n",
    "    theta2_log.append(np.degrees(theta[1]))\n",
    "    theta1_vel_log.append(np.degrees(theta_vel[0]))\n",
    "    theta2_vel_log.append(np.degrees(theta_vel[1]))\n",
    "    theta1_acc_log.append(np.degrees(theta_acc[0]))\n",
    "    theta2_acc_log.append(np.degrees(theta_acc[1]))\n",
    "    theta1_jerk_log.append(np.degrees(theta_jerk[0]))\n",
    "    theta2_jerk_log.append(np.degrees(theta_jerk[1]))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4-6. トルクと「action_same」フラグをログ\n",
    "    # ---------------------------------------------------------\n",
    "    torque1_log.append(float(action_info[0]))\n",
    "    torque2_log.append(float(action_info[1]))\n",
    "    action_same_log.append(bool(action_same))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4-7. 手先速度・加速度・躍度のノルムを計算してログ\n",
    "    #      - スカラー量に落とすことで、図にしたときの傾向が読み取りやすくなる。\n",
    "    # ---------------------------------------------------------\n",
    "    hand_speed     = float(np.linalg.norm(hand_vel))      # ||v||\n",
    "    hand_acc_norm  = float(np.linalg.norm(hand_acc))      # ||a||\n",
    "    hand_jerk_norm = float(np.linalg.norm(hand_jerk))     # ||jerk||\n",
    "\n",
    "    hand_speed_log.append(hand_speed)\n",
    "    hand_acc_norm_log.append(hand_acc_norm)\n",
    "    hand_jerk_norm_log.append(hand_jerk_norm)\n",
    "\n",
    "    # 終端条件（done / truncated）でループ終了\n",
    "    if bool(done) or bool(truncated):\n",
    "        break\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. ログを DataFrame にまとめて CSV 保存\n",
    "#    - ここで列名をきちんと決めておくと、後から解析スクリプトで扱いやすい。\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Time\": time_log,\n",
    "    \"HandX\": hand_x,\n",
    "    \"HandY\": hand_y,\n",
    "    \"HandSpeed (m/s)\": hand_speed_log,\n",
    "    \"HandAcc (m/s^2)\": hand_acc_norm_log,\n",
    "    \"HandJerk (m/s^3)\": hand_jerk_norm_log,\n",
    "    \"Theta1 (deg)\": theta1_log,\n",
    "    \"Theta2 (deg)\": theta2_log,\n",
    "    \"Theta1_vel (deg/s)\": theta1_vel_log,\n",
    "    \"Theta2_vel (deg/s)\": theta2_vel_log,\n",
    "    \"Theta1_acc (deg/s^2)\": theta1_acc_log,\n",
    "    \"Theta2_acc (deg/s^2)\": theta2_acc_log,\n",
    "    \"Theta1_jer (deg/s^3)\": theta1_jerk_log,\n",
    "    \"Theta2_jer (deg/s^3)\": theta2_jerk_log,\n",
    "    \"Torque1 (N·m)\": torque1_log,\n",
    "    \"Torque2 (N·m)\": torque2_log,\n",
    "    \"Action_same\": action_same_log\n",
    "})\n",
    "\n",
    "# 保存先フォルダとファイル名のベース\n",
    "output_folder = latest_log_folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "base_filename = os.path.join(output_folder, \"end_result_2joint_with_handnorms\")\n",
    "\n",
    "# CSV 保存\n",
    "csv_path = f\"{base_filename}.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"[INFO] CSV 保存: {csv_path}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. 可視化①: 手先軌道 + アーム姿勢\n",
    "#    - 到達軌道とアーム姿勢を重ねる図（論文の典型図）を自動で出力する。\n",
    "# -----------------------------------------------------------------------------\n",
    "fig1, ax1 = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# 肩は原点 (0,0) に固定\n",
    "shoulder = np.array([0.0, 0.0])\n",
    "\n",
    "# 手先軌道（青ライン）\n",
    "ax1.plot(df[\"HandX\"], df[\"HandY\"], linestyle=\"-\", linewidth=1.5, label=\"end-effector trajectory\")\n",
    "\n",
    "# start / end のマーカー\n",
    "ax1.scatter(df[\"HandX\"].iloc[0], df[\"HandY\"].iloc[0], color=\"red\", label=\"start\", zorder=5)\n",
    "ax1.scatter(df[\"HandX\"].iloc[-1], df[\"HandY\"].iloc[-1], color=\"green\", label=\"end\", zorder=5)\n",
    "\n",
    "# リンク長（env が l1, l2 を持っていればそれを優先）\n",
    "try:\n",
    "    l1 = float(getattr(env, \"l1\", (L * 0.5)))\n",
    "    l2 = float(getattr(env, \"l2\", (L * 0.5)))\n",
    "except Exception:\n",
    "    # 万が一 env が壊れていても最低限描画できるようにする\n",
    "    l1 = L * 0.5\n",
    "    l2 = L * 0.5\n",
    "\n",
    "# 関節角を rad に変換\n",
    "theta1_rad = np.radians(df[\"Theta1 (deg)\"].values)\n",
    "theta2_rad = np.radians(df[\"Theta2 (deg)\"].values)\n",
    "\n",
    "# 第1リンク末端（肘）の位置\n",
    "x1 = l1 * np.cos(theta1_rad)\n",
    "y1 = l1 * np.sin(theta1_rad)\n",
    "\n",
    "# 第2リンク末端（手先）の位置（forward kinematics）\n",
    "x2 = x1 + l2 * np.cos(theta1_rad + theta2_rad)\n",
    "y2 = y1 + l2 * np.sin(theta1_rad + theta2_rad)\n",
    "\n",
    "# FK で求めた手先とログされた HandX/Y が一致しているか軽くチェック（環境実装のバグ検出用）\n",
    "try:\n",
    "    mismatch = np.nanmax(\n",
    "        np.abs(\n",
    "            np.column_stack([df[\"HandX\"].values, df[\"HandY\"].values]) -\n",
    "            np.column_stack([x2, y2])\n",
    "        )\n",
    "    )\n",
    "    if mismatch > 1e-6:\n",
    "        print(f\"[WARN] forward kinematics and logged hand pos mismatch (max diff={mismatch:.6e})\")\n",
    "except Exception:\n",
    "    # チェックに失敗しても可視化自体は続行\n",
    "    pass\n",
    "\n",
    "# 時系列に沿って複数姿勢を薄いグレーで描画\n",
    "N_POSES = min(15, len(df))  # 図がごちゃごちゃにならないよう最大姿勢数を制限\n",
    "indices = np.linspace(0, len(df) - 1, N_POSES).astype(int)\n",
    "\n",
    "for idx in indices:\n",
    "    xs = [0.0, x1[idx], x2[idx]]\n",
    "    ys = [0.0, y1[idx], y2[idx]]\n",
    "    ax1.plot(xs, ys, linewidth=1, color=(0.5, 0.5, 0.5, 0.25), solid_capstyle='round')\n",
    "    ax1.scatter(\n",
    "        [xs[1], xs[2]],\n",
    "        [ys[1], ys[2]],\n",
    "        s=40,\n",
    "        edgecolors='k',\n",
    "        facecolors=(0.9, 0.9, 0.9, 0.6),\n",
    "        zorder=4\n",
    "    )\n",
    "\n",
    "# 強調: start / end 姿勢（色を変えて太めの線で描画）\n",
    "# start\n",
    "i0 = 0\n",
    "xs0 = [0.0, x1[i0], x2[i0]]\n",
    "ys0 = [0.0, y1[i0], y2[i0]]\n",
    "ax1.plot(xs0, ys0, linewidth=2, color='tab:red', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xs0[1], xs0[2]], [ys0[1], ys0[2]], s=80, color='tab:red', edgecolors='k', zorder=7)\n",
    "ax1.scatter(0.0, 0.0, s=100, color='black', zorder=8)  # 肩\n",
    "\n",
    "# end\n",
    "ie = len(df) - 1\n",
    "xse = [0.0, x1[ie], x2[ie]]\n",
    "yse = [0.0, y1[ie], y2[ie]]\n",
    "ax1.plot(xse, yse, linewidth=2, color='tab:green', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xse[1], xse[2]], [yse[1], yse[2]], s=100, color='tab:green', edgecolors='k', zorder=7)\n",
    "\n",
    "# 軸ラベルなどの装飾\n",
    "ax1.set_xlabel(\"X [m]\")\n",
    "ax1.set_ylabel(\"Y [m]\")\n",
    "ax1.grid(True)\n",
    "ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# アーム長に応じて描画範囲に余白を持たせる\n",
    "reach = l1 + l2\n",
    "ax1.set_xlim(-reach - 0.1 * reach, reach + 0.1 * reach)\n",
    "ax1.set_ylim(-0.1 * reach,      reach + 0.6 * reach)  # Y 方向は少し上に余裕を持たせる\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_trajectory_with_arms.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. 可視化②: 関節動的変数（角度・速度・加速度・躍度）\n",
    "#    - 1つの図に Joint1 / Joint2 を重ねて、対称性や差を確認しやすくする。\n",
    "# -----------------------------------------------------------------------------\n",
    "fig2, axes = plt.subplots(4, 1, figsize=(8, 12), sharex=True)\n",
    "\n",
    "# 角度\n",
    "axes[0].plot(df[\"Time\"], df[\"Theta1 (deg)\"], label=\"Theta1\")\n",
    "axes[0].plot(df[\"Time\"], df[\"Theta2 (deg)\"], linestyle=\"--\", label=\"Theta2\")\n",
    "axes[0].set_ylabel(\"Theta (deg)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "axes[0].set_ylim(0, 180)  # 2リンクアームの典型的な範囲に合わせておく\n",
    "\n",
    "# 角速度\n",
    "axes[1].plot(df[\"Time\"], df[\"Theta1_vel (deg/s)\"], label=\"Vel1\")\n",
    "axes[1].plot(df[\"Time\"], df[\"Theta2_vel (deg/s)\"], linestyle=\"--\", label=\"Vel2\")\n",
    "axes[1].set_ylabel(\"Velocity (deg/s)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "# 角加速度\n",
    "axes[2].plot(df[\"Time\"], df[\"Theta1_acc (deg/s^2)\"], label=\"Acc1\")\n",
    "axes[2].plot(df[\"Time\"], df[\"Theta2_acc (deg/s^2)\"], linestyle=\"--\", label=\"Acc2\")\n",
    "axes[2].set_ylabel(\"Acceleration (deg/s^2)\")\n",
    "axes[2].legend()\n",
    "axes[2].grid()\n",
    "\n",
    "# 角躍度\n",
    "axes[3].plot(df[\"Time\"], df[\"Theta1_jer (deg/s^3)\"], label=\"Jerk1\")\n",
    "axes[3].plot(df[\"Time\"], df[\"Theta2_jer (deg/s^3)\"], linestyle=\"--\", label=\"Jerk2\")\n",
    "axes[3].set_ylabel(\"Jerk (deg/s^3)\")\n",
    "axes[3].set_xlabel(\"Time (s)\")\n",
    "axes[3].legend()\n",
    "axes[3].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_joint_dynamics.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. 可視化③: 手先ノルム（速度・加速度・躍度）\n",
    "#    - 手先がどのくらい滑らかに動いているかを俯瞰するための図。\n",
    "# -----------------------------------------------------------------------------\n",
    "fig3, ax3 = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "ax3[0].plot(df[\"Time\"], df[\"HandSpeed (m/s)\"], label=\"Hand Speed ||v||\")\n",
    "ax3[0].set_ylabel(\"Speed (m/s)\")\n",
    "ax3[0].legend()\n",
    "ax3[0].grid()\n",
    "\n",
    "ax3[1].plot(df[\"Time\"], df[\"HandAcc (m/s^2)\"], label=\"Hand Acc ||a||\")\n",
    "ax3[1].set_ylabel(\"Acc (m/s^2)\")\n",
    "ax3[1].legend()\n",
    "ax3[1].grid()\n",
    "\n",
    "ax3[2].plot(df[\"Time\"], df[\"HandJerk (m/s^3)\"], label=\"Hand Jerk ||jerk||\")\n",
    "ax3[2].set_ylabel(\"Jerk (m/s^3)\")\n",
    "ax3[2].set_xlabel(\"Time (s)\")\n",
    "ax3[2].legend()\n",
    "ax3[2].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_hand_norms.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9. 可視化④: トルクの推移と action_same の確認\n",
    "#    - control パターン（左右対称かどうかなど）をざっくり確認するための補助図。\n",
    "# -----------------------------------------------------------------------------\n",
    "fig4, ax4 = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# トルク\n",
    "ax4[0].plot(df[\"Time\"], df[\"Torque1 (N·m)\"], label=\"Torque1\")\n",
    "ax4[0].plot(df[\"Time\"], df[\"Torque2 (N·m)\"], linestyle=\"--\", label=\"Torque2\")\n",
    "ax4[0].set_ylabel(\"Torque (N·m)\")\n",
    "ax4[0].legend()\n",
    "ax4[0].grid()\n",
    "\n",
    "# action_same を 0/1 にして描画（2関節トルクがほぼ同じタイミングがどこか一目で分かる）\n",
    "ax4[1].plot(df[\"Time\"], df[\"Action_same\"].astype(int), label=\"Action_same (int)\")\n",
    "ax4[1].set_ylabel(\"Action_same\")\n",
    "ax4[1].set_xlabel(\"Time (s)\")\n",
    "ax4[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_torques.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベストモデルの活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# best_model ロード ＋ 単一エピソード評価・ログ・可視化スクリプト\n",
    "# =============================================================================\n",
    "# 役割:\n",
    "#   - ./logs 以下の「最新の学習ログフォルダ」から best_model.zip（もしくは最新の .zip）を探す\n",
    "#   - SB3 のどのアルゴリズムか分からない状態からモデルをロードする\n",
    "#   - ノートブック側で定義済みの env を使って 1 エピソードだけロールアウト\n",
    "#   - 2 関節アームの状態・手先ノルム・トルクを CSV に保存し、図を PNG として保存する\n",
    "#\n",
    "# 設計方針:\n",
    "#   - 「最新の実験結果を手早く可視化する」ことを目的に、\n",
    "#       1) 最新 logs フォルダの自動検出\n",
    "#       2) best_model.zip の自動検出（なければ最新 .zip）\n",
    "#       3) Gym / Gymnasium 両対応の reset / step ハンドリング\n",
    "#       4) env / info の仕様が多少変わっても壊れないフォールバック\n",
    "#     を意識して実装している。\n",
    "#   - 角度は人間が読みやすい deg でログし、内部計算のみ rad を用いる。\n",
    "#   - 「なぜその値を優先するか（info を優先、env をフォールバック）」など、\n",
    "#     後から読んだときに迷わないようにコメントで方針を明示する。\n",
    "#\n",
    "# 関連コンポーネント:\n",
    "#   - env       : TwoJointReachingEnv 等の 2 関節アーム環境（ノートブック側で定義済み）\n",
    "#   - PPO/SAC… : Stable-Baselines3 の各アルゴリズムクラス（PPO, SAC, TD3, DDPG, A2C）\n",
    "#   - THETA_INIT, DT, L: 環境に関連するパラメータ（ノートブックで定義済み想定）\n",
    "#   - ./logs/   : 学習時に保存しているフォルダ\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) 最新のログフォルダを見つけ、best_model.zip を探す\n",
    "#    - 「どの実験結果を可視化するか」を人手ではなく自動で選びたいので、\n",
    "#      更新日時が最も新しい logs サブフォルダを対象にする。\n",
    "# -----------------------------------------------------------------------------\n",
    "log_base = \"./logs\"\n",
    "log_dirs = [d for d in glob.glob(os.path.join(log_base, \"*\")) if os.path.isdir(d)]\n",
    "if not log_dirs:\n",
    "    raise FileNotFoundError(\"logs フォルダにサブディレクトリがありません。学習結果を保存してください。\")\n",
    "\n",
    "latest_log_dir = max(log_dirs, key=os.path.getmtime)\n",
    "\n",
    "# best_model.zip がある想定。なければフォールバックとして、フォルダ内の最新 .zip を採用する。\n",
    "best_model_path = os.path.join(latest_log_dir, \"best_model.zip\")\n",
    "if not os.path.exists(best_model_path):\n",
    "    # フォールバック: 同ディレクトリにある .zip をすべて列挙し、最も新しいものを選ぶ\n",
    "    zips = sorted(glob.glob(os.path.join(latest_log_dir, \"*.zip\")), key=os.path.getmtime)\n",
    "    if not zips:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{best_model_path} が存在せず、{latest_log_dir} に .zip ファイルも見つかりません。\"\n",
    "        )\n",
    "    best_model_path = zips[-1]\n",
    "\n",
    "print(f\"✅ 最新のモデルファイルを使用: {best_model_path}\")\n",
    "# 例: best_model_path = \"logs/2025_11_19_22_23_03/best_model.zip\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 便利関数: ラップを外して base 環境を得る\n",
    "#   - VecEnv, Monitor, TimeLimit など SB3 由来のラッパをすべて剥がし、\n",
    "#     本体の環境インスタンスにアクセスできるようにする。\n",
    "#   - 内部状態 (theta, hand_pos など) を直接参照するために必要。\n",
    "# -----------------------------------------------------------------------------\n",
    "def unwrap_env(e):\n",
    "    \"\"\"\n",
    "    ラッパー (VecEnv, Monitor, DummyVecEnv など) があれば中身の base 環境を返す。\n",
    "    再帰的に unwrap するので、どんなラップ構造でも「一番内側の環境」に到達できる。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # VecEnv 系 (DummyVecEnv, SubprocVecEnv) は .envs に環境リストを持っている\n",
    "        if hasattr(e, \"envs\") and isinstance(getattr(e, \"envs\"), (list, tuple)) and len(e.envs) > 0:\n",
    "            return unwrap_env(e.envs[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Monitor / TimeLimit 等は .env に一つ内側を保持していることが多い\n",
    "        if hasattr(e, \"env\") and e.env is not None:\n",
    "            return unwrap_env(e.env)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # これ以上 unwrap できなければ、それが base 環境とみなす\n",
    "    return e\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) best_model をロード（どのアルゴリズムか分からない場合を想定）\n",
    "#   - 学習時にどの SB3 アルゴリズムを使ったかを覚えていなくても、\n",
    "#     代表的なクラスを順番に試し、ロードに成功したものを採用する。\n",
    "# -----------------------------------------------------------------------------\n",
    "loaded_model = None\n",
    "\n",
    "# 試行順: PPO, SAC, TD3, DDPG, A2C\n",
    "for alg in (PPO, SAC, TD3, DDPG, A2C):\n",
    "    try:\n",
    "        loaded_model = alg.load(best_model_path)\n",
    "        print(f\"✅ モデルを {alg.__name__} 形式でロードしました。\")\n",
    "        break\n",
    "    except Exception:\n",
    "        # 読み込み失敗は無視して次のアルゴリズムを試す\n",
    "        continue\n",
    "\n",
    "if loaded_model is None:\n",
    "    raise RuntimeError(\"モデルのロードに失敗しました。対応する SB3 アルゴリズムを確認してください。\")\n",
    "\n",
    "best_model = loaded_model  # 以降で使う変数名を統一\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) env のリセットと初期観測の取得\n",
    "#   - ノートブック側で env が既に定義されている前提。\n",
    "#   - Gym / Gymnasium で reset の返り値が変わっているので両対応する。\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    obs_reset = env.reset()\n",
    "    # Gymnasium: (obs, info), 旧 Gym: obs のみ\n",
    "    if isinstance(obs_reset, tuple) and len(obs_reset) >= 1:\n",
    "        obs = obs_reset[0]\n",
    "    else:\n",
    "        obs = obs_reset\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"env.reset() に失敗しました。env が正しく定義・初期化されているか確認してください。\"\n",
    "    ) from e\n",
    "\n",
    "# 内部状態を読むため、ラップを剥がした base_env を取得\n",
    "base_env = unwrap_env(env)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) ログ用のリストを初期化（2 関節アーム＋手先ノルム＋トルク）\n",
    "#   - time_log を軸に、各物理量を列ごとのリストとして蓄積していく。\n",
    "# -----------------------------------------------------------------------------\n",
    "time_log = []\n",
    "hand_x, hand_y = [], []\n",
    "\n",
    "# 関節ごとのログ（joint1 / joint2）\n",
    "theta1_log, theta2_log = [], []\n",
    "theta1_vel_log, theta2_vel_log = [], []\n",
    "theta1_acc_log, theta2_acc_log = [], []\n",
    "theta1_jerk_log, theta2_jerk_log = [], []\n",
    "\n",
    "# トルクログ [N·m]\n",
    "torque1_log, torque2_log = [], []\n",
    "action_same_log = []  # 2 関節トルクがほぼ同じかどうか\n",
    "\n",
    "# 手先ノルムログ（速度, 加速度, 躍度）\n",
    "hand_speed_log = []\n",
    "hand_acc_norm_log = []\n",
    "hand_jerk_norm_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) 初期状態を取得して、時刻 t=0 の行をログに追加する\n",
    "#   - reset 直後の状態も後から解析したいので、最初のステップから記録する。\n",
    "#   - base_env が状態を持っていない場合に備え、フォールバックを用意。\n",
    "# -----------------------------------------------------------------------------\n",
    "t = 0.0\n",
    "time_log.append(t)\n",
    "\n",
    "try:\n",
    "    base = base_env  # alias\n",
    "\n",
    "    # theta 系（存在しなければ THETA_INIT を使う）\n",
    "    init_theta = np.asarray(\n",
    "        getattr(base, \"theta\", np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)]))\n",
    "    ).reshape(-1)[:2]\n",
    "    init_theta_vel = np.asarray(\n",
    "        getattr(base, \"theta_vel\", np.zeros(2))\n",
    "    ).reshape(-1)[:2]\n",
    "    init_theta_acc = np.asarray(\n",
    "        getattr(base, \"theta_acc\", np.zeros(2))\n",
    "    ).reshape(-1)[:2]\n",
    "    init_theta_jerk = np.asarray(\n",
    "        getattr(base, \"theta_jerk\", np.zeros(2))\n",
    "    ).reshape(-1)[:2]\n",
    "\n",
    "    # 手先位置: hand_pos があればそれを優先。\n",
    "    if hasattr(base, \"hand_pos\"):\n",
    "        init_hand = np.asarray(getattr(base, \"hand_pos\")).astype(float)\n",
    "    else:\n",
    "        # なければ forward_kinematics か、リンク長 L から単純な FK を行う\n",
    "        if hasattr(base, \"forward_kinematics\"):\n",
    "            init_hand = base.forward_kinematics(init_theta)\n",
    "        else:\n",
    "            L_val = float(getattr(base, \"l\", getattr(base, \"l1\", 1.0) + getattr(base, \"l2\", 0.0)))\n",
    "            init_hand = L_val * np.array([np.cos(init_theta[0]), np.sin(init_theta[0])])\n",
    "except Exception:\n",
    "    # base_env に期待した属性がない場合のフォールバック\n",
    "    init_theta = np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)])\n",
    "    init_theta_vel = np.zeros(2)\n",
    "    init_theta_acc = np.zeros(2)\n",
    "    init_theta_jerk = np.zeros(2)\n",
    "    try:\n",
    "        init_hand = np.asarray(getattr(base_env, \"hand_pos\", np.array([0.0, 0.0])))\n",
    "    except Exception:\n",
    "        init_hand = np.array([0.0, 0.0])\n",
    "\n",
    "# --- 初期値をログに追加（角度は deg、位置は SI 単位） ---\n",
    "hand_x.append(float(init_hand[0]))\n",
    "hand_y.append(float(init_hand[1]))\n",
    "theta1_log.append(np.degrees(init_theta[0]))\n",
    "theta2_log.append(np.degrees(init_theta[1]))\n",
    "theta1_vel_log.append(np.degrees(init_theta_vel[0]))\n",
    "theta2_vel_log.append(np.degrees(init_theta_vel[1]))\n",
    "theta1_acc_log.append(np.degrees(init_theta_acc[0]))\n",
    "theta2_acc_log.append(np.degrees(init_theta_acc[1]))\n",
    "theta1_jerk_log.append(np.degrees(init_theta_jerk[0]))\n",
    "theta2_jerk_log.append(np.degrees(init_theta_jerk[1]))\n",
    "\n",
    "# 手先ノルム初期値（速度, 加速度, 躍度）\n",
    "init_hand_vel = np.asarray(getattr(base_env, \"hand_vel\", np.zeros(2))).reshape(-1)[:2]\n",
    "init_hand_acc = np.asarray(getattr(base_env, \"hand_acc\", np.zeros(2))).reshape(-1)[:2]\n",
    "init_hand_jerk = np.asarray(getattr(base_env, \"hand_jerk\", np.zeros(2))).reshape(-1)[:2]\n",
    "\n",
    "hand_speed_log.append(float(np.linalg.norm(init_hand_vel)))\n",
    "hand_acc_norm_log.append(float(np.linalg.norm(init_hand_acc)))\n",
    "hand_jerk_norm_log.append(float(np.linalg.norm(init_hand_jerk)))\n",
    "\n",
    "# t=0 は「まだ制御していない」のでトルクは NaN にしておく\n",
    "torque1_log.append(np.nan)\n",
    "torque2_log.append(np.nan)\n",
    "action_same_log.append(False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) シミュレーション実行ループ（best_model を用いて行動を生成）\n",
    "#   - Gym / Gymnasium 両対応の step 返り値をハンドリング。\n",
    "#   - 各ステップで手先・関節・トルクをログに追加。\n",
    "# -----------------------------------------------------------------------------\n",
    "MAX_STEPS_RUN = 2000  # 可視化用途なので、必要に応じてここを増減させる\n",
    "\n",
    "for step_i in range(MAX_STEPS_RUN):\n",
    "    # 6-1. モデルから関節トルク [N·m] を取得\n",
    "    action, _ = best_model.predict(obs, deterministic=True)\n",
    "    action = np.asarray(action).reshape(-1)\n",
    "    if action.shape[0] != 2:\n",
    "        raise RuntimeError(\n",
    "            f\"predict returned action with wrong shape {action.shape}. expected (2,) for 2-joint environment.\"\n",
    "        )\n",
    "\n",
    "    # 6-2. env.step 実行（Gym / Gymnasium の返り値に対応）\n",
    "    next_return = env.step(action)\n",
    "\n",
    "    if isinstance(next_return, tuple) and len(next_return) == 5:\n",
    "        # Gymnasium: (obs, reward, terminated, truncated, info)\n",
    "        obs, reward, terminated, truncated, info = next_return\n",
    "        done = bool(terminated or truncated)\n",
    "    elif isinstance(next_return, tuple) and len(next_return) == 4:\n",
    "        # 旧 Gym: (obs, reward, done, info)\n",
    "        obs, reward, done, info = next_return\n",
    "        terminated = bool(done)\n",
    "        truncated = False\n",
    "    else:\n",
    "        raise RuntimeError(\"env.step() の返り値の形式が不明です。\")\n",
    "\n",
    "    # 6-3. 時刻を進める\n",
    "    dt = getattr(base_env, \"dt\", None)\n",
    "    if dt is None:\n",
    "        # env が dt を持っていない場合は DT を参照、それもなければ 0.01 [s]\n",
    "        try:\n",
    "            dt = float(DT)\n",
    "        except Exception:\n",
    "            dt = 0.01\n",
    "    t += dt\n",
    "    time_log.append(t)\n",
    "\n",
    "    # 6-4. info を優先して状態を取得（info にその時点の値を詰めている設計のため）\n",
    "    if info is not None:\n",
    "        # 手先位置\n",
    "        if \"hand_pos\" in info:\n",
    "            hand_pos = np.asarray(info[\"hand_pos\"]).reshape(-1)[:2]\n",
    "        else:\n",
    "            # hand_pos がない場合は theta から順運動学で推定\n",
    "            if hasattr(base_env, \"forward_kinematics\"):\n",
    "                try:\n",
    "                    theta_info = np.asarray(\n",
    "                        info.get(\"theta\", getattr(base_env, \"theta\", init_theta))\n",
    "                    ).reshape(-1)[:2]\n",
    "                    hand_pos = base_env.forward_kinematics(theta_info)\n",
    "                except Exception:\n",
    "                    hand_pos = np.array([np.nan, np.nan])\n",
    "            else:\n",
    "                # forward_kinematics がなくても最低限の位置を推定できるようにしておく\n",
    "                try:\n",
    "                    L_val = float(\n",
    "                        getattr(base_env, \"l\", getattr(base_env, \"l1\", 1.0) + getattr(base_env, \"l2\", 0.0))\n",
    "                    )\n",
    "                    theta0 = float(\n",
    "                        np.asarray(info.get(\"theta\", getattr(base_env, \"theta\", init_theta)))[0]\n",
    "                    )\n",
    "                    hand_pos = L_val * np.array([np.cos(theta0), np.sin(theta0)])\n",
    "                except Exception:\n",
    "                    hand_pos = np.array([np.nan, np.nan])\n",
    "\n",
    "        # 関節角・速度・加速度・躍度\n",
    "        theta_arr = np.asarray(\n",
    "            info.get(\"theta\", getattr(base_env, \"theta\", init_theta))\n",
    "        ).reshape(-1)[:2]\n",
    "        theta_vel_arr = np.asarray(\n",
    "            info.get(\"theta_vel\", getattr(base_env, \"theta_vel\", init_theta_vel))\n",
    "        ).reshape(-1)[:2]\n",
    "        theta_acc_arr = np.asarray(\n",
    "            info.get(\"theta_acc\", getattr(base_env, \"theta_acc\", init_theta_acc))\n",
    "        ).reshape(-1)[:2]\n",
    "        theta_jerk_arr = np.asarray(\n",
    "            info.get(\"theta_jerk\", getattr(base_env, \"theta_jerk\", init_theta_jerk))\n",
    "        ).reshape(-1)[:2]\n",
    "\n",
    "        # 手先速度・加速度・躍度\n",
    "        hand_vel = np.asarray(\n",
    "            info.get(\"hand_vel\", getattr(base_env, \"hand_vel\", np.zeros(2)))\n",
    "        ).reshape(-1)[:2]\n",
    "        hand_acc = np.asarray(\n",
    "            info.get(\"hand_acc\", getattr(base_env, \"hand_acc\", np.zeros(2)))\n",
    "        ).reshape(-1)[:2]\n",
    "        hand_jerk = np.asarray(\n",
    "            info.get(\"hand_jerk\", getattr(base_env, \"hand_jerk\", np.zeros(2)))\n",
    "        ).reshape(-1)[:2]\n",
    "    else:\n",
    "        # info が空の場合は base_env の属性を直接読む\n",
    "        try:\n",
    "            theta_arr = np.asarray(getattr(base_env, \"theta\", init_theta)).reshape(-1)[:2]\n",
    "            if hasattr(base_env, \"forward_kinematics\"):\n",
    "                hand_pos = base_env.forward_kinematics(theta_arr)\n",
    "            else:\n",
    "                L_val = float(\n",
    "                    getattr(base_env, \"l\", getattr(base_env, \"l1\", 1.0) + getattr(base_env, \"l2\", 0.0))\n",
    "                )\n",
    "                hand_pos = L_val * np.array([np.cos(theta_arr[0]), np.sin(theta_arr[0])])\n",
    "\n",
    "            theta_vel_arr = np.asarray(getattr(base_env, \"theta_vel\", init_theta_vel)).reshape(-1)[:2]\n",
    "            theta_acc_arr = np.asarray(getattr(base_env, \"theta_acc\", init_theta_acc)).reshape(-1)[:2]\n",
    "            theta_jerk_arr = np.asarray(getattr(base_env, \"theta_jerk\", init_theta_jerk)).reshape(-1)[:2]\n",
    "\n",
    "            hand_vel = np.asarray(getattr(base_env, \"hand_vel\", np.zeros(2))).reshape(-1)[:2]\n",
    "            hand_acc = np.asarray(getattr(base_env, \"hand_acc\", np.zeros(2))).reshape(-1)[:2]\n",
    "            hand_jerk = np.asarray(getattr(base_env, \"hand_jerk\", np.zeros(2))).reshape(-1)[:2]\n",
    "        except Exception:\n",
    "            # base_env に必要な情報がない場合の最後のフォールバック\n",
    "            theta_arr = init_theta\n",
    "            theta_vel_arr = init_theta_vel\n",
    "            theta_acc_arr = init_theta_acc\n",
    "            theta_jerk_arr = init_theta_jerk\n",
    "            hand_pos = np.array([np.nan, np.nan])\n",
    "            hand_vel = np.zeros(2)\n",
    "            hand_acc = np.zeros(2)\n",
    "            hand_jerk = np.zeros(2)\n",
    "\n",
    "    # 6-5. ログに追加（位置・角度・トルク・ノルム）\n",
    "    hand_x.append(float(hand_pos[0]))\n",
    "    hand_y.append(float(hand_pos[1]))\n",
    "    theta1_log.append(np.degrees(theta_arr[0]))\n",
    "    theta2_log.append(np.degrees(theta_arr[1]))\n",
    "    theta1_vel_log.append(np.degrees(theta_vel_arr[0]))\n",
    "    theta2_vel_log.append(np.degrees(theta_vel_arr[1]))\n",
    "    theta1_acc_log.append(np.degrees(theta_acc_arr[0]))\n",
    "    theta2_acc_log.append(np.degrees(theta_acc_arr[1]))\n",
    "    theta1_jerk_log.append(np.degrees(theta_jerk_arr[0]))\n",
    "    theta2_jerk_log.append(np.degrees(theta_jerk_arr[1]))\n",
    "\n",
    "    # トルク記録 [N·m]（info[\"action\"] があればそれを「実際にかけた値」として優先）\n",
    "    if info is not None and \"action\" in info:\n",
    "        torque_vec = np.asarray(info[\"action\"]).reshape(-1)\n",
    "    else:\n",
    "        torque_vec = action  # env.step に渡したもの\n",
    "\n",
    "    torque1_log.append(float(torque_vec[0]))\n",
    "    torque2_log.append(float(torque_vec[1]))\n",
    "    action_same_log.append(bool(np.allclose(torque_vec[0], torque_vec[1], atol=1e-6)))\n",
    "\n",
    "    # 手先ノルム（SI 単位）を計算してログ\n",
    "    hand_speed_log.append(float(np.linalg.norm(hand_vel)))\n",
    "    hand_acc_norm_log.append(float(np.linalg.norm(hand_acc)))\n",
    "    hand_jerk_norm_log.append(float(np.linalg.norm(hand_jerk)))\n",
    "\n",
    "    # 終端条件（terminated / truncated）または異常な長さで break\n",
    "    if bool(terminated) or bool(truncated) or (len(time_log) > 100000):\n",
    "        break\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) DataFrame 作成 & CSV 保存（2関節・手先ノルム・トルク列）\n",
    "#   - 解析用にすべてのログを 1 つの CSV にまとめる。\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Time\": time_log,\n",
    "    \"HandX\": hand_x,\n",
    "    \"HandY\": hand_y,\n",
    "    \"HandSpeed (m/s)\": hand_speed_log,\n",
    "    \"HandAcc (m/s^2)\": hand_acc_norm_log,\n",
    "    \"HandJerk (m/s^3)\": hand_jerk_norm_log,\n",
    "    \"Theta1 (deg)\": theta1_log,\n",
    "    \"Theta2 (deg)\": theta2_log,\n",
    "    \"Theta1_vel (deg/s)\": theta1_vel_log,\n",
    "    \"Theta2_vel (deg/s)\": theta2_vel_log,\n",
    "    \"Theta1_acc (deg/s^2)\": theta1_acc_log,\n",
    "    \"Theta2_acc (deg/s^2)\": theta2_acc_log,\n",
    "    \"Theta1_jer (deg/s^3)\": theta1_jerk_log,\n",
    "    \"Theta2_jer (deg/s^3)\": theta2_jerk_log,\n",
    "    \"Torque1 (N·m)\": torque1_log,\n",
    "    \"Torque2 (N·m)\": torque2_log,\n",
    "    \"Action_same\": action_same_log\n",
    "})\n",
    "\n",
    "output_dir = latest_log_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_path = os.path.join(output_dir, \"best_result_2joint_with_initial_and_handnorms.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV 保存: {csv_path}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) プロット群の生成: 手先軌跡、関節ダイナミクス、手先ノルム、トルク\n",
    "#   - 図の出力パスは output_dir にまとめて保存する。\n",
    "#   - 研究ノートにそのまま貼れるクオリティを想定している。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# ===== 8-1. 手先軌道 + アーム姿勢を重ねる =====\n",
    "fig1, ax1 = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax1.plot(df[\"HandX\"], df[\"HandY\"], linestyle=\"-\", linewidth=1.5, label=\"end-effector trajectory\")\n",
    "ax1.scatter(df[\"HandX\"].iloc[0], df[\"HandY\"].iloc[0], color=\"red\", label=\"start\", zorder=5)\n",
    "ax1.scatter(df[\"HandX\"].iloc[-1], df[\"HandY\"].iloc[-1], color=\"green\", label=\"end\", zorder=5)\n",
    "\n",
    "# リンク長（env から取得できなければ L を半分ずつ使う）\n",
    "try:\n",
    "    l1 = float(getattr(env, \"l1\", (L * 0.5)))\n",
    "    l2 = float(getattr(env, \"l2\", (L * 0.5)))\n",
    "except Exception:\n",
    "    l1 = L * 0.5\n",
    "    l2 = L * 0.5\n",
    "\n",
    "theta1_rad = np.radians(df[\"Theta1 (deg)\"].values)\n",
    "theta2_rad = np.radians(df[\"Theta2 (deg)\"].values)\n",
    "\n",
    "x1 = l1 * np.cos(theta1_rad)\n",
    "y1 = l1 * np.sin(theta1_rad)\n",
    "x2 = x1 + l2 * np.cos(theta1_rad + theta2_rad)\n",
    "y2 = y1 + l2 * np.sin(theta1_rad + theta2_rad)\n",
    "\n",
    "# FK 計算結果と HandX/Y の乖離をチェック（環境バグの早期発見用）\n",
    "try:\n",
    "    mismatch = np.nanmax(\n",
    "        np.abs(\n",
    "            np.column_stack([df[\"HandX\"].values, df[\"HandY\"].values]) -\n",
    "            np.column_stack([x2, y2])\n",
    "        )\n",
    "    )\n",
    "    if mismatch > 1e-6:\n",
    "        print(f\"[WARN] forward kinematics and logged hand pos mismatch (max diff={mismatch:.6e})\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# アーム姿勢を時系列に沿って薄いグレーで重ね描画\n",
    "N_POSES = min(15, len(df))\n",
    "indices = np.linspace(0, len(df) - 1, N_POSES).astype(int)\n",
    "\n",
    "for idx in indices:\n",
    "    xs = [0.0, x1[idx], x2[idx]]\n",
    "    ys = [0.0, y1[idx], y2[idx]]\n",
    "    ax1.plot(xs, ys, linewidth=1, color=(0.5, 0.5, 0.5, 0.25), solid_capstyle='round')\n",
    "    ax1.scatter(\n",
    "        [xs[1], xs[2]],\n",
    "        [ys[1], ys[2]],\n",
    "        s=40,\n",
    "        edgecolors='k',\n",
    "        facecolors=(0.9, 0.9, 0.9, 0.6),\n",
    "        zorder=4\n",
    "    )\n",
    "\n",
    "# 強調: start / end 姿勢\n",
    "i0 = 0\n",
    "xs0 = [0.0, x1[i0], x2[i0]]\n",
    "ys0 = [0.0, y1[i0], y2[i0]]\n",
    "ax1.plot(xs0, ys0, linewidth=2, color='tab:red', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xs0[1], xs0[2]], [ys0[1], ys0[2]], s=80, color='tab:red', edgecolors='k', zorder=7)\n",
    "ax1.scatter(0.0, 0.0, s=100, color='black', zorder=8)\n",
    "\n",
    "ie = len(df) - 1\n",
    "xse = [0.0, x1[ie], x2[ie]]\n",
    "yse = [0.0, y1[ie], y2[ie]]\n",
    "ax1.plot(xse, yse, linewidth=2, color='tab:green', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xse[1], xse[2]], [yse[1], yse[2]], s=100, color='tab:green', edgecolors='k', zorder=7)\n",
    "\n",
    "ax1.set_xlabel(\"X [m]\")\n",
    "ax1.set_ylabel(\"Y [m]\")\n",
    "ax1.grid(True)\n",
    "ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "reach = l1 + l2\n",
    "ax1.set_xlim(-reach - 0.1 * reach, reach + 0.1 * reach)\n",
    "ax1.set_ylim(-0.1 * reach, reach + 0.6 * reach)\n",
    "\n",
    "base_filename = os.path.join(output_dir, \"best_result_2joint_with_handnorms\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_trajectory_with_arms.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ===== 8-2. 関節動的変数（角度・速度・加速度・躍度） =====\n",
    "fig2, axes = plt.subplots(4, 1, figsize=(8, 12), sharex=True)\n",
    "\n",
    "axes[0].plot(df[\"Time\"], df[\"Theta1 (deg)\"], label=\"Theta1\")\n",
    "axes[0].plot(df[\"Time\"], df[\"Theta2 (deg)\"], linestyle=\"--\", label=\"Theta2\")\n",
    "axes[0].set_ylabel(\"Theta (deg)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "axes[0].set_ylim(0, 180)\n",
    "\n",
    "axes[1].plot(df[\"Time\"], df[\"Theta1_vel (deg/s)\"], label=\"Vel1\")\n",
    "axes[1].plot(df[\"Time\"], df[\"Theta2_vel (deg/s)\"], linestyle=\"--\", label=\"Vel2\")\n",
    "axes[1].set_ylabel(\"Velocity (deg/s)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "axes[2].plot(df[\"Time\"], df[\"Theta1_acc (deg/s^2)\"], label=\"Acc1\")\n",
    "axes[2].plot(df[\"Time\"], df[\"Theta2_acc (deg/s^2)\"], linestyle=\"--\", label=\"Acc2\")\n",
    "axes[2].set_ylabel(\"Acceleration (deg/s^2)\")\n",
    "axes[2].legend()\n",
    "axes[2].grid()\n",
    "\n",
    "axes[3].plot(df[\"Time\"], df[\"Theta1_jer (deg/s^3)\"], label=\"Jerk1\")\n",
    "axes[3].plot(df[\"Time\"], df[\"Theta2_jer (deg/s^3)\"], linestyle=\"--\", label=\"Jerk2\")\n",
    "axes[3].set_ylabel(\"Jerk (deg/s^3)\")\n",
    "axes[3].set_xlabel(\"Time (s)\")\n",
    "axes[3].legend()\n",
    "axes[3].grid()\n",
    "\n",
    "png_dynamics = os.path.join(output_dir, \"best_result_2joint_joint_dynamics.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_dynamics, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ 関節動的変数プロット保存: {png_dynamics}\")\n",
    "\n",
    "# ===== 8-3. 手先ノルム（速度・加速度・躍度） =====\n",
    "fig3, ax3 = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "ax3[0].plot(df[\"Time\"], df[\"HandSpeed (m/s)\"], label=\"||v||\")\n",
    "ax3[0].set_ylabel(\"Speed (m/s)\")\n",
    "ax3[0].legend()\n",
    "ax3[0].grid()\n",
    "\n",
    "ax3[1].plot(df[\"Time\"], df[\"HandAcc (m/s^2)\"], label=\"||a||\")\n",
    "ax3[1].set_ylabel(\"Acc (m/s^2)\")\n",
    "ax3[1].legend()\n",
    "ax3[1].grid()\n",
    "\n",
    "ax3[2].plot(df[\"Time\"], df[\"HandJerk (m/s^3)\"], label=\"||jerk||\")\n",
    "ax3[2].set_ylabel(\"Jerk (m/s^3)\")\n",
    "ax3[2].set_xlabel(\"Time (s)\")\n",
    "ax3[2].legend()\n",
    "ax3[2].grid()\n",
    "\n",
    "png_handnorms = os.path.join(output_dir, \"best_result_2joint_hand_norms.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_handnorms, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ 手先ノルムプロット保存: {png_handnorms}\")\n",
    "\n",
    "# ===== 8-4. トルクの遷移と action_same フラグ =====\n",
    "fig4, ax4 = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "ax4[0].plot(df[\"Time\"], df[\"Torque1 (N·m)\"], label=\"Torque1\")\n",
    "ax4[0].plot(df[\"Time\"], df[\"Torque2 (N·m)\"], linestyle=\"--\", label=\"Torque2\")\n",
    "ax4[0].set_ylabel(\"Torque (N·m)\")\n",
    "ax4[0].legend()\n",
    "ax4[0].grid()\n",
    "\n",
    "ax4[1].plot(df[\"Time\"], df[\"Action_same\"].astype(int), label=\"Action_same\")\n",
    "ax4[1].set_ylabel(\"Action_same\")\n",
    "ax4[1].set_xlabel(\"Time (s)\")\n",
    "ax4[1].grid()\n",
    "\n",
    "png_torques = os.path.join(output_dir, \"best_result_2joint_torques.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_torques, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ トルクプロット保存: {png_torques}\")\n",
    "\n",
    "# ===== 完了メッセージ =====\n",
    "print(\"=== 実行完了: 2関節対応の評価ログ & プロットを保存しました ===\")\n",
    "print(f\"出力フォルダ: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エピソードごとの様々な報酬の推移などの多指標をプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# episode_full_metrics.csv から学習ログをまとめて可視化するスクリプト\n",
    "# =============================================================================\n",
    "# 役割:\n",
    "#   - ./logs 以下の「最新ログフォルダ」から episode_full_metrics.csv を自動検出\n",
    "#   - 学習エピソードごとの各種指標（報酬、距離・躍度コスト、時間ボーナス、jerk_sum など）を読み込む\n",
    "#   - 指標をグルーピングしてまとめプロット＋個別プロットを自動生成し、PNG 画像として保存する\n",
    "#   - すべてのプロットについて「生データ版」と「移動平均版（rolling mean）」を出力する\n",
    "#\n",
    "# 設計方針:\n",
    "#   - 「あとから列が増えたり減ったりしても壊れない」ことを重視し、\n",
    "#       - 列が存在したものだけプロット\n",
    "#       - グループ定義も存在チェックの上で描画\n",
    "#   - X 軸は基本的に episode 番号だが、episode 列がなくてもインデックスで代用できる。\n",
    "#   - 移動平均窓幅 MOVING_AVG_WINDOW を冒頭でまとめて設定し、ノートブック上で調整しやすくする。\n",
    "#\n",
    "# 関連コンポーネント:\n",
    "#   - ./logs/****/episode_full_metrics.csv : 学習ループから出力されるメトリクス CSV\n",
    "#   - label_unit_map : CSV 列名 → プロット用のラベル・単位への変換マップ\n",
    "#   - groups         : まとめて重ねて見たい指標グループの定義\n",
    "# =============================================================================\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12  # スライド・論文でも見やすいフォントサイズ\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 設定: 移動平均窓幅\n",
    "#   - 学習がノイジーなときにトレンドを見やすくするためのパラメータ。\n",
    "#   - エピソード数に応じて 10, 50, 100 などに変更して使う。\n",
    "# -----------------------------------------------------------------------------\n",
    "MOVING_AVG_WINDOW = 10\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) 最新ログフォルダの自動検出\n",
    "#   - ./logs 配下のサブディレクトリを列挙し、更新日時が最も新しいものを使用する。\n",
    "# -----------------------------------------------------------------------------\n",
    "logs_dir = \"./logs\"\n",
    "log_folders = [\n",
    "    os.path.join(logs_dir, d)\n",
    "    for d in os.listdir(logs_dir)\n",
    "    if os.path.isdir(os.path.join(logs_dir, d))\n",
    "]\n",
    "if not log_folders:\n",
    "    raise FileNotFoundError(\"`./logs` 内にフォルダが見つかりません。学習ログがあるディレクトリを確認してください。\")\n",
    "\n",
    "latest_log_folder = max(log_folders, key=os.path.getmtime)\n",
    "print(f\"[INFO] 最新ログフォルダ: {latest_log_folder}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) episode_full_metrics.csv のパス決定\n",
    "#   - デフォルト名が見つからない場合は、名前が少し違っていても使えるように近い名前を探索する。\n",
    "# -----------------------------------------------------------------------------\n",
    "csv_filename = \"episode_full_metrics.csv\"\n",
    "csv_path = os.path.join(latest_log_folder, csv_filename)\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    # ファイル名が微妙に違う場合に備えて、「episode」「full」を含む CSV を候補として探す\n",
    "    candidates = glob.glob(os.path.join(latest_log_folder, \"*episode*.csv\")) \\\n",
    "                 + glob.glob(os.path.join(latest_log_folder, \"*full*.csv\"))\n",
    "    if candidates:\n",
    "        csv_path = candidates[-1]\n",
    "        print(f\"[WARN] {csv_filename} が見つかりませんでした。代わりに候補を使用します: {csv_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{csv_filename} が見つかりません: {latest_log_folder}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) CSV 読み込み\n",
    "#   - 通常はヘッダ 1 行目として読み込む。\n",
    "#   - エクスポートの仕方によっては 1 行目がコメントになっていることもあるので、\n",
    "#     エラー時は skiprows=1 で再トライする。\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"[INFO] CSV を読み込みました: {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] 直接読み込みに失敗しました。skiprows=1 を試します。エラー:\", e)\n",
    "    df = pd.read_csv(csv_path, skiprows=1)\n",
    "    print(f\"[INFO] CSV を読み込みました（skiprows=1）: {csv_path}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) 列名 → ラベル・単位のマッピング\n",
    "#   - プロットの軸ラベルに人間が読める形の名称と単位を付けたいので、\n",
    "#     生の列名からラベル・単位を引ける辞書を用意しておく。\n",
    "# -----------------------------------------------------------------------------\n",
    "label_unit_map = {\n",
    "    \"episode\":                (\"Episode\", \"\"),\n",
    "    \"episode_length\":         (\"Episode length\", \"steps\"),\n",
    "    \"total_reward\":           (\"Total reward\", \"arb.\"),  # 報酬は設計依存なので単位は任意\n",
    "    \"sum_reward_dist_step\":   (\"Sum distance shaping reward\", \"arb.\"),\n",
    "    \"sum_reward_jerk_step\":   (\"Sum jerk shaping reward\", \"arb.\"),\n",
    "    \"sum_reward_time_step\":   (\"Sum time shaping reward\", \"arb.\"),\n",
    "    \"sum_terminal_jerk_penalty\": (\"Terminal jerk penalty\", \"arb.\"),\n",
    "    \"sum_terminal_vel_penalty\":  (\"Terminal velocity penalty\", \"arb.\"),\n",
    "    \"sum_time_bonus\":         (\"Terminal time bonus\", \"arb.\"),\n",
    "    \"jerk_sum\":               (\"Jerk sum\", \"rad^2/s^5\"),\n",
    "    \"running_mean_total_reward_100\": (\"Running mean total reward (100)\", \"arb.\"),\n",
    "    \"episode_wall_time\":      (\"Episode wall time\", \"s\"),\n",
    "    \"sigma_T\":                (\"sigma_T\", \"\"),\n",
    "    \"shaping_scale\":          (\"shaping_scale\", \"\")\n",
    "    # 必要になったら loss 系や success 系もここに追加する\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) 利用可能な列名を表示（デバッグ・確認用）\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"[INFO] CSV の列:\")\n",
    "for c in df.columns:\n",
    "    print(\"  \", c)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) X 軸に使う episode 番号を決定\n",
    "#   - 本来は episode 列を使うが、ない場合は単純に 1..N をエピソード番号として扱う。\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"episode\" in df.columns:\n",
    "    episodes = df[\"episode\"].values\n",
    "else:\n",
    "    episodes = np.arange(1, len(df) + 1)\n",
    "    print(\"[WARN] 'episode' 列が見つかりません。インデックスをエピソード番号として使用します。\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) グループ定義：同じ図にまとめて重ねたい列を定義\n",
    "#   - 報酬成分だけをまとめる、ステップ系と終端系を分ける、など\n",
    "#     「比較して見たいもの」を 1 枚の図に載せる。\n",
    "# -----------------------------------------------------------------------------\n",
    "groups = {\n",
    "    \"reward_components\": [\n",
    "        \"total_reward\",\n",
    "        \"sum_terminal_jerk_penalty\",\n",
    "        \"sum_terminal_vel_penalty\",\n",
    "        \"sum_time_bonus\",\n",
    "        \"sum_reward_dist_step\",\n",
    "        \"sum_reward_jerk_step\",\n",
    "        \"sum_reward_time_step\",\n",
    "    ],\n",
    "    \"reward_goal_components\": [\n",
    "        \"sum_terminal_jerk_penalty\",\n",
    "        \"sum_terminal_vel_penalty\",\n",
    "        \"sum_time_bonus\"\n",
    "    ],\n",
    "    \"reward__step_components\": [\n",
    "        \"sum_reward_dist_step\",\n",
    "        \"sum_reward_jerk_step\",\n",
    "        \"sum_reward_time_step\",\n",
    "    ],\n",
    "    \"dynamics\": [\n",
    "        \"jerk_sum\",\n",
    "        \"episode_length\",\n",
    "        \"episode_wall_time\"\n",
    "    ]\n",
    "    # 例えば学習安定性を見たいときは learning_metrics / running_stats を追加定義する\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) 保存ディレクトリの準備\n",
    "#   - 出力先: 最新ログフォルダ/plots 以下にすべての PNG をまとめる。\n",
    "# -----------------------------------------------------------------------------\n",
    "out_dir = latest_log_folder\n",
    "plots_dir = os.path.join(out_dir, \"plots\")\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9) 描画ヘルパー関数\n",
    "#   - safe_get_col: 列がなければ None を返す\n",
    "#   - plot_series: 1 or 多系列の折れ線グラフを保存\n",
    "# -----------------------------------------------------------------------------\n",
    "def safe_get_col(df, col):\n",
    "    \"\"\"DataFrame から列を安全に取得する。存在しなければ None を返す。\"\"\"\n",
    "    return df[col].values if col in df.columns else None\n",
    "\n",
    "def plot_series(x, y, xlabel, ylabel, savepath, legend=None, ylim=None):\n",
    "    \"\"\"\n",
    "    単一系列または複数系列を描画して保存するヘルパー。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        X 軸（エピソード番号など）\n",
    "    y : array-like or dict[str, array-like]\n",
    "        Y 軸データ。dict の場合は key を凡例名として複数系列を重ね描画。\n",
    "    xlabel, ylabel : str\n",
    "        軸ラベル。\n",
    "    savepath : str\n",
    "        保存先 PNG ファイルパス。\n",
    "    legend : bool or None\n",
    "        True のとき凡例を表示。None のときは表示しない。\n",
    "    ylim : tuple or None\n",
    "        Y 軸の範囲を指定したいときに使用。\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        print(f\"[SKIP] {savepath} : データが存在しないためスキップします。\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "\n",
    "    if isinstance(y, dict):\n",
    "        # 複数系列\n",
    "        for name, arr in y.items():\n",
    "            plt.plot(x, arr, label=name)\n",
    "    else:\n",
    "        # 単一系列\n",
    "        plt.plot(x, y)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    if legend:\n",
    "        plt.legend(loc=\"best\")\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[SAVED] {savepath}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10) 移動平均を計算するヘルパー関数\n",
    "#   - NaN を許容しつつ単純移動平均を返す（pandas の rolling を活用）。\n",
    "# -----------------------------------------------------------------------------\n",
    "def moving_average(arr, window):\n",
    "    \"\"\"NaN を許容しつつ単純移動平均を返す。pandas.Series.rolling を利用。\"\"\"\n",
    "    if arr is None:\n",
    "        return None\n",
    "    return pd.Series(arr).rolling(window=window, min_periods=1).mean().to_numpy()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 11) グループごとのまとめプロット\n",
    "#   - 各グループについて、\n",
    "#       (1) 元データ版の重ね描画\n",
    "#       (2) 移動平均版の重ね描画\n",
    "#     の 2 種類を出力する。\n",
    "# -----------------------------------------------------------------------------\n",
    "for gname, cols in groups.items():\n",
    "    # 実際に CSV に存在する列だけを対象にする\n",
    "    available = [c for c in cols if c in df.columns]\n",
    "    if not available:\n",
    "        print(f\"[SKIP] グループ '{gname}' にプロット可能な列がありません。\")\n",
    "        continue\n",
    "\n",
    "    # --- 元データでのプロット ---\n",
    "    series_dict = {}\n",
    "    for c in available:\n",
    "        arr = df[c].values\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        label_full = f\"{label} [{unit}]\" if unit else label\n",
    "        series_dict[label_full] = arr\n",
    "\n",
    "    savepath = os.path.join(plots_dir, f\"group_{gname}.png\")\n",
    "    plot_series(\n",
    "        episodes,\n",
    "        series_dict,\n",
    "        xlabel=\"Episode\",\n",
    "        ylabel=\"Value\",\n",
    "        savepath=savepath,\n",
    "        legend=True\n",
    "    )\n",
    "\n",
    "    # --- 移動平均版のプロット ---\n",
    "    ma_series_dict = {}\n",
    "    for c in available:\n",
    "        arr = df[c].values\n",
    "        arr_ma = moving_average(arr, MOVING_AVG_WINDOW)\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        label_full = f\"{label} [{unit}]\" if unit else label\n",
    "        ma_series_dict[label_full] = arr_ma\n",
    "\n",
    "    savepath_ma = os.path.join(plots_dir, f\"group_{gname}_ma_w{MOVING_AVG_WINDOW}.png\")\n",
    "    plot_series(\n",
    "        episodes,\n",
    "        ma_series_dict,\n",
    "        xlabel=\"Episode\",\n",
    "        ylabel=f\"Mean Value (window={MOVING_AVG_WINDOW})\",\n",
    "        savepath=savepath_ma,\n",
    "        legend=True\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 12) 各列ごとの個別プロット\n",
    "#   - 全列に対して 1 枚ずつのプロットを作る。\n",
    "#   - success 系は 0〜1 の比率なので ylim を 0〜1 に固定するなど、列に応じて調整。\n",
    "# -----------------------------------------------------------------------------\n",
    "for col in df.columns:\n",
    "    if col == \"episode\":\n",
    "        continue\n",
    "\n",
    "    arr = safe_get_col(df, col)\n",
    "    if arr is None:\n",
    "        continue\n",
    "\n",
    "    label, unit = label_unit_map.get(col, (col, \"\"))\n",
    "    ylabel = f\"{label} [{unit}]\" if unit else label\n",
    "    savepath = os.path.join(plots_dir, f\"episode_{col}.png\")\n",
    "\n",
    "    # success / running_success_rate の場合は 0〜1 の比率\n",
    "    ylim = None\n",
    "    if col in (\"success\", \"running_success_rate_100\"):\n",
    "        ylim = (-0.05, 1.05)\n",
    "\n",
    "    # 元データ版\n",
    "    plot_series(\n",
    "        episodes,\n",
    "        arr,\n",
    "        xlabel=\"Episode\",\n",
    "        ylabel=ylabel,\n",
    "        savepath=savepath,\n",
    "        legend=False,\n",
    "        ylim=ylim\n",
    "    )\n",
    "\n",
    "    # 移動平均版\n",
    "    arr_ma = moving_average(arr, MOVING_AVG_WINDOW)\n",
    "    savepath_ma = os.path.join(plots_dir, f\"episode_{col}_ma_w{MOVING_AVG_WINDOW}.png\")\n",
    "    plot_series(\n",
    "        episodes,\n",
    "        arr_ma,\n",
    "        xlabel=\"Episode\",\n",
    "        ylabel=f\"{label} (moving avg)\",\n",
    "        savepath=savepath_ma,\n",
    "        legend=False,\n",
    "        ylim=ylim\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 13) 全報酬成分を重ねた特別プロット\n",
    "#   - reward_components グループの列だけを取り出し、\n",
    "#     報酬成分のバランスや学習による変化を一望できる図を作る。\n",
    "# -----------------------------------------------------------------------------\n",
    "reward_cols = [c for c in groups[\"reward_components\"] if c in df.columns]\n",
    "\n",
    "if reward_cols:\n",
    "    # 元データ版\n",
    "    series = {}\n",
    "    for c in reward_cols:\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        series[f\"{label} [{unit}]\"] = df[c].values\n",
    "\n",
    "    savepath = os.path.join(plots_dir, \"all_reward_components_overlay.png\")\n",
    "    plot_series(\n",
    "        episodes,\n",
    "        series,\n",
    "        xlabel=\"Episode\",\n",
    "        ylabel=\"Reward (arb.)\",\n",
    "        savepath=savepath,\n",
    "        legend=True\n",
    "    )\n",
    "\n",
    "    # 移動平均版\n",
    "    series_ma = {}\n",
    "    for c in reward_cols:\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        series_ma[f\"{label} [{unit}]\"] = moving_average(df[c].values, MOVING_AVG_WINDOW)\n",
    "\n",
    "    savepath_ma = os.path.join(plots_dir, f\"all_reward_components_overlay_ma_w{MOVING_AVG_WINDOW}.png\")\n",
    "    plot_series(\n",
    "        episodes,\n",
    "        series_ma,\n",
    "        xlabel=\"Episode\",\n",
    "        ylabel=\"Reward (moving avg)\",\n",
    "        savepath=savepath_ma,\n",
    "        legend=True\n",
    "    )\n",
    "\n",
    "print(\"[DONE] プロット作成完了。結果はフォルダに保存されています:\", plots_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
