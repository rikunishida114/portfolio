{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SB3 を用いた 2 リンクアームの躍度最小化軌道の学習(https://github.com/DLR-RM/stable-baselines3)\n",
    "### この jupyternotebook は以下のような構造になっています\n",
    "1. モジュール管理\n",
    "2. パラメータ管理\n",
    "3. 環境設定\n",
    "4. 環境が正しく動くかの確認\n",
    "5. エピソードごとの報酬などの学習内部指標を定期的に保存するためのクラス\n",
    "6. 学習\n",
    "7. 学習済みデータを用いたシミュレーション\n",
    "8. ベストモデルを用いたシミュレーション\n",
    "9. エピソードごとの様々な報酬の推移などの多指標をプロット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュール管理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 基本的なライブラリ =====\n",
    "import os             # ファイルパスやディレクトリの操作（例：ログ保存先の作成など）\n",
    "import json           # 設定ファイルや学習結果をJSON形式で保存・読み込むために使用\n",
    "import csv            # 結果やログをCSV形式で出力するために使用\n",
    "import time           # 実行時間の計測やスリープ処理などに利用\n",
    "import glob           # 特定パターンのファイル探索（例：\"*.csv\" など）\n",
    "from typing import Optional, List  # 型ヒントのため（関数の引数・戻り値の明示）\n",
    "from datetime import datetime      # 実行開始時刻やログのタイムスタンプ記録に利用\n",
    "import numpy as np    # 数値計算用ライブラリ（ベクトル・行列演算、乱数生成など）\n",
    "import pandas as pd   # データ処理やCSVの読み書き・集計・可視化に便利\n",
    "\n",
    "# ===== 可視化関連 =====\n",
    "import matplotlib.pyplot as plt  # 学習曲線、報酬推移、動作ログなどの可視化に使用\n",
    "\n",
    "# ===== 強化学習関連（Stable-Baselines3） =====\n",
    "from stable_baselines3 import PPO, SAC, TD3, DDPG, A2C  \n",
    "# → 代表的な強化学習アルゴリズム（方策勾配系やアクタークリティック系）をインポート\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback  \n",
    "# → 学習中に評価や早期終了などを行うためのコールバック機能を利用するため\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env  \n",
    "# → 独自実装したGym環境がSB3に適合しているかを検証するための関数\n",
    "\n",
    "from collections import deque  # 一定長の履歴（移動平均など）を保持するのに便利\n",
    "\n",
    "# ===== Gym関連 =====\n",
    "import gymnasium as gym  # 強化学習環境の作成・管理（GymnasiumはGymの後継ライブラリ）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== シミュレーションパラメータ =====\n",
    "DT = 0.01           # シミュレーションの時間刻み (秒)\n",
    "L = 1.0             # アームの長さ (m)\n",
    "STEPS_MAX = 200     # 1エピソードあたりの最大ステップ数\n",
    "THRESHOLD = 0.03    # ゴール判定の許容距離 (m)\n",
    "\n",
    "# ===== アクションと状態の範囲 =====\n",
    "ACTION_MIN, ACTION_MAX = -200, 200  # アクション範囲（角速度の下限・上限）[deg/s]\n",
    "THETA_MIN, THETA_MAX = 0, 180       # 関節角度の下限・上限 [deg]\n",
    "THETA_INIT = [40,100]                     # 初期角度 [deg]\n",
    "GOAL_POS = [0.0,0.9]               # ゴール位置 (x, y)\n",
    "\n",
    "# --- 報酬関連（終端 + shaping） ---\n",
    "REWARD_P_V = 10.0         # 終端速度ペナルティ係数\n",
    "REWARD_J = 4e-4          # 終端躍度係数（累積にかける）\n",
    "REWARD_JE_LIM = 1.0      # 比率表示用（infoに出すだけ）\n",
    "\n",
    "# --- 目標時間と時間軸の扱い（ソフト制約） ---\n",
    "T_TARGET = 0.45           # 目標所要時間 [s]\n",
    "SIGMA_T = 2.0           # 時間ボーナスの許容幅 [s]\n",
    "R_GOAL = 50.0            # 成功時の基礎ボーナス（十分大きくする）\n",
    "TIME_COST = 1e-3         # 毎ステップの時間コスト（小）\n",
    "\n",
    "# --- shaping（毎ステップの小さなガイド） ---\n",
    "SHAPING_DIST_COEFF = 10.0     # 距離改善に対するステップ報酬倍率\n",
    "SHAPING_JERK_COEFF = 1e-7    # 瞬時躍度ペナルティ係数（小さく）\n",
    "\n",
    "# --- 停滞検出（進捗が無いと早めに打ち切る） ---\n",
    "STALL_WINDOW_S = 0.25        # 停滞ウィンドウ長 [s]\n",
    "STALL_WINDOW = max(1, int(STALL_WINDOW_S / DT))  # ステップ単位\n",
    "MIN_PROGRESS_PER_WINDOW = 1e-4   # この期間の進捗がこれ未満なら停滞と判定（m）\n",
    "\n",
    "# ===== 学習設定 =====\n",
    "TOTAL_TIMESTEPS = 3000000  # 学習全体での総ステップ数\n",
    "LEARNING_RATE = 0.0003     # 学習率\n",
    "TAU = 0.01                 #SACを使う際のαの学習りつ\n",
    "HID_LAY = 64               #隠れ層のノード数\n",
    "BUFFER_SIZE = 100000        # リプレイバッファのサイズ\n",
    "BATCH_SIZE = 256             # ミニバッチサイズ\n",
    "EVAL_FREQ = 100             # 何ステップごとに評価するか\n",
    "SAVE_INTERVAL = 1           # 何エピソードごとにモデルを保存するか\n",
    "RUNNNIG_WINDOW = 10 # 直近何エピソードの平均を計算するか\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定（2リンクアームのリーチング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoJointReachingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    2関節アームのリーチング環境（拡張版）\n",
    "    - 直近 100 エピソードの成功率に応じて self.sigma_T を自動で更新（先鋭化/鈍化）します。\n",
    "    - 変更点・追加:\n",
    "        * self._success_window : deque を使って直近成功フラグを保持（size = 100）\n",
    "        * self.episode_count : エピソードカウンタ\n",
    "        * update_sigma_by_success_rate() : 成功率に応じた sigma_T の更新ロジック\n",
    "        * step() の末尾でエピソード終了時に success を push -> 必要なら sigma を更新\n",
    "        * info に sigma 関連のログを追加\n",
    "    - 注意: この実装はエピソード終了（terminated or truncated）ごとにしか success-window を更新しません。\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- 基本パラメータ（グローバル変数前提） ----------\n",
    "        self.dt = DT  # シミュレーション刻み幅（秒）\n",
    "        # 全長 L を 2 等分して link 長さに割当て\n",
    "        self.l1 = float(L) * 0.5\n",
    "        self.l2 = float(L) * 0.5\n",
    "\n",
    "        # エピソード最大ステップ、ゴール閾値、ゴール位置\n",
    "        self.max_steps = STEPS_MAX\n",
    "        self.goal_threshold = THRESHOLD\n",
    "        self.goal_pos = np.array(GOAL_POS, dtype=np.float32)\n",
    "\n",
    "        # --- 報酬 / タスクパラメータ（グローバルから読み取り） ---\n",
    "        self.T_target = T_TARGET\n",
    "        self.sigma_T = SIGMA_T   # <-- この値を success-rate に応じて更新する\n",
    "        self.R_goal = R_GOAL\n",
    "        self.time_cost = TIME_COST\n",
    "        self.shaping_dist_coeff = SHAPING_DIST_COEFF\n",
    "        self.shaping_jerk_coeff = SHAPING_JERK_COEFF\n",
    "        self.terminal_jerk_coeff = REWARD_J       # 終端で累積手先躍度にかける重み\n",
    "        self.terminal_vel_coeff = REWARD_P_V      # 終端での手先速度ノルム^2 にかける重み\n",
    "\n",
    "        # 停滞判定ウィンドウ（ステップ数）と最小進捗量\n",
    "        self.stall_window = STALL_WINDOW\n",
    "        self.min_progress_per_window = MIN_PROGRESS_PER_WINDOW\n",
    "\n",
    "        # ---------- 観測空間（9次元） ----------\n",
    "        obs_low = np.array([-1.0]*4 + [0.0], dtype=np.float32)\n",
    "        obs_high = np.array([1.0]*4 + [1.0], dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "\n",
    "        # ---------- アクション空間（各関節の角速度 rad/s, shape=(2,)） ----------\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.radians(ACTION_MIN),\n",
    "            high=np.radians(ACTION_MAX),\n",
    "            shape=(2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # デバッグ用: 各ステップで両関節が同じ角速度になっていないかを確認するためのカウンタ\n",
    "        self._same_action_counter = 0\n",
    "        self._same_action_tol = 1e-8\n",
    "\n",
    "        # ---------- --- sigma 自動更新に関するハイパラ --- ----------\n",
    "        # window size: 100 episodes （あなたの指定）\n",
    "        self._success_window_size = 100\n",
    "        # deque に成功フラグを入れて管理（1=success, 0=failure）\n",
    "        self._success_window = deque(maxlen=self._success_window_size)\n",
    "        self._success_window_size = 100\n",
    "\n",
    "        # episode counter（reset でエピソード開始時にインクリメントされる想定）\n",
    "        self.episode_count = 0\n",
    "\n",
    "        # ここだけ追加：ウィンドウ満杯かつこの interval に達したときだけ更新する\n",
    "        # 例えば 10 にすれば「100エピソードが溜まった後、10エピソードごとに判定」\n",
    "        self.sigma_update_interval = 10\n",
    "        self._sigma_update_counter = 0\n",
    "\n",
    "        # しきい値: この成功率を超えたら \"先鋭化（sigma を小さく）\" する\n",
    "        self.sigma_sharpen_threshold = 0.80    # 例: 80% 以上なら先鋭化\n",
    "        # しきい値: この成功率を下回ったら \"鈍化（sigma を大きく）\" する\n",
    "        self.sigma_blunt_threshold = 0.40      # 例: 40% 以下なら鈍化\n",
    "\n",
    "        # 変化率: 先鋭化するときの乗算ファクタ (<1 で sigma が小さくなる)\n",
    "        self.sigma_sharpen_factor = 0.90       # 例: sigma *= 0.90（10%小さく）\n",
    "        # 変化率: 鈍化するときの乗算ファクタ (>1 で sigma が大きくなる)\n",
    "        self.sigma_blunt_factor = 1.10         # 例: sigma *= 1.10（10%大きく）\n",
    "\n",
    "        # sigma の上下限（安定化のため）\n",
    "        self.sigma_min = 0.2\n",
    "        self.sigma_max = max(1e-3, SIGMA_T * 10.0)\n",
    "\n",
    "        # ログ／デバッグ用に直近更新内容を記録\n",
    "        self._last_sigma_update = {\"episode\": None, \"old_sigma\": None, \"new_sigma\": None, \"success_rate\": None, \"action\": None}\n",
    "\n",
    "        # 環境を初期化（reset を呼ぶ）\n",
    "        self.reset()\n",
    "\n",
    "    # ---------- ヘルパー ----------\n",
    "    def set_penalty_weight(self, new_weight):\n",
    "        \"\"\"外部から終端躍度重みを上書き\"\"\"\n",
    "        self.terminal_jerk_coeff = float(new_weight)\n",
    "\n",
    "    def reached_goal(self, dist_to_goal):\n",
    "        \"\"\"距離ベースのゴール判定\"\"\"\n",
    "        return float(dist_to_goal) <= float(self.goal_threshold)\n",
    "\n",
    "    def forward_kinematics(self, thetas):\n",
    "        \"\"\"順運動学: thetas = [th1, th2] (rad) -> hand_pos [x,y] (m)\"\"\"\n",
    "        th1, th2 = float(thetas[0]), float(thetas[1])\n",
    "        x = self.l1 * np.cos(th1) + self.l2 * np.cos(th1 + th2)\n",
    "        y = self.l1 * np.sin(th1) + self.l2 * np.sin(th1 + th2)\n",
    "        return np.array([x, y], dtype=np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        観測を生成して返す（正規化済み）。\n",
    "        観測ベクトルの順序（呼び出し箇所と合わせてください）:\n",
    "         [hand_x, hand_y, hand_vx, hand_vy, time_scaled]\n",
    "        （元の仕様の通りに整形して返します）\n",
    "        \"\"\"\n",
    "        # --- 角度スケール（使っていれば） ---\n",
    "        theta_min_rad = np.radians(THETA_MIN)\n",
    "        theta_max_rad = np.radians(THETA_MAX)\n",
    "        theta_clipped = np.clip(self.theta, theta_min_rad, theta_max_rad)\n",
    "        theta_scaled = 2.0 * (theta_clipped - theta_min_rad) / max(1e-8, (theta_max_rad - theta_min_rad)) - 1.0\n",
    "        theta_scaled = np.asarray(theta_scaled).ravel()\n",
    "        theta_scaled = np.clip(theta_scaled, -1.0, 1.0)\n",
    "\n",
    "        # --- 関節角速度スケール ---\n",
    "        vel_scale = np.radians(max(abs(ACTION_MIN), abs(ACTION_MAX)))\n",
    "        theta_vel_scaled = np.asarray(self.theta_vel / max(1e-8, vel_scale)).ravel()\n",
    "        theta_vel_scaled = np.clip(theta_vel_scaled, -1.0, 1.0)\n",
    "\n",
    "        # --- 手先位置スケール (m) ---\n",
    "        pos_scale = (self.l1 + self.l2)\n",
    "        hand_pos_scaled = np.asarray(self.hand_pos / max(1e-8, pos_scale)).ravel()\n",
    "        hand_pos_scaled = np.clip(hand_pos_scaled, -1.0, 1.0)\n",
    "\n",
    "        # --- 手先速度スケール (m/s) ---\n",
    "        max_joint_speed_rad = vel_scale\n",
    "        max_hand_speed = max_joint_speed_rad * pos_scale\n",
    "        hand_vel_scaled = np.asarray(self.hand_vel / max(1e-8, max_hand_speed)).ravel()\n",
    "        hand_vel_scaled = np.clip(hand_vel_scaled, -1.0, 1.0)\n",
    "\n",
    "        # --- 時間スケール ---\n",
    "        time_scaled = np.array([np.clip(float(self.steps) / max(1, int(self.max_steps)), 0.0, 1.0)], dtype=np.float32)\n",
    "\n",
    "        # 最終 obs を連結して返す（必要に応じて順序や要素を戻してください）\n",
    "        obs = np.concatenate([\n",
    "            hand_pos_scaled.astype(np.float32),     # 2\n",
    "            hand_vel_scaled.astype(np.float32),     # 2\n",
    "            time_scaled.astype(np.float32)          # 1\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    # ---------- Gymnasium API: reset ----------\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        初期化:\n",
    "        - THETA_INIT がスカラー（deg）の場合は両関節に同じ初期角度を入れる\n",
    "        - iterable（長さ2）の場合は関節ごとに初期角度を設定\n",
    "        戻り値: (obs, info) を返す（gymnasium 互換）\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # (epi count は reset を呼ぶ側がエピソードを終えたあとに呼ぶことを想定)\n",
    "        # ここでは reset が呼ばれる度に episode_count をインクリメントします。\n",
    "        # 外部の学習ループが env.reset() を使ってエピソードを進める場合、\n",
    "        # episode_count はエピソード数と同期します。\n",
    "        self.episode_count += 1\n",
    "\n",
    "        # THETA_INIT の取り扱い（deg -> rad）\n",
    "        try:\n",
    "            th_init_arr = np.asarray(THETA_INIT)\n",
    "            if th_init_arr.size == 1:\n",
    "                th_init_arr = np.array([th_init_arr.item(), th_init_arr.item()], dtype=np.float32)\n",
    "            else:\n",
    "                th_init_arr = th_init_arr.reshape(-1)[:2].astype(np.float32)\n",
    "        except Exception:\n",
    "            th_init_arr = np.array([THETA_INIT, THETA_INIT], dtype=np.float32)\n",
    "\n",
    "        self.theta = np.radians(th_init_arr).astype(np.float32)\n",
    "        self.steps = 0\n",
    "        self.t = 0.0\n",
    "\n",
    "        # 関節微分量の初期化\n",
    "        self.theta_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.theta_acc = np.zeros(2, dtype=np.float32)\n",
    "        self.theta_jerk = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "        # 手先の状態（順運動学で初期化）\n",
    "        self.hand_pos = self.forward_kinematics(self.theta)\n",
    "        self.hand_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.hand_acc = np.zeros(2, dtype=np.float32)\n",
    "        self.hand_jerk = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "        # 終端評価用の累積手先躍度\n",
    "        self.jerk_sum = 0.0\n",
    "\n",
    "        # 進捗監視用\n",
    "        self.prev_dist = np.linalg.norm(self.hand_pos - self.goal_pos)\n",
    "        self.dist_window = [self.prev_dist]\n",
    "\n",
    "        # action 同一カウンタをリセット\n",
    "        self._same_action_counter = 0\n",
    "\n",
    "        # Gymnasium 規約に合わせて (obs, info) を返す\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    # ---------- Sigma 更新ロジック ----------\n",
    "    def update_sigma_by_success_rate(self):\n",
    "        \"\"\"\n",
    "        直近 self._success_window_size (=100) エピソードの成功率に基づいて self.sigma_T を更新する。\n",
    "\n",
    "        動作:\n",
    "          - success_rate >= sigma_sharpen_threshold -> 先鋭化（sigma *= sigma_sharpen_factor）\n",
    "          - success_rate <= sigma_blunt_threshold   -> 鈍化（sigma *= sigma_blunt_factor）\n",
    "          - そうでなければ変更なし\n",
    "\n",
    "        安定化:\n",
    "          - sigma は self.sigma_min ～ self.sigma_max にクリップされる\n",
    "          - 更新履歴は self._last_sigma_update に格納（デバッグ用）\n",
    "        \"\"\"\n",
    "        if len(self._success_window) < self._success_window_size:\n",
    "            return False\n",
    "\n",
    "        success_rate = float(sum(self._success_window)) / float(self._success_window_size)\n",
    "        old_sigma = float(self.sigma_T)\n",
    "\n",
    "        update_action = None\n",
    "\n",
    "        if success_rate >= self.sigma_sharpen_threshold:\n",
    "            new_sigma = old_sigma * float(self.sigma_sharpen_factor)\n",
    "            update_action = \"sharpen\"\n",
    "        elif success_rate <= self.sigma_blunt_threshold:\n",
    "            new_sigma = old_sigma * float(self.sigma_blunt_factor)\n",
    "            update_action = \"blunt\"\n",
    "        else:\n",
    "            new_sigma = old_sigma\n",
    "\n",
    "        new_sigma = float(np.clip(new_sigma, self.sigma_min, self.sigma_max))\n",
    "        self.sigma_T = new_sigma\n",
    "\n",
    "        self._last_sigma_update = {\n",
    "            \"episode\": int(self.episode_count),\n",
    "            \"old_sigma\": float(old_sigma),\n",
    "            \"new_sigma\": float(new_sigma),\n",
    "            \"success_rate\": float(success_rate),\n",
    "            \"action\": update_action\n",
    "        }\n",
    "        return True\n",
    "\n",
    "    # ---------- Gymnasium API: step ----------\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        1 ステップ進める（Gymnasium 互換）:\n",
    "        入力: action (array-like shape (2,), rad/s)\n",
    "        戻り: obs, reward, terminated, truncated, info\n",
    "\n",
    "        追加: エピソード終了時に success-window を更新し、\n",
    "              100 エピソード分が溜まっていれば sigma_T の更新ロジックを呼ぶ。\n",
    "        \"\"\"\n",
    "        prev_theta = self.theta.copy()\n",
    "        prev_vel = self.theta_vel.copy()\n",
    "        prev_acc = self.theta_acc.copy()\n",
    "\n",
    "        a = np.asarray(action).reshape(-1)\n",
    "        if a.shape[0] != 2:\n",
    "            raise ValueError(f\"action must have shape (2,), got {a.shape}\")\n",
    "\n",
    "        a_clipped = np.clip(a, np.radians(ACTION_MIN), np.radians(ACTION_MAX))\n",
    "        action_same = bool(np.allclose(a_clipped[0], a_clipped[1], atol=self._same_action_tol))\n",
    "        if action_same:\n",
    "            self._same_action_counter += 1\n",
    "        else:\n",
    "            self._same_action_counter = 0\n",
    "\n",
    "        # 関節角度更新（簡易: 角速度指令 * dt）\n",
    "        self.theta = self.theta + a_clipped * self.dt\n",
    "        self.theta = np.clip(self.theta, np.radians(THETA_MIN), np.radians(THETA_MAX))\n",
    "\n",
    "        # 関節微分量更新（有限差分）\n",
    "        self.update_joint_dynamics(prev_theta, prev_vel, prev_acc)\n",
    "\n",
    "        # 手先の運動（有限差分）\n",
    "        prev_hand_pos = self.hand_pos.copy()\n",
    "        prev_hand_vel = self.hand_vel.copy()\n",
    "        prev_hand_acc = self.hand_acc.copy()\n",
    "\n",
    "        self.hand_pos = self.forward_kinematics(self.theta)\n",
    "        self.hand_vel = (self.hand_pos - prev_hand_pos) / self.dt\n",
    "        self.hand_acc = (self.hand_vel - prev_hand_vel) / self.dt\n",
    "        self.hand_jerk = (self.hand_acc - prev_hand_acc) / self.dt\n",
    "\n",
    "        hand_jerk_norm_sq = float(np.dot(self.hand_jerk, self.hand_jerk))\n",
    "        self.jerk_sum += hand_jerk_norm_sq * self.dt\n",
    "\n",
    "        self.steps += 1\n",
    "        self.t += self.dt\n",
    "\n",
    "        dist_to_goal = float(np.linalg.norm(self.hand_pos - self.goal_pos))\n",
    "\n",
    "        # shaping 報酬\n",
    "        reward_dist_step = self.shaping_dist_coeff * (self.prev_dist - dist_to_goal)\n",
    "        reward_jerk_step = - self.shaping_jerk_coeff * hand_jerk_norm_sq * self.dt\n",
    "        reward_time_step = - self.time_cost\n",
    "        reward = reward_dist_step + reward_jerk_step + reward_time_step\n",
    "\n",
    "        # 進捗ウィンドウ更新\n",
    "        self.dist_window.append(dist_to_goal)\n",
    "        if len(self.dist_window) > self.stall_window:\n",
    "            self.dist_window.pop(0)\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        terminal_jerk_penalty = 0.0\n",
    "        terminal_vel_penalty = 0.0\n",
    "        time_bonus = 0.0\n",
    "\n",
    "        if self.reached_goal(dist_to_goal):\n",
    "            # 時間ボーナスの実装（ガウス型：中心 T_target、幅 sigma_T）\n",
    "            # note: sigma_T は self.sigma_T を使う（これが動的に更新される）\n",
    "            time_bonus = self.R_goal * np.exp(- (self.t - self.T_target)**2 / (2 * max(1e-12, (self.sigma_T**2))))\n",
    "            terminal_jerk_penalty = - self.terminal_jerk_coeff * (self.jerk_sum / max(1, self.steps))\n",
    "            hand_vel_norm_sq = float(np.dot(self.hand_vel, self.hand_vel))\n",
    "            terminal_vel_penalty = - self.terminal_vel_coeff * hand_vel_norm_sq\n",
    "\n",
    "            reward += time_bonus + terminal_jerk_penalty + terminal_vel_penalty\n",
    "            terminated = True\n",
    "\n",
    "        elif len(self.dist_window) >= self.stall_window:\n",
    "            prog = self.dist_window[0] - self.dist_window[-1]\n",
    "            if prog < self.min_progress_per_window:\n",
    "                truncated = True\n",
    "                reward += -0.5\n",
    "\n",
    "        if not terminated and self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "            reward += - self.terminal_jerk_coeff * (self.jerk_sum / max(1, self.steps))\n",
    "\n",
    "        reward_components = {\n",
    "            \"reward_dist_step\": float(reward_dist_step),\n",
    "            \"reward_jerk_step\": float(reward_jerk_step),\n",
    "            \"reward_time_step\": float(reward_time_step),\n",
    "            \"terminal_jerk_penalty\": float(terminal_jerk_penalty),\n",
    "            \"terminal_vel_penalty\": float(terminal_vel_penalty),\n",
    "            \"time_bonus\": float(time_bonus),\n",
    "            \"jerk_sum\": float(self.jerk_sum),\n",
    "            \"hand_jerk_norm_sq\": hand_jerk_norm_sq\n",
    "        }\n",
    "\n",
    "        info = {\n",
    "            \"hand_pos\": self.hand_pos,\n",
    "            \"hand_vel\": self.hand_vel,\n",
    "            \"hand_acc\": self.hand_acc,\n",
    "            \"hand_jerk\": self.hand_jerk,\n",
    "            \"theta\": self.theta.copy(),\n",
    "            \"theta_vel\": self.theta_vel.copy(),\n",
    "            \"theta_acc\": self.theta_acc.copy(),\n",
    "            \"theta_jerk\": self.theta_jerk.copy(),\n",
    "            \"dist_to_goal\": dist_to_goal,\n",
    "            \"t\": float(self.t),\n",
    "            \"action\": a_clipped.copy(),\n",
    "            \"action_same\": action_same,\n",
    "            \"same_action_counter\": int(self._same_action_counter),\n",
    "            \"reward_total\": float(reward),\n",
    "            **reward_components,\n",
    "            \n",
    "        }\n",
    "\n",
    "        # --- エピソード終端なら success-window を更新し、必要なら sigma を更新 ---\n",
    "        if terminated or truncated:\n",
    "            success_flag = 1 if terminated else 0\n",
    "            self._success_window.append(success_flag)\n",
    "\n",
    "            sigma_updated = False\n",
    "            # --- 修正箇所: ウィンドウが満杯かつ interval に到達したときだけ update を呼ぶ ---\n",
    "            if len(self._success_window) == self._success_window_size:\n",
    "                # カウンタをインクリメント（ウィンドウが満杯になってからカウント）\n",
    "                self._sigma_update_counter += 1\n",
    "                # interval に到達したタイミングでのみ更新判定を行う\n",
    "                if (self._sigma_update_counter % self.sigma_update_interval) == 0:\n",
    "                    sigma_updated = self.update_sigma_by_success_rate()\n",
    "            # info に sigma 情報を付加して返す（ログしやすくする）\n",
    "            info[\"sigma_T\"] = float(self.sigma_T)\n",
    "            info[\"sigma_update\"] = dict(self._last_sigma_update)\n",
    "\n",
    "        else:\n",
    "            info[\"sigma_T\"] = float(self.sigma_T)\n",
    "            info[\"sigma_update\"] = dict(self._last_sigma_update)\n",
    "\n",
    "        self.prev_dist = dist_to_goal\n",
    "\n",
    "        return self._get_obs(), float(reward), bool(terminated), bool(truncated), info\n",
    "\n",
    "    # ---------- 関節微分量更新 ----------\n",
    "    def update_joint_dynamics(self, prev_theta, prev_vel, prev_acc):\n",
    "        \"\"\"角速度・角加速度・角躍度を有限差分で更新する（シンプル実装）\"\"\"\n",
    "        self.theta_vel = (self.theta - prev_theta) / self.dt\n",
    "        self.theta_acc = (self.theta_vel - prev_vel) / self.dt\n",
    "        self.theta_jerk = (self.theta_acc - prev_acc) / self.dt\n",
    "\n",
    "    # ---------- 描画 ----------\n",
    "    def render(self):\n",
    "        \"\"\"簡易描画: コンソールに手先や角度を出力\"\"\"\n",
    "        print(\n",
    "            f\"t={self.t:.3f}s step={self.steps}, \"\n",
    "            f\"theta1={np.degrees(self.theta[0]):.2f} deg, theta2={np.degrees(self.theta[1]):.2f} deg, \"\n",
    "            f\"hand={self.hand_pos}, sigma_T={self.sigma_T:.6f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境が動くかの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoJointReachingEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エピソードごとの報酬などの学習内部指標を定期的に保存するためのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullMonitorCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    エピソード指標 + 学習内部指標（actor_loss, critic_loss, ent_coef, avg_q など）を\n",
    "    CSV に定期的に保存するコールバック（SB3 用）。\n",
    "\n",
    "    追加機能:\n",
    "      - CSV の最終列に env.info に含まれる \"sigma_T\" を書き込む（存在すれば）\n",
    "      - エピソード内で info に含まれるアクション分布の標準偏差をステップ毎に収集し、\n",
    "        その**平均値**をエピソードごとに計算して CSV の最終列に \"mean_action_std\" として保存\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 log_dir: str,\n",
    "                 save_interval = SAVE_INTERVAL,\n",
    "                 running_window = RUNNNIG_WINDOW,\n",
    "                 verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.save_interval = int(save_interval)\n",
    "        self.running_window = int(running_window)\n",
    "\n",
    "        # CSVファイルのパス（固定）\n",
    "        self.csv_path = os.path.join(self.log_dir, \"episode_full_metrics.csv\")\n",
    "\n",
    "        # 基本ヘッダ（最後に sigma_T と mean_action_std を追加）\n",
    "        header = [\n",
    "            \"episode\",\n",
    "            \"episode_length\",\n",
    "            \"total_reward\",\n",
    "            \"sum_reward_dist_step\",\n",
    "            \"sum_reward_jerk_step\",\n",
    "            \"sum_reward_time_step\",\n",
    "            \"sum_terminal_jerk_penalty\",\n",
    "            \"sum_terminal_vel_penalty\",\n",
    "            \"sum_time_bonus\",\n",
    "            \"jerk_sum\",\n",
    "            \"success\",\n",
    "            \"actor_loss\",\n",
    "            \"critic_loss\",\n",
    "            \"ent_coef\",\n",
    "            \"ent_coef_loss\",\n",
    "            f\"running_mean_total_reward_{self.running_window}\",\n",
    "            f\"running_success_rate_{self.running_window}\",\n",
    "            \"episode_wall_time\",\n",
    "            # --- 追加項目（末尾へ） ---\n",
    "            \"mean_action_std\",   # 1エピソード内の action-std の平均（ない場合 NaN）\n",
    "            \"sigma_T\"            # info[\"sigma_T\"] の値（ない場合 NaN）\n",
    "        ]\n",
    "\n",
    "        # ヘッダーを書き込む（ファイル上書き）\n",
    "        with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "\n",
    "        # 内部バッファ（エピソード単位）\n",
    "        self._step_rewards = []     # 各ステップ報酬\n",
    "        self._step_infos = []       # 各ステップ info の shallow copy\n",
    "        self._step_action_stds = [] # 各ステップで見つかった action-std の値（float）\n",
    "        self.episode_count = 0\n",
    "        self.episode_records = []   # running_window に使うサマリ\n",
    "        self._ep_start_time = None\n",
    "        self._train_start_time = None\n",
    "\n",
    "        # logger key 候補（action_std を探すための key 候補群）\n",
    "        self._action_std_keys = [\n",
    "            \"action_std\", \"action_sigma\", \"pi_std\", \"policy_std\", \"std\",\n",
    "            \"stddev\", \"log_std\", \"scale\", \"action_dist_std\"\n",
    "        ]\n",
    "\n",
    "        # その他 logger key 候補（元実装のまま）\n",
    "        self._actor_loss_keys = [\"train/actor_loss\", \"actor_loss\", \"train/actor_loss/mean\"]\n",
    "        self._critic_loss_keys = [\"train/critic_loss\", \"critic_loss\", \"train/critic_loss/mean\"]\n",
    "        self._ent_coef_keys = [\"train/ent_coef\", \"ent_coef\", \"train/entropy_coef\"]\n",
    "        self._ent_coef_loss_keys = [\"train/ent_coef_loss\", \"ent_coef_loss\", \"ent_coef/grad\"]\n",
    "        self._avg_q_keys = [\"train/average_q\", \"train/mean_q\", \"train/q_value\", \"average_q\"]\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self._train_start_time = time.time()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # locals から infos / rewards を取得（VecEnv の場合はリストになる）\n",
    "        infos = self.locals.get(\"infos\")\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "\n",
    "        # infos の取り出し（最初の env を対象）\n",
    "        if isinstance(infos, (list, tuple, np.ndarray)):\n",
    "            info = infos[0] if len(infos) > 0 else {}\n",
    "        else:\n",
    "            info = infos or {}\n",
    "\n",
    "        # rewards の取り出し（最初の env を対象）\n",
    "        if isinstance(rewards, (list, tuple, np.ndarray)):\n",
    "            reward = float(rewards[0])\n",
    "        else:\n",
    "            reward = float(rewards or 0.0)\n",
    "\n",
    "        # ステップデータをバッファへ追加\n",
    "        self._step_rewards.append(reward)\n",
    "        self._step_infos.append(dict(info))  # shallow copy\n",
    "\n",
    "        # --- アクション std を info 内から探す（複数候補） ---\n",
    "        std_val = None\n",
    "        if info:\n",
    "            for key in self._action_std_keys:\n",
    "                if key in info:\n",
    "                    val = info.get(key)\n",
    "                    try:\n",
    "                        arr = np.asarray(val)\n",
    "                        # 配列なら平均絶対値（要素ごとの std のときの扱い）\n",
    "                        if arr.size > 1:\n",
    "                            std_val = float(np.mean(np.abs(arr)))\n",
    "                        else:\n",
    "                            std_val = float(arr.item())\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            std_val = float(val)\n",
    "                        except Exception:\n",
    "                            std_val = None\n",
    "                    break\n",
    "            # もし info 内に \"action_dist\" のような辞書が入っているケースを探る（便利なら）\n",
    "            if std_val is None:\n",
    "                # common nested patterns\n",
    "                nested = info.get(\"action_dist\") or info.get(\"dist\") or info.get(\"policy_dist\")\n",
    "                if isinstance(nested, dict):\n",
    "                    for key in self._action_std_keys:\n",
    "                        if key in nested:\n",
    "                            try:\n",
    "                                arr = np.asarray(nested[key])\n",
    "                                std_val = float(np.mean(np.abs(arr))) if arr.size > 1 else float(arr.item())\n",
    "                                break\n",
    "                            except Exception:\n",
    "                                pass\n",
    "\n",
    "        # ステップごとの action-std を記録（存在しないステップはスキップ）\n",
    "        if std_val is not None:\n",
    "            self._step_action_stds.append(std_val)\n",
    "\n",
    "        # dones を調べてエピソード終了判定（最初の env を対象）\n",
    "        dones = self.locals.get(\"dones\")\n",
    "        if isinstance(dones, (list, tuple, np.ndarray)):\n",
    "            done_flag = bool(dones[0])\n",
    "        else:\n",
    "            done_flag = bool(dones)\n",
    "\n",
    "        # エピソード開始時のタイマー設定\n",
    "        if self._ep_start_time is None:\n",
    "            self._ep_start_time = time.time()\n",
    "\n",
    "        # エピソード終了処理\n",
    "        if done_flag:\n",
    "            self.episode_count += 1\n",
    "            ep_len = len(self._step_rewards)\n",
    "            total_reward = float(np.sum(self._step_rewards)) if self._step_rewards else 0.0\n",
    "\n",
    "            # helper to sum info key across steps\n",
    "            def sum_info_key(key: str) -> float:\n",
    "                return float(sum([it.get(key, 0.0) for it in self._step_infos]))\n",
    "\n",
    "            sum_reward_dist_step = sum_info_key(\"reward_dist_step\")\n",
    "            sum_reward_jerk_step = sum_info_key(\"reward_jerk_step\")\n",
    "            sum_reward_time_step = sum_info_key(\"reward_time_step\")\n",
    "            sum_terminal_jerk_penalty = sum_info_key(\"terminal_jerk_penalty\")\n",
    "            sum_terminal_vel_penalty = sum_info_key(\"terminal_vel_penalty\")\n",
    "            sum_time_bonus = sum_info_key(\"time_bonus\")\n",
    "\n",
    "            # jerk_sum の取得（info に直接あればそれを使い、なければ近似）\n",
    "            jerk_sum = None\n",
    "            if self._step_infos:\n",
    "                if \"jerk_sum\" in self._step_infos[-1]:\n",
    "                    jerk_sum = float(self._step_infos[-1].get(\"jerk_sum\", 0.0))\n",
    "                else:\n",
    "                    # try to approximate from theta_jerk using env.dt if available\n",
    "                    dt = None\n",
    "                    try:\n",
    "                        base_env = getattr(self.training_env, \"envs\", [None])[0]\n",
    "                        dt = getattr(base_env, \"dt\", None)\n",
    "                    except Exception:\n",
    "                        dt = None\n",
    "                    if dt is None:\n",
    "                        dt = 0.01\n",
    "                    jerk_acc = 0.0\n",
    "                    for it in self._step_infos:\n",
    "                        tj = float(it.get(\"theta_jerk\", 0.0))\n",
    "                        jerk_acc += (tj ** 2) * float(dt)\n",
    "                    jerk_sum = jerk_acc\n",
    "\n",
    "            # success 判定\n",
    "            success = bool(self._step_infos[-1].get(\"success\", False)) if self._step_infos else False\n",
    "            if not success:\n",
    "                last_dist = float(self._step_infos[-1].get(\"dist_to_goal\", np.inf)) if self._step_infos else np.inf\n",
    "                try:\n",
    "                    base_env = getattr(self.training_env, \"envs\", [None])[0]\n",
    "                    goal_thresh = getattr(base_env, \"goal_threshold\", None)\n",
    "                    if goal_thresh is None:\n",
    "                        goal_thresh = getattr(base_env, \"THRESHOLD\", None)\n",
    "                    if goal_thresh is not None:\n",
    "                        success = last_dist <= float(goal_thresh)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # エピソード経過時間（wall time）\n",
    "            ep_wall_time = time.time() - (self._ep_start_time or time.time())\n",
    "            self._ep_start_time = None\n",
    "\n",
    "            # running window summary\n",
    "            self.episode_records.append({\n",
    "                \"episode\": self.episode_count,\n",
    "                \"episode_length\": ep_len,\n",
    "                \"total_reward\": total_reward,\n",
    "                \"success\": int(success),\n",
    "                \"jerk_sum\": jerk_sum\n",
    "            })\n",
    "            last_N = self.episode_records[-self.running_window:]\n",
    "            running_mean_total_reward = float(np.mean([r[\"total_reward\"] for r in last_N])) if last_N else 0.0\n",
    "            running_success_rate = float(np.mean([r[\"success\"] for r in last_N])) if last_N else 0.0\n",
    "\n",
    "            # 学習内部指標の取得（logger から）\n",
    "            actor_loss = np.nan; critic_loss = np.nan; ent_coef = np.nan; ent_coef_loss = np.nan; avg_q = np.nan\n",
    "            name_to_value = {}\n",
    "            try:\n",
    "                if hasattr(self.model, \"logger\") and hasattr(self.model.logger, \"name_to_value\"):\n",
    "                    name_to_value = getattr(self.model.logger, \"name_to_value\", {}) or {}\n",
    "                elif hasattr(self.logger, \"name_to_value\"):\n",
    "                    name_to_value = getattr(self.logger, \"name_to_value\", {}) or {}\n",
    "            except Exception:\n",
    "                name_to_value = {}\n",
    "\n",
    "            def pick_first_key(candidates: List[str]) -> Optional[float]:\n",
    "                for k in candidates:\n",
    "                    if k in name_to_value:\n",
    "                        try:\n",
    "                            return float(name_to_value.get(k))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                return None\n",
    "\n",
    "            val = pick_first_key(self._actor_loss_keys)\n",
    "            if val is not None:\n",
    "                actor_loss = val\n",
    "            val = pick_first_key(self._critic_loss_keys)\n",
    "            if val is not None:\n",
    "                critic_loss = val\n",
    "            val = pick_first_key(self._ent_coef_keys)\n",
    "            if val is not None:\n",
    "                ent_coef = val\n",
    "            val = pick_first_key(self._ent_coef_loss_keys)\n",
    "            if val is not None:\n",
    "                ent_coef_loss = val\n",
    "            val = pick_first_key(self._avg_q_keys)\n",
    "            if val is not None:\n",
    "                avg_q = val\n",
    "\n",
    "            # --- 追加: エピソード内の action-std の平均を計算 ---\n",
    "            if len(self._step_action_stds) > 0:\n",
    "                mean_action_std = float(np.nanmean(self._step_action_stds))\n",
    "            else:\n",
    "                mean_action_std = float(\"nan\")\n",
    "\n",
    "            # --- 追加: sigma_T を取得 (info の最後のステップに入っている想定) ---\n",
    "            sigma_T_val = float(self._step_infos[-1].get(\"sigma_T\", float(\"nan\"))) if self._step_infos else float(\"nan\")\n",
    "\n",
    "            # CSV 行作成（末尾に mean_action_std と sigma_T を追加）\n",
    "            row = [\n",
    "                self.episode_count,\n",
    "                ep_len,\n",
    "                total_reward,\n",
    "                sum_reward_dist_step,\n",
    "                sum_reward_jerk_step,\n",
    "                sum_reward_time_step,\n",
    "                sum_terminal_jerk_penalty,\n",
    "                sum_terminal_vel_penalty,\n",
    "                sum_time_bonus,\n",
    "                jerk_sum,\n",
    "                int(success),\n",
    "                actor_loss,\n",
    "                critic_loss,\n",
    "                ent_coef,\n",
    "                ent_coef_loss,\n",
    "                running_mean_total_reward,\n",
    "                running_success_rate,\n",
    "                ep_wall_time,\n",
    "                # 末尾追加\n",
    "                mean_action_std,\n",
    "                sigma_T_val\n",
    "            ]\n",
    "\n",
    "            # save_interval ごとにファイルへ追記\n",
    "            if (self.episode_count % self.save_interval) == 0:\n",
    "                with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "                if self.verbose:\n",
    "                    print(f\"[FullMonitor] Saved episode {self.episode_count} to {self.csv_path}\")\n",
    "\n",
    "            # SB3 logger にも記録\n",
    "            try:\n",
    "                self.logger.record(\"episode/total_reward\", total_reward)\n",
    "                self.logger.record(\"episode/jerk_sum\", jerk_sum)\n",
    "                self.logger.record(\"episode/length\", ep_len)\n",
    "                self.logger.record(\"episode/success\", int(success))\n",
    "                self.logger.record(\"episode/running_mean_reward\", running_mean_total_reward)\n",
    "                self.logger.record(\"episode/running_success_rate\", running_success_rate)\n",
    "                # record action-std and sigma\n",
    "                self.logger.record(\"episode/mean_action_std\", mean_action_std)\n",
    "                self.logger.record(\"episode/sigma_T\", sigma_T_val)\n",
    "                if not np.isnan(actor_loss):\n",
    "                    self.logger.record(\"train/actor_loss\", float(actor_loss))\n",
    "                if not np.isnan(critic_loss):\n",
    "                    self.logger.record(\"train/critic_loss\", float(critic_loss))\n",
    "                if not np.isnan(ent_coef):\n",
    "                    self.logger.record(\"train/ent_coef\", float(ent_coef))\n",
    "                if not np.isnan(avg_q):\n",
    "                    self.logger.record(\"train/avg_q\", float(avg_q))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # エピソードバッファをクリア\n",
    "            self._step_rewards.clear()\n",
    "            self._step_infos.clear()\n",
    "            self._step_action_stds.clear()\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コード\n",
    "### SAC モデルと EvalCallback の設定\n",
    "\n",
    "ここでは、SAC (Soft Actor-Critic) を用いて `OneJointReachingEnv` を学習します。  \n",
    "重要なオプションとその意味は以下の通りです：\n",
    "- **total_timesteps**: 学習に使う総ステップ数（エピソードではなく「1ステップ」の数）\n",
    "- **eval_freq**: 何ステップごとに評価を行うか（例：2000 → 2000ステップごと）\n",
    "- **best_model_save_path**: 評価が一番良かったモデルを保存するフォルダ\n",
    "- **deterministic**: 評価時に行動を決定的にするか（学習したポリシーの実力確認用）\n",
    "- **render**: 評価時に描画するか（Trueにすると遅くなるが動きが見える）\n",
    "\n",
    "参考: [Stable-Baselines3 EvalCallback](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#evalcallback)\n",
    "\n",
    "### 学習ログの見方\n",
    "```plaintext\n",
    "| episode/                |          \n",
    "|    jerk_sum             | １エピソードの躍度の総和 \n",
    "|    length               | １エピソードのステップ数       \n",
    "|    mean_action_std      | actor-networkの出力値（開発中）     \n",
    "|    running_mean_reward  | 報酬の総和の平均\n",
    "|    running_success_rate | 到達成功率（開発中）\n",
    "|    sigma_T              | 躍度における報酬付与の変化の度合い\n",
    "|    success              | 到達成功\n",
    "|    total_reward         | 報酬の総和\n",
    "| eval/                   |          \n",
    "|    mean_ep_length       | エピソードの平均ステップ数\n",
    "|    mean_reward          | 報酬の総和の平均（ステップ数を変えた場合）\n",
    "| time/                   |          \n",
    "|    total_timesteps      | 学習で実行した環境ステップの総数\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 実行ごとに一意なディレクトリを作成（日時で名前をつける）\n",
    "run_id = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "log_dir = f\"./logs/{run_id}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# ✅ パラメータを保存しておく\n",
    "training_params = {\n",
    "    \"DT\":DT,\n",
    "    \"L\" : L,           # アームの長さ (m)\n",
    "    \"THETA_INIT\" :THETA_INIT,        # 初期角度 [deg]\"\"\n",
    "    \"GOAL_POS\" : GOAL_POS,              # ゴール位置 (x, y)\n",
    "    \"STEPS_MAX\" : STEPS_MAX,     # 1エピソードあたりの最大ステップ数\n",
    "    \"THRESHOLD\" : THRESHOLD,    # ゴール判定の許容距離 (m)  \n",
    "    \"REWARD_P_V\" : REWARD_P_V,        # 速度ペナルティの重み\n",
    "    \"REWARD_J\" : REWARD_J,   # 躍度ペナルティの重み\n",
    "    \"REWARD_JE_LIM\" : REWARD_JE_LIM,     # 躍度ペナルティの上限\n",
    "    \"T_TARGET\" : T_TARGET,\n",
    "    \"SIGMA_T\" : SIGMA_T,\n",
    "    \"R_GOAL\" : R_GOAL,\n",
    "    \"TIME_COST\" : TIME_COST,\n",
    "    \"SHAPING_DIST_COEFF\" : SHAPING_DIST_COEFF,     # 距離改善に対するステップ報酬倍率\n",
    "    \"SHAPING_JERK_COEFF\" : SHAPING_JERK_COEFF,   # 瞬時躍度ペナルティ係数（小さく）\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"buffer_size\": BUFFER_SIZE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_freq\": EVAL_FREQ,\n",
    "    \"hid_lay\": HID_LAY,\n",
    "    \"TAU\": TAU,\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(log_dir, \"params.json\"), \"w\") as f:\n",
    "    json.dump(training_params, f, indent=4)\n",
    "\n",
    "# ✅ 環境を作成\n",
    "env = TwoJointReachingEnv()\n",
    "eval_env = TwoJointReachingEnv()\n",
    "\n",
    "\n",
    "# ✅ EvalCallback のベストモデル保存先を日時付きに変更\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,                               # 評価用環境\n",
    "    best_model_save_path=log_dir,           # ベストモデルを保存するディレクトリ\n",
    "    log_path=log_dir,                       # 評価結果をログ保存するディレクトリ\n",
    "    eval_freq=training_params[\"eval_freq\"], # 何ステップごとに評価するか\n",
    "    deterministic=True,                     # 評価時は行動を確率的にせず決定論的にする\n",
    "    render=False                            # 評価中に画面描画するか（通常False）\n",
    ")\n",
    "\n",
    "# ✅ SACモデル作成\n",
    "policy_kwargs = dict(net_arch=[HID_LAY, HID_LAY])\n",
    "\n",
    "\n",
    "# model = SAC(\n",
    "#     \"MlpPolicy\",                                     # ポリシーネットワークの種類（多層パーセプトロン）\n",
    "#     env,                                             # 学習環境\n",
    "#     verbose=1,                                       # ログ出力レベル\n",
    "#     learning_rate=training_params[\"learning_rate\"],  # 学習率（必要に応じて変更可\n",
    "#     buffer_size=training_params[\"buffer_size\"],      # リプレイバッファサイズ\n",
    "#     batch_size=training_params[\"batch_size\"],        # バッチサイズ\n",
    "#     tau=training_params[\"TAU\"],\n",
    "#     policy_kwargs=policy_kwargs\n",
    "# )\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",                                     # ポリシーネットワークの種類（多層パーセプトロン）\n",
    "    env,                                             # 学習環境\n",
    "    verbose=1,                                       # ログ出力レベル\n",
    "    learning_rate=training_params[\"learning_rate\"],  # 学習率（必要に応じて変更可\n",
    "    batch_size=training_params[\"batch_size\"],        # バッチサイズ\n",
    "    policy_kwargs=policy_kwargs                      #方策ネットワークの構成を渡す\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ 学習実行（EvalCallbackと報酬記録を組み合わせる）\n",
    "callback=[eval_callback, FullMonitorCallback(log_dir,verbose=1)]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=training_params[\"total_timesteps\"], # 総ステップ数（ここを変えれば学習量を調整できる）\n",
    "    callback=callback                                   # 評価コールバック\n",
    ")\n",
    "\n",
    "# ===== 学習終了時に final model を保存する=====\n",
    "final_model_path = os.path.join(log_dir, \"final_model.zip\")\n",
    "model.save(final_model_path)\n",
    "print(f\"✅ Final model saved: {final_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習終了時点のモデルの活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 設定: 最新ログフォルダを取得（保存先）\n",
    "# -------------------------\n",
    "logs_dir = \"./logs\"\n",
    "log_folders = [os.path.join(logs_dir, d) for d in os.listdir(logs_dir) if os.path.isdir(os.path.join(logs_dir, d))]\n",
    "latest_log_folder = max(log_folders, key=os.path.getmtime)\n",
    "print(f\"[INFO] 最新ログフォルダを使用: {latest_log_folder}\")\n",
    "\n",
    "# -------------------------\n",
    "# ログ用リスト初期化（2関節 + 手先ノルム）\n",
    "# -------------------------\n",
    "time_log = []\n",
    "hand_x, hand_y = [], []\n",
    "\n",
    "# 関節ごとのログ\n",
    "theta1_log, theta2_log = [], []\n",
    "theta1_vel_log, theta2_vel_log = [], []\n",
    "theta1_acc_log, theta2_acc_log = [], []\n",
    "theta1_jerk_log, theta2_jerk_log = [], []\n",
    "\n",
    "# action とチェック用\n",
    "action1_log, action2_log = [], []\n",
    "action_same_log = []\n",
    "\n",
    "# 手先ノルム系ログ（追加）\n",
    "hand_speed_log = []       # ||v||\n",
    "hand_acc_norm_log = []    # ||a||\n",
    "hand_jerk_norm_log = []   # ||jerk||\n",
    "\n",
    "# ----- 初期観測を取得して初期状態をログに追加 -----\n",
    "obs, _ = env.reset()\n",
    "\n",
    "try:\n",
    "    init_theta = np.asarray(env.theta).reshape(-1)\n",
    "    init_theta_vel = np.asarray(env.theta_vel).reshape(-1)\n",
    "    init_theta_acc = np.asarray(env.theta_acc).reshape(-1)\n",
    "    init_theta_jerk = np.asarray(env.theta_jerk).reshape(-1)\n",
    "    init_hand_pos = np.asarray(getattr(env, \"hand_pos\", env.forward_kinematics(init_theta)))\n",
    "    init_hand_vel = np.asarray(getattr(env, \"hand_vel\", np.zeros(2)))\n",
    "    init_hand_acc = np.asarray(getattr(env, \"hand_acc\", np.zeros(2)))\n",
    "    init_hand_jerk = np.asarray(getattr(env, \"hand_jerk\", np.zeros(2)))\n",
    "except Exception:\n",
    "    # フォールバック\n",
    "    init_theta = np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)])\n",
    "    init_theta_vel = np.zeros(2)\n",
    "    init_theta_acc = np.zeros(2)\n",
    "    init_theta_jerk = np.zeros(2)\n",
    "    init_hand_pos = np.asarray(getattr(env, \"hand_pos\", np.array([0.0, 0.0])))\n",
    "    init_hand_vel = np.zeros(2)\n",
    "    init_hand_acc = np.zeros(2)\n",
    "    init_hand_jerk = np.zeros(2)\n",
    "\n",
    "# 時刻ゼロを設定して初期値を格納（deg に変換して保存）\n",
    "t = 0.0\n",
    "time_log.append(t)\n",
    "hand_x.append(float(init_hand_pos[0]))\n",
    "hand_y.append(float(init_hand_pos[1]))\n",
    "\n",
    "theta1_log.append(np.degrees(init_theta[0]))\n",
    "theta2_log.append(np.degrees(init_theta[1]))\n",
    "theta1_vel_log.append(np.degrees(init_theta_vel[0]))\n",
    "theta2_vel_log.append(np.degrees(init_theta_vel[1]))\n",
    "theta1_acc_log.append(np.degrees(init_theta_acc[0]))\n",
    "theta2_acc_log.append(np.degrees(init_theta_acc[1]))\n",
    "theta1_jerk_log.append(np.degrees(init_theta_jerk[0]))\n",
    "theta2_jerk_log.append(np.degrees(init_theta_jerk[1]))\n",
    "\n",
    "# 初期の手先ノルム\n",
    "hand_speed_log.append(float(np.linalg.norm(init_hand_vel)))\n",
    "hand_acc_norm_log.append(float(np.linalg.norm(init_hand_acc)))\n",
    "hand_jerk_norm_log.append(float(np.linalg.norm(init_hand_jerk)))\n",
    "\n",
    "action1_log.append(np.nan)\n",
    "action2_log.append(np.nan)\n",
    "action_same_log.append(False)\n",
    "\n",
    "# ===== 実行ループ =====\n",
    "max_steps_to_run = 200\n",
    "for _ in range(max_steps_to_run):\n",
    "    # 学習済みモデルから行動（2次元）を取得\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    action = np.asarray(action).reshape(-1)\n",
    "    if action.shape[0] != 2:\n",
    "        raise RuntimeError(f\"model.predict returned action with wrong shape {action.shape}. expected (2,)\")\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # 時刻進行\n",
    "    t += getattr(env, \"dt\", 0.01)\n",
    "    time_log.append(t)\n",
    "\n",
    "    # info を優先して手先・関節情報を取得、なければ env 属性を参照\n",
    "    if info is not None:\n",
    "        hand_pos = np.asarray(info.get(\"hand_pos\", getattr(env, \"hand_pos\", np.array([0.0, 0.0]))))\n",
    "        hand_vel = np.asarray(info.get(\"hand_vel\", getattr(env, \"hand_vel\", np.zeros(2))))\n",
    "        hand_acc = np.asarray(info.get(\"hand_acc\", getattr(env, \"hand_acc\", np.zeros(2))))\n",
    "        hand_jerk = np.asarray(info.get(\"hand_jerk\", getattr(env, \"hand_jerk\", np.zeros(2))))\n",
    "\n",
    "        theta = np.asarray(info.get(\"theta\", getattr(env, \"theta\", np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)]))))\n",
    "        theta_vel = np.asarray(info.get(\"theta_vel\", getattr(env, \"theta_vel\", np.zeros(2))))\n",
    "        theta_acc = np.asarray(info.get(\"theta_acc\", getattr(env, \"theta_acc\", np.zeros(2))))\n",
    "        theta_jerk = np.asarray(info.get(\"theta_jerk\", getattr(env, \"theta_jerk\", np.zeros(2))))\n",
    "\n",
    "        action_info = np.asarray(info.get(\"action\", action))\n",
    "        action_same = bool(info.get(\"action_same\", np.allclose(action_info[0], action_info[1], atol=1e-8)))\n",
    "    else:\n",
    "        # フォールバック\n",
    "        hand_pos = np.asarray(getattr(env, \"hand_pos\", np.array([0.0,0.0])))\n",
    "        hand_vel = np.asarray(getattr(env, \"hand_vel\", np.zeros(2)))\n",
    "        hand_acc = np.asarray(getattr(env, \"hand_acc\", np.zeros(2)))\n",
    "        hand_jerk = np.asarray(getattr(env, \"hand_jerk\", np.zeros(2)))\n",
    "\n",
    "        theta = np.asarray(getattr(env, \"theta\", np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)])))\n",
    "        theta_vel = np.asarray(getattr(env, \"theta_vel\", np.zeros(2)))\n",
    "        theta_acc = np.asarray(getattr(env, \"theta_acc\", np.zeros(2)))\n",
    "        theta_jerk = np.asarray(getattr(env, \"theta_jerk\", np.zeros(2)))\n",
    "\n",
    "        action_info = np.asarray(action)\n",
    "        action_same = bool(np.allclose(action_info[0], action_info[1], atol=1e-8))\n",
    "\n",
    "    # 手先位置\n",
    "    hand_x.append(float(hand_pos[0]))\n",
    "    hand_y.append(float(hand_pos[1]))\n",
    "\n",
    "    # 関節（deg）に変換してログ\n",
    "    theta1_log.append(np.degrees(theta[0])); theta2_log.append(np.degrees(theta[1]))\n",
    "    theta1_vel_log.append(np.degrees(theta_vel[0])); theta2_vel_log.append(np.degrees(theta_vel[1]))\n",
    "    theta1_acc_log.append(np.degrees(theta_acc[0])); theta2_acc_log.append(np.degrees(theta_acc[1]))\n",
    "    theta1_jerk_log.append(np.degrees(theta_jerk[0])); theta2_jerk_log.append(np.degrees(theta_jerk[1]))\n",
    "\n",
    "    # action を deg/s 表示で保存\n",
    "    action_deg = np.degrees(action_info)\n",
    "    action1_log.append(float(action_deg[0])); action2_log.append(float(action_deg[1]))\n",
    "    action_same_log.append(bool(action_same))\n",
    "\n",
    "    # --- 手先ノルムを計算してログ ---\n",
    "    hand_speed = float(np.linalg.norm(hand_vel))     # ||v||\n",
    "    hand_acc_norm = float(np.linalg.norm(hand_acc))  # ||a||\n",
    "    hand_jerk_norm = float(np.linalg.norm(hand_jerk))# ||jerk||\n",
    "\n",
    "    hand_speed_log.append(hand_speed)\n",
    "    hand_acc_norm_log.append(hand_acc_norm)\n",
    "    hand_jerk_norm_log.append(hand_jerk_norm)\n",
    "\n",
    "    if bool(done) or bool(truncated):\n",
    "        break\n",
    "\n",
    "# ===== DataFrame に変換（手先ノルム列を追加） =====\n",
    "df = pd.DataFrame({\n",
    "    \"Time\": time_log,\n",
    "    \"HandX\": hand_x,\n",
    "    \"HandY\": hand_y,\n",
    "    \"HandSpeed (m/s)\": hand_speed_log,\n",
    "    \"HandAcc (m/s^2)\": hand_acc_norm_log,\n",
    "    \"HandJerk (m/s^3)\": hand_jerk_norm_log,\n",
    "    \"Theta1 (deg)\": theta1_log,\n",
    "    \"Theta2 (deg)\": theta2_log,\n",
    "    \"Theta1_vel (deg/s)\": theta1_vel_log,\n",
    "    \"Theta2_vel (deg/s)\": theta2_vel_log,\n",
    "    \"Theta1_acc (deg/s^2)\": theta1_acc_log,\n",
    "    \"Theta2_acc (deg/s^2)\": theta2_acc_log,\n",
    "    \"Theta1_jer (deg/s^3)\": theta1_jerk_log,\n",
    "    \"Theta2_jer (deg/s^3)\": theta2_jerk_log,\n",
    "    \"Action1 (deg/s)\": action1_log,\n",
    "    \"Action2 (deg/s)\": action2_log,\n",
    "    \"Action_same\": action_same_log\n",
    "})\n",
    "\n",
    "# ===== 保存 =====\n",
    "output_folder = latest_log_folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "base_filename = os.path.join(output_folder, \"end_result_2joint_with_handnorms\")\n",
    "csv_path = f\"{base_filename}.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"[INFO] CSV 保存: {csv_path}\")\n",
    "\n",
    "# ===== プロット: 手先軌道 =====\n",
    "# ===== プロット: 手先軌道 + アーム姿勢を重ねる =====\n",
    "fig1, ax1 = plt.subplots(figsize=(6,6))\n",
    "\n",
    "# shoulder を原点に固定\n",
    "shoulder = np.array([0.0, 0.0])\n",
    "\n",
    "# 手先軌道（青ライン）\n",
    "ax1.plot(df[\"HandX\"], df[\"HandY\"], linestyle=\"-\", linewidth=1.5, label=\"end-effector trajectory\")\n",
    "\n",
    "# start / end markers\n",
    "ax1.scatter(df[\"HandX\"].iloc[0], df[\"HandY\"].iloc[0], color=\"red\", label=\"start\", zorder=5)\n",
    "ax1.scatter(df[\"HandX\"].iloc[-1], df[\"HandY\"].iloc[-1], color=\"green\", label=\"end\", zorder=5)\n",
    "\n",
    "# リンク長（env から取得できなければ L を半分ずつ使う）\n",
    "try:\n",
    "    l1 = float(getattr(env, \"l1\", (L*0.5)))\n",
    "    l2 = float(getattr(env, \"l2\", (L*0.5)))\n",
    "except Exception:\n",
    "    l1 = L * 0.5\n",
    "    l2 = L * 0.5\n",
    "\n",
    "# 関節位置を計算（theta は deg で保存されている想定）\n",
    "theta1_rad = np.radians(df[\"Theta1 (deg)\"].values)\n",
    "theta2_rad = np.radians(df[\"Theta2 (deg)\"].values)\n",
    "\n",
    "# 第1関節（肘ではなくリンク1の末端）位置\n",
    "x1 = l1 * np.cos(theta1_rad)\n",
    "y1 = l1 * np.sin(theta1_rad)\n",
    "# 手先（リンク2末端）\n",
    "x2 = x1 + l2 * np.cos(theta1_rad + theta2_rad)\n",
    "y2 = y1 + l2 * np.sin(theta1_rad + theta2_rad)\n",
    "\n",
    "# 確認：手先ログと一致するはず\n",
    "# (もし mismatch があれば警告を出す)\n",
    "try:\n",
    "    mismatch = np.nanmax(np.abs(np.column_stack([df[\"HandX\"].values, df[\"HandY\"].values]) - np.column_stack([x2, y2])))\n",
    "    if mismatch > 1e-6:\n",
    "        # print 小さめの警告（実行中に表示）\n",
    "        print(f\"[WARN] forward kinematics and logged hand pos mismatch (max diff={mismatch:.6e})\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# アーム姿勢を複数タイムスタンプで描く\n",
    "N_POSES = min(15, len(df))  # 最大描画姿勢数\n",
    "indices = np.linspace(0, len(df)-1, N_POSES).astype(int)\n",
    "\n",
    "# 薄いグレーで時系列に沿ってアームをプロット（透過）\n",
    "for idx in indices:\n",
    "    xs = [0.0, x1[idx], x2[idx]]\n",
    "    ys = [0.0, y1[idx], y2[idx]]\n",
    "    ax1.plot(xs, ys, linewidth=1, color=(0.5,0.5,0.5,0.25), solid_capstyle='round')  # 太めの線\n",
    "    ax1.scatter([xs[1], xs[2]], [ys[1], ys[2]], s=40, edgecolors='k', facecolors=(0.9,0.9,0.9,0.6), zorder=4)\n",
    "\n",
    "# 強調: start, mid, end の姿勢（色とマーカーで分かりやすく）\n",
    "# start\n",
    "i0 = 0\n",
    "xs0 = [0.0, x1[i0], x2[i0]]\n",
    "ys0 = [0.0, y1[i0], y2[i0]]\n",
    "ax1.plot(xs0, ys0, linewidth=2, color='tab:red', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xs0[1], xs0[2]], [ys0[1], ys0[2]], s=80, color='tab:red', edgecolors='k', zorder=7)\n",
    "ax1.scatter(0.0, 0.0, s=100, color='black', zorder=8)  # shoulder\n",
    "\n",
    "\n",
    "# end\n",
    "ie = len(df)-1\n",
    "xse = [0.0, x1[ie], x2[ie]]\n",
    "yse = [0.0, y1[ie], y2[ie]]\n",
    "ax1.plot(xse, yse, linewidth=2, color='tab:green', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xse[1], xse[2]], [yse[1], yse[2]], s=100, color='tab:green', edgecolors='k', zorder=7)\n",
    "\n",
    "# # 関節角度の数値も近傍に表示（start と end のみ、必要なら増やせます）\n",
    "# ang_off = 0.03 * (l1 + l2)  # 表示オフセット\n",
    "# ax1.text(xs0[1]+ang_off, ys0[1]+ang_off, f\"{np.degrees(theta1_rad[i0]):.1f}°\", color='tab:red', fontsize=9)\n",
    "# ax1.text(xs0[2]+ang_off, ys0[2]+ang_off, f\"{np.degrees(theta2_rad[i0]):.1f}°\", color='tab:red', fontsize=9)\n",
    "# ax1.text(xse[1]+ang_off, yse[1]+ang_off, f\"{np.degrees(theta1_rad[ie]):.1f}°\", color='tab:green', fontsize=9)\n",
    "# ax1.text(xse[2]+ang_off, yse[2]+ang_off, f\"{np.degrees(theta2_rad[ie]):.1f}°\", color='tab:green', fontsize=9)\n",
    "\n",
    "# 装飾\n",
    "ax1.set_xlabel(\"X [m]\"); ax1.set_ylabel(\"Y [m]\")\n",
    "# ax1.set_title(\"End-effector Trajectory with Arm Poses (shoulder at (0,0))\")\n",
    "ax1.grid(True)\n",
    "ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# 自動スケール（アーム長に沿ったマージンを付ける）\n",
    "reach = l1 + l2\n",
    "ax1.set_xlim(-reach-0.1*reach, reach+0.1*reach)\n",
    "ax1.set_ylim(-0.1*reach, reach+0.6*reach)  # y 上方向に少し余裕\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_trajectory_with_arms.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ===== プロット: 関節動的変数（Joint1 / Joint2 両表示） =====\n",
    "fig2, axes = plt.subplots(4,1, figsize=(8,12), sharex=True)\n",
    "axes[0].plot(df[\"Time\"], df[\"Theta1 (deg)\"], label=\"Theta1\"); axes[0].plot(df[\"Time\"], df[\"Theta2 (deg)\"], linestyle=\"--\", label=\"Theta2\")\n",
    "axes[0].set_ylabel(\"Theta (deg)\"); axes[0].legend(); axes[0].grid()\n",
    "axes[0].set_ylim(0, 180)\n",
    "\n",
    "axes[1].plot(df[\"Time\"], df[\"Theta1_vel (deg/s)\"], label=\"Vel1\"); axes[1].plot(df[\"Time\"], df[\"Theta2_vel (deg/s)\"], linestyle=\"--\", label=\"Vel2\")\n",
    "axes[1].set_ylabel(\"Velocity (deg/s)\"); axes[1].legend(); axes[1].grid()\n",
    "\n",
    "axes[2].plot(df[\"Time\"], df[\"Theta1_acc (deg/s^2)\"], label=\"Acc1\"); axes[2].plot(df[\"Time\"], df[\"Theta2_acc (deg/s^2)\"], linestyle=\"--\", label=\"Acc2\")\n",
    "axes[2].set_ylabel(\"Acceleration (deg/s^2)\"); axes[2].legend(); axes[2].grid()\n",
    "\n",
    "axes[3].plot(df[\"Time\"], df[\"Theta1_jer (deg/s^3)\"], label=\"Jerk1\"); axes[3].plot(df[\"Time\"], df[\"Theta2_jer (deg/s^3)\"], linestyle=\"--\", label=\"Jerk2\")\n",
    "axes[3].set_ylabel(\"Jerk (deg/s^3)\"); axes[3].set_xlabel(\"Time (s)\"); axes[3].legend(); axes[3].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_joint_dynamics.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ===== プロット: 手先ノルム（速度・加速度・躍度の大きさ） =====\n",
    "fig3, ax3 = plt.subplots(3,1, figsize=(8,9), sharex=True)\n",
    "ax3[0].plot(df[\"Time\"], df[\"HandSpeed (m/s)\"], label=\"Hand Speed ||v||\")\n",
    "ax3[0].set_ylabel(\"Speed (m/s)\"); ax3[0].legend(); ax3[0].grid()\n",
    "\n",
    "ax3[1].plot(df[\"Time\"], df[\"HandAcc (m/s^2)\"], label=\"Hand Acc ||a||\")\n",
    "ax3[1].set_ylabel(\"Acc (m/s^2)\"); ax3[1].legend(); ax3[1].grid()\n",
    "\n",
    "ax3[2].plot(df[\"Time\"], df[\"HandJerk (m/s^3)\"], label=\"Hand Jerk ||jerk||\")\n",
    "ax3[2].set_ylabel(\"Jerk (m/s^3)\"); ax3[2].set_xlabel(\"Time (s)\"); ax3[2].legend(); ax3[2].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_hand_norms.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ===== 補助プロット: action の推移と action_same の確認（任意） =====\n",
    "fig4, ax4 = plt.subplots(2,1, figsize=(8,6), sharex=True)\n",
    "ax4[0].plot(df[\"Time\"], df[\"Action1 (deg/s)\"], label=\"Action1\"); ax4[0].plot(df[\"Time\"], df[\"Action2 (deg/s)\"], linestyle=\"--\", label=\"Action2\")\n",
    "ax4[0].set_ylabel(\"Action (deg/s)\"); ax4[0].legend(); ax4[0].grid()\n",
    "ax4[1].plot(df[\"Time\"], df[\"Action_same\"].astype(int), label=\"Action_same (int)\")\n",
    "ax4[1].set_ylabel(\"Action_same\"); ax4[1].set_xlabel(\"Time (s)\"); ax4[1].grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_actions.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベストモデルの活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1) 最新のログフォルダを見つけ、best_model.zip を探す\n",
    "# -------------------------\n",
    "log_base = \"./logs\"\n",
    "log_dirs = [d for d in glob.glob(os.path.join(log_base, \"*\")) if os.path.isdir(d)]\n",
    "if not log_dirs:\n",
    "    raise FileNotFoundError(\"logs フォルダにサブディレクトリがありません。学習結果を保存してください。\")\n",
    "\n",
    "latest_log_dir = max(log_dirs, key=os.path.getmtime)\n",
    "# best_model.zip がある想定。なければ最も新しい .zip を探すフォールバックを行う\n",
    "best_model_path = os.path.join(latest_log_dir, \"best_model.zip\")\n",
    "if not os.path.exists(best_model_path):\n",
    "    # フォールバック: フォルダ内の .zip を探して一番新しいものを選ぶ\n",
    "    zips = sorted(glob.glob(os.path.join(latest_log_dir, \"*.zip\")), key=os.path.getmtime)\n",
    "    if not zips:\n",
    "        raise FileNotFoundError(f\"{best_model_path} が存在せず、{latest_log_dir} に .zip ファイルも見つかりません。\")\n",
    "    best_model_path = zips[-1]\n",
    "\n",
    "print(f\"✅ 最新のモデルファイルを使用: {best_model_path}\")\n",
    "best_model_path = \"logs/2025_10_21_23_59_10/best_model.zip\"\n",
    "\n",
    "# -------------------------\n",
    "# 便利関数: ラップを外してベース環境を得る\n",
    "# -------------------------\n",
    "def unwrap_env(e):\n",
    "    \"\"\"\n",
    "    ラッパー (VecEnv, Monitor, DummyVecEnv など) があれば中身の base 環境を返す。\n",
    "    再帰的に unwrap するので、どんなラップでもだいたい対応可能。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # VecEnv の場合: .envs がリスト\n",
    "        if hasattr(e, \"envs\") and isinstance(getattr(e, \"envs\"), (list, tuple)) and len(e.envs) > 0:\n",
    "            return unwrap_env(e.envs[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        # Monitor / TimeLimit 等は .env を持つ場合がある\n",
    "        if hasattr(e, \"env\") and e.env is not None:\n",
    "            return unwrap_env(e.env)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return e\n",
    "\n",
    "# -------------------------\n",
    "# 2) best_model をロード（どのアルゴリズムか分からない場合を想定）\n",
    "# -------------------------\n",
    "loaded_model = None\n",
    "# 試行順: PPO, SAC, TD3, DDPG, A2C\n",
    "for alg in (PPO, SAC, TD3, DDPG, A2C):\n",
    "    try:\n",
    "        loaded_model = alg.load(best_model_path)\n",
    "        print(f\"✅ モデルを {alg.__name__} 形式でロードしました。\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        # 読み込み失敗は無視して次のアルゴリズムを試す\n",
    "        # print(f\"load with {alg.__name__} failed: {e}\")\n",
    "        pass\n",
    "\n",
    "if loaded_model is None:\n",
    "    raise RuntimeError(\"モデルのロードに失敗しました。対応する SB3 アルゴリズムを確認してください。\")\n",
    "\n",
    "best_model = loaded_model  # 名前を統一\n",
    "\n",
    "# -------------------------\n",
    "# 3) env のリセットと初期状態取得（env はユーザーのノートブックで定義済みの前提）\n",
    "# -------------------------\n",
    "# env が未定義だと失敗するので事前に env = TwoJointReachingEnv(...) を作成しておいてください\n",
    "try:\n",
    "    obs_reset = env.reset()\n",
    "    # Gymnasium なら (obs, info), 古い Gym なら obs のみ。柔軟にハンドリング。\n",
    "    if isinstance(obs_reset, tuple) and len(obs_reset) >= 1:\n",
    "        obs = obs_reset[0]\n",
    "    else:\n",
    "        obs = obs_reset\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"env.reset() に失敗しました。env が正しく定義・初期化されているか確認してください.\") from e\n",
    "\n",
    "# unwrap して base_env を取る（内部状態読み出し用）\n",
    "base_env = unwrap_env(env)\n",
    "\n",
    "# -------------------------\n",
    "# 4) ログ用リストを初期化（2関節対応）\n",
    "# -------------------------\n",
    "time_log = []\n",
    "hand_x, hand_y = [], []\n",
    "\n",
    "# 関節ごとのログ（joint1 / joint2）\n",
    "theta1_log, theta2_log = [], []\n",
    "theta1_vel_log, theta2_vel_log = [], []\n",
    "theta1_acc_log, theta2_acc_log = [], []\n",
    "theta1_jerk_log, theta2_jerk_log = [], []\n",
    "\n",
    "# action ログ（deg/s 表示）\n",
    "action1_log, action2_log = [], []\n",
    "action_same_log = []\n",
    "\n",
    "# 手先ノルムログ（速度, 加速度, 躍度）\n",
    "hand_speed_log = []\n",
    "hand_acc_norm_log = []\n",
    "hand_jerk_norm_log = []\n",
    "\n",
    "# -------------------------\n",
    "# 5) 初期状態を取得してログに追加（時刻=0）\n",
    "#    base_env が持つ属性を優先。なければ obs から推定（ただし obs 設計に依存するので注意）\n",
    "# -------------------------\n",
    "t = 0.0\n",
    "time_log.append(t)\n",
    "\n",
    "# try で base_env の配列属性を取り出す（theta は配列 [th1, th2] を期待）\n",
    "try:\n",
    "    base = base_env  # alias\n",
    "    # theta 等は配列で返ることを期待（TwoJointReachingEnv の実装次第）\n",
    "    init_theta = np.asarray(getattr(base, \"theta\", np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)]))).reshape(-1)[:2]\n",
    "    init_theta_vel = np.asarray(getattr(base, \"theta_vel\", np.zeros(2))).reshape(-1)[:2]\n",
    "    init_theta_acc = np.asarray(getattr(base, \"theta_acc\", np.zeros(2))).reshape(-1)[:2]\n",
    "    init_theta_jerk = np.asarray(getattr(base, \"theta_jerk\", np.zeros(2))).reshape(-1)[:2]\n",
    "\n",
    "    # hand_pos があれば優先して使う。無ければ順運動学か L を使って推定\n",
    "    if hasattr(base, \"hand_pos\"):\n",
    "        init_hand = np.asarray(getattr(base, \"hand_pos\")).astype(float)\n",
    "    else:\n",
    "        # try forward_kinematics if exists\n",
    "        if hasattr(base, \"forward_kinematics\"):\n",
    "            init_hand = base.forward_kinematics(init_theta)\n",
    "        else:\n",
    "            # fallback: 単純に L を手先長として theta[0] を使う (古い 1-joint 想定)\n",
    "            L_val = float(getattr(base, \"l\", getattr(base, \"l1\", 1.0) + getattr(base, \"l2\", 0.0)))\n",
    "            init_hand = L_val * np.array([np.cos(init_theta[0]), np.sin(init_theta[0])])\n",
    "except Exception:\n",
    "    # 最終フォールバック（安全）\n",
    "    init_theta = np.array([np.radians(THETA_INIT), np.radians(THETA_INIT)])\n",
    "    init_theta_vel = np.zeros(2)\n",
    "    init_theta_acc = np.zeros(2)\n",
    "    init_theta_jerk = np.zeros(2)\n",
    "    try:\n",
    "        init_hand = np.asarray(getattr(base_env, \"hand_pos\", np.array([0.0, 0.0])))\n",
    "    except Exception:\n",
    "        init_hand = np.array([0.0, 0.0])\n",
    "\n",
    "# 初期値をログに追加（human-friendly に deg / SI 単位で）\n",
    "hand_x.append(float(init_hand[0])); hand_y.append(float(init_hand[1]))\n",
    "theta1_log.append(np.degrees(init_theta[0])); theta2_log.append(np.degrees(init_theta[1]))\n",
    "theta1_vel_log.append(np.degrees(init_theta_vel[0])); theta2_vel_log.append(np.degrees(init_theta_vel[1]))\n",
    "theta1_acc_log.append(np.degrees(init_theta_acc[0])); theta2_acc_log.append(np.degrees(init_theta_acc[1]))\n",
    "theta1_jerk_log.append(np.degrees(init_theta_jerk[0])); theta2_jerk_log.append(np.degrees(init_theta_jerk[1]))\n",
    "\n",
    "# 手先ノルム初期値（速度, acc, jerk）\n",
    "init_hand_vel = np.asarray(getattr(base_env, \"hand_vel\", np.zeros(2))).reshape(-1)[:2]\n",
    "init_hand_acc = np.asarray(getattr(base_env, \"hand_acc\", np.zeros(2))).reshape(-1)[:2]\n",
    "init_hand_jerk = np.asarray(getattr(base_env, \"hand_jerk\", np.zeros(2))).reshape(-1)[:2]\n",
    "\n",
    "hand_speed_log.append(float(np.linalg.norm(init_hand_vel)))\n",
    "hand_acc_norm_log.append(float(np.linalg.norm(init_hand_acc)))\n",
    "hand_jerk_norm_log.append(float(np.linalg.norm(init_hand_jerk)))\n",
    "\n",
    "action1_log.append(np.nan); action2_log.append(np.nan); action_same_log.append(False)\n",
    "\n",
    "# -------------------------\n",
    "# 6) シミュレーション実行ループ（best_model を用いて行動を生成）\n",
    "# -------------------------\n",
    "MAX_STEPS_RUN = 2000  # 必要に応じて増減\n",
    "\n",
    "for step_i in range(MAX_STEPS_RUN):\n",
    "    # model.predict に渡す obs の形状に注意。VecEnv の場合は (n_envs, obs_dim) かもしれないので、\n",
    "    # ここでは model.predict が期待する形式で obs をそのまま渡す (ユーザー環境次第)。\n",
    "    action, _ = best_model.predict(obs, deterministic=True)\n",
    "\n",
    "    # 整形: action が (2,) であることを確認\n",
    "    action = np.asarray(action).reshape(-1)\n",
    "    if action.shape[0] != 2:\n",
    "        raise RuntimeError(f\"predict returned action with wrong shape {action.shape}. expected (2,) for 2-joint environment.\")\n",
    "\n",
    "    # env.step 実行\n",
    "    next_return = env.step(action)\n",
    "\n",
    "    # Gymnasium と古い gym の返り値形式に対応\n",
    "    if isinstance(next_return, tuple) and len(next_return) == 5:\n",
    "        obs, reward, terminated, truncated, info = next_return\n",
    "        done = bool(terminated or truncated)\n",
    "    elif isinstance(next_return, tuple) and len(next_return) == 4:\n",
    "        obs, reward, done, info = next_return\n",
    "        terminated = bool(done)\n",
    "        truncated = False\n",
    "    else:\n",
    "        raise RuntimeError(\"env.step() の返り値の形式が不明です。\")\n",
    "\n",
    "    # 時刻進行：base_env に dt があればそれを使い、なければグローバル DT にフォールバック\n",
    "    dt = getattr(base_env, \"dt\", None)\n",
    "    if dt is None:\n",
    "        try:\n",
    "            dt = float(DT)\n",
    "        except Exception:\n",
    "            dt = 0.01\n",
    "    t += dt\n",
    "    time_log.append(t)\n",
    "\n",
    "    # info 優先で状態を取得。なければ base_env 属性を使う（フォールバック）\n",
    "    if info is not None:\n",
    "        # 手先位置\n",
    "        if \"hand_pos\" in info:\n",
    "            hand_pos = np.asarray(info[\"hand_pos\"]).reshape(-1)[:2]\n",
    "        else:\n",
    "            # forward_kinematics があればそれを使う\n",
    "            if hasattr(base_env, \"forward_kinematics\"):\n",
    "                try:\n",
    "                    # theta は info か base_env のどちらかから取る\n",
    "                    theta_info = np.asarray(info.get(\"theta\", getattr(base_env, \"theta\", init_theta))).reshape(-1)[:2]\n",
    "                    hand_pos = base_env.forward_kinematics(theta_info)\n",
    "                except Exception:\n",
    "                    hand_pos = np.array([np.nan, np.nan])\n",
    "            else:\n",
    "                # 最終フォールバック（1関節的に計算）\n",
    "                try:\n",
    "                    L_val = float(getattr(base_env, \"l\", getattr(base_env, \"l1\", 1.0) + getattr(base_env, \"l2\", 0.0)))\n",
    "                    theta0 = float(np.asarray(info.get(\"theta\", getattr(base_env, \"theta\", init_theta)))[0])\n",
    "                    hand_pos = L_val * np.array([np.cos(theta0), np.sin(theta0)])\n",
    "                except Exception:\n",
    "                    hand_pos = np.array([np.nan, np.nan])\n",
    "\n",
    "        theta_arr = np.asarray(info.get(\"theta\", getattr(base_env, \"theta\", init_theta))).reshape(-1)[:2]\n",
    "        theta_vel_arr = np.asarray(info.get(\"theta_vel\", getattr(base_env, \"theta_vel\", init_theta_vel))).reshape(-1)[:2]\n",
    "        theta_acc_arr = np.asarray(info.get(\"theta_acc\", getattr(base_env, \"theta_acc\", init_theta_acc))).reshape(-1)[:2]\n",
    "        theta_jerk_arr = np.asarray(info.get(\"theta_jerk\", getattr(base_env, \"theta_jerk\", init_theta_jerk))).reshape(-1)[:2]\n",
    "\n",
    "        # 手先の速度/加速度/躍度が info にあるならそれを使う（推奨）\n",
    "        hand_vel = np.asarray(info.get(\"hand_vel\", getattr(base_env, \"hand_vel\", np.zeros(2)))).reshape(-1)[:2]\n",
    "        hand_acc = np.asarray(info.get(\"hand_acc\", getattr(base_env, \"hand_acc\", np.zeros(2)))).reshape(-1)[:2]\n",
    "        hand_jerk = np.asarray(info.get(\"hand_jerk\", getattr(base_env, \"hand_jerk\", np.zeros(2)))).reshape(-1)[:2]\n",
    "    else:\n",
    "        # info が None の場合は base_env の属性を見に行く\n",
    "        try:\n",
    "            # base_env.forward_kinematics があるなら theta から手先位置を計算\n",
    "            theta_arr = np.asarray(getattr(base_env, \"theta\", init_theta)).reshape(-1)[:2]\n",
    "            if hasattr(base_env, \"forward_kinematics\"):\n",
    "                hand_pos = base_env.forward_kinematics(theta_arr)\n",
    "            else:\n",
    "                L_val = float(getattr(base_env, \"l\", getattr(base_env, \"l1\", 1.0) + getattr(base_env, \"l2\", 0.0)))\n",
    "                hand_pos = L_val * np.array([np.cos(theta_arr[0]), np.sin(theta_arr[0])])\n",
    "            theta_vel_arr = np.asarray(getattr(base_env, \"theta_vel\", init_theta_vel)).reshape(-1)[:2]\n",
    "            theta_acc_arr = np.asarray(getattr(base_env, \"theta_acc\", init_theta_acc)).reshape(-1)[:2]\n",
    "            theta_jerk_arr = np.asarray(getattr(base_env, \"theta_jerk\", init_theta_jerk)).reshape(-1)[:2]\n",
    "            hand_vel = np.asarray(getattr(base_env, \"hand_vel\", np.zeros(2))).reshape(-1)[:2]\n",
    "            hand_acc = np.asarray(getattr(base_env, \"hand_acc\", np.zeros(2))).reshape(-1)[:2]\n",
    "            hand_jerk = np.asarray(getattr(base_env, \"hand_jerk\", np.zeros(2))).reshape(-1)[:2]\n",
    "        except Exception:\n",
    "            theta_arr = init_theta\n",
    "            theta_vel_arr = init_theta_vel\n",
    "            theta_acc_arr = init_theta_acc\n",
    "            theta_jerk_arr = init_theta_jerk\n",
    "            hand_pos = np.array([np.nan, np.nan])\n",
    "            hand_vel = np.zeros(2)\n",
    "            hand_acc = np.zeros(2)\n",
    "            hand_jerk = np.zeros(2)\n",
    "\n",
    "    # ----- ログに追加 -----\n",
    "    hand_x.append(float(hand_pos[0])); hand_y.append(float(hand_pos[1]))\n",
    "    theta1_log.append(np.degrees(theta_arr[0])); theta2_log.append(np.degrees(theta_arr[1]))\n",
    "    theta1_vel_log.append(np.degrees(theta_vel_arr[0])); theta2_vel_log.append(np.degrees(theta_vel_arr[1]))\n",
    "    theta1_acc_log.append(np.degrees(theta_acc_arr[0])); theta2_acc_log.append(np.degrees(theta_acc_arr[1]))\n",
    "    theta1_jerk_log.append(np.degrees(theta_jerk_arr[0])); theta2_jerk_log.append(np.degrees(theta_jerk_arr[1]))\n",
    "\n",
    "    # アクション記録（deg/s 単位で保存）\n",
    "    action_deg = np.degrees(action)\n",
    "    action1_log.append(float(action_deg[0])); action2_log.append(float(action_deg[1]))\n",
    "    action_same_log.append(bool(np.allclose(action_deg[0], action_deg[1], atol=1e-6)))\n",
    "\n",
    "    # 手先ノルム（SI 単位）を計算してログ\n",
    "    hand_speed_log.append(float(np.linalg.norm(hand_vel)))\n",
    "    hand_acc_norm_log.append(float(np.linalg.norm(hand_acc)))\n",
    "    hand_jerk_norm_log.append(float(np.linalg.norm(hand_jerk)))\n",
    "\n",
    "    # 終了判定: terminated/truncated または安全上の上限ステップ数など\n",
    "    if bool(terminated) or bool(truncated) or (len(time_log) > 100000):\n",
    "        break\n",
    "\n",
    "# -------------------------\n",
    "# 7) DataFrame 作成 & CSV 保存（2関節・手先ノルム列あり）\n",
    "# -------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Time\": time_log,\n",
    "    \"HandX\": hand_x,\n",
    "    \"HandY\": hand_y,\n",
    "    \"HandSpeed (m/s)\": hand_speed_log,\n",
    "    \"HandAcc (m/s^2)\": hand_acc_norm_log,\n",
    "    \"HandJerk (m/s^3)\": hand_jerk_norm_log,\n",
    "    \"Theta1 (deg)\": theta1_log,\n",
    "    \"Theta2 (deg)\": theta2_log,\n",
    "    \"Theta1_vel (deg/s)\": theta1_vel_log,\n",
    "    \"Theta2_vel (deg/s)\": theta2_vel_log,\n",
    "    \"Theta1_acc (deg/s^2)\": theta1_acc_log,\n",
    "    \"Theta2_acc (deg/s^2)\": theta2_acc_log,\n",
    "    \"Theta1_jer (deg/s^3)\": theta1_jerk_log,\n",
    "    \"Theta2_jer (deg/s^3)\": theta2_jerk_log,\n",
    "    \"Action1 (deg/s)\": action1_log,\n",
    "    \"Action2 (deg/s)\": action2_log,\n",
    "    \"Action_same\": action_same_log\n",
    "})\n",
    "\n",
    "output_dir = latest_log_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_path = os.path.join(output_dir, \"best_result_2joint_with_initial_and_handnorms.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV 保存: {csv_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# 8) プロット: 手先軌跡、関節ダイナミクス（両関節表示）、手先ノルム\n",
    "# -------------------------\n",
    "# ===== プロット: 手先軌道 =====\n",
    "# ===== プロット: 手先軌道 + アーム姿勢を重ねる =====\n",
    "fig1, ax1 = plt.subplots(figsize=(6,6))\n",
    "\n",
    "# shoulder を原点に固定\n",
    "shoulder = np.array([0.0, 0.0])\n",
    "\n",
    "# 手先軌道（青ライン）\n",
    "ax1.plot(df[\"HandX\"], df[\"HandY\"], linestyle=\"-\", linewidth=1.5, label=\"end-effector trajectory\")\n",
    "\n",
    "# start / end markers\n",
    "ax1.scatter(df[\"HandX\"].iloc[0], df[\"HandY\"].iloc[0], color=\"red\", label=\"start\", zorder=5)\n",
    "ax1.scatter(df[\"HandX\"].iloc[-1], df[\"HandY\"].iloc[-1], color=\"green\", label=\"end\", zorder=5)\n",
    "\n",
    "# リンク長（env から取得できなければ L を半分ずつ使う）\n",
    "try:\n",
    "    l1 = float(getattr(env, \"l1\", (L*0.5)))\n",
    "    l2 = float(getattr(env, \"l2\", (L*0.5)))\n",
    "except Exception:\n",
    "    l1 = L * 0.5\n",
    "    l2 = L * 0.5\n",
    "\n",
    "# 関節位置を計算（theta は deg で保存されている想定）\n",
    "theta1_rad = np.radians(df[\"Theta1 (deg)\"].values)\n",
    "theta2_rad = np.radians(df[\"Theta2 (deg)\"].values)\n",
    "\n",
    "# 第1関節（肘ではなくリンク1の末端）位置\n",
    "x1 = l1 * np.cos(theta1_rad)\n",
    "y1 = l1 * np.sin(theta1_rad)\n",
    "# 手先（リンク2末端）\n",
    "x2 = x1 + l2 * np.cos(theta1_rad + theta2_rad)\n",
    "y2 = y1 + l2 * np.sin(theta1_rad + theta2_rad)\n",
    "\n",
    "# 確認：手先ログと一致するはず\n",
    "# (もし mismatch があれば警告を出す)\n",
    "try:\n",
    "    mismatch = np.nanmax(np.abs(np.column_stack([df[\"HandX\"].values, df[\"HandY\"].values]) - np.column_stack([x2, y2])))\n",
    "    if mismatch > 1e-6:\n",
    "        # print 小さめの警告（実行中に表示）\n",
    "        print(f\"[WARN] forward kinematics and logged hand pos mismatch (max diff={mismatch:.6e})\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# アーム姿勢を複数タイムスタンプで描く\n",
    "N_POSES = min(15, len(df))  # 最大描画姿勢数\n",
    "indices = np.linspace(0, len(df)-1, N_POSES).astype(int)\n",
    "\n",
    "# 薄いグレーで時系列に沿ってアームをプロット（透過）\n",
    "for idx in indices:\n",
    "    xs = [0.0, x1[idx], x2[idx]]\n",
    "    ys = [0.0, y1[idx], y2[idx]]\n",
    "    ax1.plot(xs, ys, linewidth=1, color=(0.5,0.5,0.5,0.25), solid_capstyle='round')  # 太めの線\n",
    "    ax1.scatter([xs[1], xs[2]], [ys[1], ys[2]], s=40, edgecolors='k', facecolors=(0.9,0.9,0.9,0.6), zorder=4)\n",
    "\n",
    "# 強調: start, mid, end の姿勢（色とマーカーで分かりやすく）\n",
    "# start\n",
    "i0 = 0\n",
    "xs0 = [0.0, x1[i0], x2[i0]]\n",
    "ys0 = [0.0, y1[i0], y2[i0]]\n",
    "ax1.plot(xs0, ys0, linewidth=2, color='tab:red', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xs0[1], xs0[2]], [ys0[1], ys0[2]], s=80, color='tab:red', edgecolors='k', zorder=7)\n",
    "ax1.scatter(0.0, 0.0, s=100, color='black', zorder=8)  # shoulder\n",
    "\n",
    "\n",
    "# end\n",
    "ie = len(df)-1\n",
    "xse = [0.0, x1[ie], x2[ie]]\n",
    "yse = [0.0, y1[ie], y2[ie]]\n",
    "ax1.plot(xse, yse, linewidth=2, color='tab:green', solid_capstyle='round', zorder=6)\n",
    "ax1.scatter([xse[1], xse[2]], [yse[1], yse[2]], s=100, color='tab:green', edgecolors='k', zorder=7)\n",
    "\n",
    "# # 関節角度の数値も近傍に表示（start と end のみ、必要なら増やせます）\n",
    "# ang_off = 0.03 * (l1 + l2)  # 表示オフセット\n",
    "# ax1.text(xs0[1]+ang_off, ys0[1]+ang_off, f\"{np.degrees(theta1_rad[i0]):.1f}°\", color='tab:red', fontsize=9)\n",
    "# ax1.text(xs0[2]+ang_off, ys0[2]+ang_off, f\"{np.degrees(theta2_rad[i0]):.1f}°\", color='tab:red', fontsize=9)\n",
    "# ax1.text(xse[1]+ang_off, yse[1]+ang_off, f\"{np.degrees(theta1_rad[ie]):.1f}°\", color='tab:green', fontsize=9)\n",
    "# ax1.text(xse[2]+ang_off, yse[2]+ang_off, f\"{np.degrees(theta2_rad[ie]):.1f}°\", color='tab:green', fontsize=9)\n",
    "\n",
    "# 装飾\n",
    "ax1.set_xlabel(\"X [m]\"); ax1.set_ylabel(\"Y [m]\")\n",
    "# ax1.set_title(\"End-effector Trajectory with Arm Poses (shoulder at (0,0))\")\n",
    "ax1.grid(True)\n",
    "ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# 自動スケール（アーム長に沿ったマージンを付ける）\n",
    "reach = l1 + l2\n",
    "ax1.set_xlim(-reach-0.1*reach, reach+0.1*reach)\n",
    "ax1.set_ylim(-0.1*reach, reach+0.6*reach)  # y 上方向に少し余裕\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_filename}_trajectory_with_arms.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 関節動的変数（Theta, Vel, Acc, Jerk）: 各プロットに Joint1/Joint2 を表示\n",
    "fig2, axes = plt.subplots(4, 1, figsize=(8, 12), sharex=True)\n",
    "axes[0].plot(df[\"Time\"], df[\"Theta1 (deg)\"], label=\"Theta1\"); axes[0].plot(df[\"Time\"], df[\"Theta2 (deg)\"], linestyle=\"--\", label=\"Theta2\")\n",
    "axes[0].set_ylabel(\"Theta (deg)\"); axes[0].legend(); axes[0].grid()\n",
    "axes[0].set_ylim(0, 180)\n",
    "\n",
    "axes[1].plot(df[\"Time\"], df[\"Theta1_vel (deg/s)\"], label=\"Vel1\"); axes[1].plot(df[\"Time\"], df[\"Theta2_vel (deg/s)\"], linestyle=\"--\", label=\"Vel2\")\n",
    "axes[1].set_ylabel(\"Velocity (deg/s)\"); axes[1].legend(); axes[1].grid()\n",
    "\n",
    "axes[2].plot(df[\"Time\"], df[\"Theta1_acc (deg/s^2)\"], label=\"Acc1\"); axes[2].plot(df[\"Time\"], df[\"Theta2_acc (deg/s^2)\"], linestyle=\"--\", label=\"Acc2\")\n",
    "axes[2].set_ylabel(\"Acceleration (deg/s^2)\"); axes[2].legend(); axes[2].grid()\n",
    "\n",
    "axes[3].plot(df[\"Time\"], df[\"Theta1_jer (deg/s^3)\"], label=\"Jerk1\"); axes[3].plot(df[\"Time\"], df[\"Theta2_jer (deg/s^3)\"], linestyle=\"--\", label=\"Jerk2\")\n",
    "axes[3].set_ylabel(\"Jerk (deg/s^3)\"); axes[3].set_xlabel(\"Time (s)\"); axes[3].legend(); axes[3].grid()\n",
    "\n",
    "png_dynamics = os.path.join(output_dir, \"best_result_2joint_joint_dynamics.png\")\n",
    "plt.tight_layout(); plt.savefig(png_dynamics, dpi=300); plt.show()\n",
    "print(f\"✅ 関節動的変数プロット保存: {png_dynamics}\")\n",
    "\n",
    "# 手先ノルム（速度・加速度・躍度）\n",
    "fig3, ax3 = plt.subplots(3,1, figsize=(8,9), sharex=True)\n",
    "ax3[0].plot(df[\"Time\"], df[\"HandSpeed (m/s)\"], label=\"||v||\"); ax3[0].set_ylabel(\"Speed (m/s)\"); ax3[0].legend(); ax3[0].grid()\n",
    "ax3[1].plot(df[\"Time\"], df[\"HandAcc (m/s^2)\"], label=\"||a||\"); ax3[1].set_ylabel(\"Acc (m/s^2)\"); ax3[1].legend(); ax3[1].grid()\n",
    "ax3[2].plot(df[\"Time\"], df[\"HandJerk (m/s^3)\"], label=\"||jerk||\"); ax3[2].set_ylabel(\"Jerk (m/s^3)\"); ax3[2].set_xlabel(\"Time (s)\"); ax3[2].legend(); ax3[2].grid()\n",
    "\n",
    "png_handnorms = os.path.join(output_dir, \"best_result_2joint_hand_norms.png\")\n",
    "plt.tight_layout(); plt.savefig(png_handnorms, dpi=300); plt.show()\n",
    "print(f\"✅ 手先ノルムプロット保存: {png_handnorms}\")\n",
    "\n",
    "# 補助: action の遷移と same フラグ\n",
    "fig4, ax4 = plt.subplots(2,1, figsize=(8,6), sharex=True)\n",
    "ax4[0].plot(df[\"Time\"], df[\"Action1 (deg/s)\"], label=\"Action1\"); ax4[0].plot(df[\"Time\"], df[\"Action2 (deg/s)\"], linestyle=\"--\", label=\"Action2\")\n",
    "ax4[0].set_ylabel(\"Action (deg/s)\"); ax4[0].legend(); ax4[0].grid()\n",
    "ax4[1].plot(df[\"Time\"], df[\"Action_same\"].astype(int), label=\"Action_same\")\n",
    "ax4[1].set_ylabel(\"Action_same\"); ax4[1].set_xlabel(\"Time (s)\"); ax4[1].grid()\n",
    "png_actions = os.path.join(output_dir, \"best_result_2joint_actions.png\")\n",
    "plt.tight_layout(); plt.savefig(png_actions, dpi=300); plt.show()\n",
    "print(f\"✅ action プロット保存: {png_actions}\")\n",
    "\n",
    "# ===== 完了メッセージ =====\n",
    "print(\"=== 実行完了: 2関節対応の評価ログ & プロットを保存しました ===\")\n",
    "print(f\"出力フォルダ: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エピソードごとの様々な報酬の推移などの多指標をプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== episode_full_metrics.csv を読み込んで多指標プロットを自動生成するスクリプト ======\n",
    "# 最新ログフォルダ検出 -> CSV 読込 -> グループ別プロットと個別プロットを保存。\n",
    "# 追加機能: 各グループプロットについて、移動平均(rolling mean)版も出力。\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "# ===== 設定: 移動平均窓幅 =====\n",
    "MOVING_AVG_WINDOW = 10  # ここを変えれば移動平均の平滑化窓を調整できます（エピソード数に応じて適宜設定）\n",
    "\n",
    "# ===== 最新ログフォルダを取得 =====\n",
    "logs_dir = \"./logs\"\n",
    "log_folders = [os.path.join(logs_dir, d) for d in os.listdir(logs_dir) if os.path.isdir(os.path.join(logs_dir, d))]\n",
    "if not log_folders:\n",
    "    raise FileNotFoundError(\"`./logs` 内にフォルダが見つかりません。学習ログがあるディレクトリを確認してください。\")\n",
    "latest_log_folder = max(log_folders, key=os.path.getmtime)\n",
    "print(f\"[INFO] 最新ログフォルダ: {latest_log_folder}\")\n",
    "\n",
    "# ===== CSV パス =====\n",
    "csv_filename = \"episode_full_metrics.csv\"\n",
    "csv_path = os.path.join(latest_log_folder, csv_filename)\n",
    "if not os.path.exists(csv_path):\n",
    "    # 場合によってはファイル名が違う可能性があるので近い名前を探す\n",
    "    candidates = glob.glob(os.path.join(latest_log_folder, \"*episode*.csv\")) + glob.glob(os.path.join(latest_log_folder, \"*full*.csv\"))\n",
    "    if candidates:\n",
    "        csv_path = candidates[-1]\n",
    "        print(f\"[WARN] 指定ファイルが見つかりませんでした。代わりに候補を使用します: {csv_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{csv_filename} が見つかりません: {latest_log_folder}\")\n",
    "\n",
    "# csv_path =\"logs/2025_10_21_22_45_48/episode_full_metrics.csv\"\n",
    "# ===== CSV 読み込み（ヘッダが1行目にあることを想定） =====\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"[INFO] CSVを読み込みました: {csv_path}\")\n",
    "except Exception as e:\n",
    "    # 場合によっては1行目がコメントなので skiprows=1 を試す\n",
    "    print(\"[WARN] 直接読み込みに失敗しました。skiprows=1 を試します。エラー:\", e)\n",
    "    df = pd.read_csv(csv_path, skiprows=1)\n",
    "    print(f\"[INFO] CSVを読み込みました（skiprows=1）: {csv_path}\")\n",
    "\n",
    "# ===== 列名と表示ラベル・単位のマッピング =====\n",
    "label_unit_map = {\n",
    "    \"episode\":                (\"Episode\", \"\"),\n",
    "    \"episode_length\":         (\"Episode length\", \"steps\"),\n",
    "    \"total_reward\":           (\"Total reward\", \"arb.\"),    # 任意単位（報酬は設計次第）\n",
    "    \"sum_reward_dist_step\":   (\"Sum distance shaping reward\", \"arb.\"),\n",
    "    \"sum_reward_jerk_step\":   (\"Sum jerk shaping reward\", \"arb.\"),\n",
    "    \"sum_reward_time_step\":   (\"Sum time shaping reward\", \"arb.\"),\n",
    "    \"sum_terminal_jerk_penalty\": (\"Terminal jerk penalty\", \"arb.\"),\n",
    "    \"sum_terminal_vel_penalty\":  (\"Terminal velocity penalty\", \"arb.\"),\n",
    "    \"sum_time_bonus\":         (\"Terminal time bonus\", \"arb.\"),\n",
    "    \"jerk_sum\":               (\"Jerk sum\", \"rad^2/s^5\"),\n",
    "    \"success\":                (\"Success\", \"0/1\"),\n",
    "    \"actor_loss\":             (\"Actor loss\", \"arb.\"),\n",
    "    \"critic_loss\":            (\"Critic loss\", \"arb.\"),\n",
    "    \"ent_coef\":               (\"Entropy coeff\", \"arb.\"),\n",
    "    \"ent_coef_loss\":          (\"Ent coef loss\", \"arb.\"),\n",
    "    \"avg_q\":                  (\"Average Q\", \"arb.\"),\n",
    "    \"running_mean_total_reward_100\": (\"Running mean total reward (100)\", \"arb.\"),\n",
    "    \"running_success_rate_100\": (\"Running success rate (100)\", \"ratio\"),\n",
    "    \"episode_wall_time\":      (\"Episode wall time\", \"s\"),\n",
    "    \"sigma_T\":  (\"sigma_T\", \"\"),\n",
    "}\n",
    "\n",
    "# ===== CSV 中の列名（利用可能なもの）を表示 =====\n",
    "print(\"[INFO] CSV の列:\")\n",
    "for c in df.columns:\n",
    "    print(\"  \", c)\n",
    "\n",
    "# ===== 基本 X 軸: エピソード番号取得 =====\n",
    "if \"episode\" in df.columns:\n",
    "    episodes = df[\"episode\"].values\n",
    "else:\n",
    "    episodes = np.arange(1, len(df) + 1)\n",
    "    print(\"[WARN] 'episode' 列が見つかりません。インデックスをエピソード番号として使用します。\")\n",
    "\n",
    "# ===== グループ定義：同じ図にまとめたい列をグループ化 =====\n",
    "groups = {\n",
    "    \"reward_components\": [\n",
    "        \"total_reward\",\n",
    "        \"sum_terminal_jerk_penalty\",\n",
    "        \"sum_terminal_vel_penalty\",\n",
    "        \"sum_time_bonus\",\n",
    "        \"sum_reward_dist_step\",\n",
    "        \"sum_reward_jerk_step\",\n",
    "        \"sum_reward_time_step\",\n",
    "    ],\n",
    "    \"reward_goal_components\": [\n",
    "        \"sum_terminal_jerk_penalty\",\n",
    "        \"sum_terminal_vel_penalty\",\n",
    "        \"sum_time_bonus\"\n",
    "    ],\n",
    "    \"reward__step_components\": [\n",
    "        \"sum_reward_dist_step\",\n",
    "        \"sum_reward_jerk_step\",\n",
    "        \"sum_reward_time_step\",\n",
    "    ],\n",
    "    \"dynamics\": [\n",
    "        \"jerk_sum\",\n",
    "        \"episode_length\",\n",
    "        \"episode_wall_time\"\n",
    "    ],\n",
    "    \"learning_metrics\": [\n",
    "        \"actor_loss\",\n",
    "        \"critic_loss\",\n",
    "        \"ent_coef\",\n",
    "        \"ent_coef_loss\",\n",
    "        \"avg_q\"\n",
    "    ],\n",
    "    \"running_stats\": [\n",
    "        \"running_mean_total_reward_100\",\n",
    "        \"running_success_rate_100\",\n",
    "        \"success\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ===== 保存ディレクトリ（最新ログフォルダ内に出す） =====\n",
    "out_dir = latest_log_folder\n",
    "plots_dir = os.path.join(out_dir, \"plots\")\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# ===== 描画ヘルパー関数 =====\n",
    "def safe_get_col(df, col):\n",
    "    return df[col].values if col in df.columns else None\n",
    "\n",
    "def plot_series(x, y,xlabel,ylabel, savepath, legend=None, ylim=None):\n",
    "    \"\"\"単一系列または複数系列を描画して保存するヘルパー\"\"\"\n",
    "    if y is None:\n",
    "        print(f\"[SKIP] {title} : データが存在しません。\")\n",
    "        return\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    if isinstance(y, dict):\n",
    "        # 複数系列\n",
    "        for name, arr in y.items():\n",
    "            plt.plot(x, arr, label=name)\n",
    "    else:\n",
    "        plt.plot(x, y)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if legend is not None:\n",
    "        plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[SAVED] {savepath}\")\n",
    "\n",
    "# ===== 便利: 移動平均を作る関数 =====\n",
    "def moving_average(arr, window):\n",
    "    \"\"\"NaN を許容しつつ単純移動平均を返す。pandas を使って簡潔に処理。\"\"\"\n",
    "    if arr is None:\n",
    "        return None\n",
    "    # pandas.Series を使って rolling(mean) を計算（min_periods=1 で立ち上がりも値を返す）\n",
    "    return pd.Series(arr).rolling(window=window, min_periods=1).mean().to_numpy()\n",
    "\n",
    "# ===== 1) グループごとのまとめプロット（同じスケールで重ねて見たいもの） =====\n",
    "for gname, cols in groups.items():\n",
    "    # 実際に存在する列だけ取り出す\n",
    "    available = [c for c in cols if c in df.columns]\n",
    "    if not available:\n",
    "        print(f\"[SKIP] グループ '{gname}' にプロット可能な列がありません。\")\n",
    "        continue\n",
    "\n",
    "    # --- 元データでのプロット（元からある動作） ---\n",
    "    series_dict = {}\n",
    "    legend_labels = []\n",
    "    for c in available:\n",
    "        arr = df[c].values\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        label_full = f\"{label} [{unit}]\" if unit else label\n",
    "        series_dict[label_full] = arr\n",
    "        legend_labels.append(label_full)\n",
    "\n",
    "    title = f\"{gname} (group) over episodes\"\n",
    "    savepath = os.path.join(plots_dir, f\"group_{gname}.png\")\n",
    "    plot_series(episodes, series_dict, \"Episode\", \"Value\", savepath, legend=True)\n",
    "\n",
    "    # --- 追加: 移動平均版のプロット（同じ形式） ---\n",
    "    # 各系列に対して移動平均を取り、新しい辞書を作る\n",
    "    ma_series_dict = {}\n",
    "    for c in available:\n",
    "        arr = df[c].values\n",
    "        arr_ma = moving_average(arr, MOVING_AVG_WINDOW)\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        label_full = f\"{label} [{unit}]\" if unit else label\n",
    "        ma_series_dict[label_full] = arr_ma\n",
    "\n",
    "    title_ma = f\"{gname} (group) - moving avg (window={MOVING_AVG_WINDOW})\"\n",
    "    savepath_ma = os.path.join(plots_dir, f\"group_{gname}_ma_w{MOVING_AVG_WINDOW}.png\")\n",
    "    plot_series(episodes, ma_series_dict, \"Episode\", f\"Mean Value (window={MOVING_AVG_WINDOW})\", savepath_ma, legend=None)\n",
    "\n",
    "# ===== 2) 各列ごとの個別プロット（ラベルと単位を軸に反映） =====\n",
    "for col in df.columns:\n",
    "    if col == \"episode\":\n",
    "        continue\n",
    "    arr = safe_get_col(df, col)\n",
    "    if arr is None:\n",
    "        continue\n",
    "\n",
    "    label, unit = label_unit_map.get(col, (col, \"\"))\n",
    "    ylabel = f\"{label} [{unit}]\" if unit else label\n",
    "    title = f\"{label} over episodes\"\n",
    "    savepath = os.path.join(plots_dir, f\"episode_{col}.png\")\n",
    "\n",
    "    # success / running_success_rate の場合、0-1 の比率なので ylim を合せる\n",
    "    ylim = None\n",
    "    if col in (\"success\", \"running_success_rate_100\"):\n",
    "        ylim = (-0.05, 1.05)\n",
    "\n",
    "    plot_series(episodes, arr,\"Episode\", ylabel, savepath, legend=None, ylim=ylim)\n",
    "\n",
    "    # 追加: 個別プロットの移動平均（必要なら出力。元データがある場合のみ）\n",
    "    arr_ma = moving_average(arr, MOVING_AVG_WINDOW)\n",
    "    savepath_ma = os.path.join(plots_dir, f\"episode_{col}_ma_w{MOVING_AVG_WINDOW}.png\")\n",
    "    title_ma = f\"{label} - moving avg (window={MOVING_AVG_WINDOW})\"\n",
    "    plot_series(episodes, arr_ma,\"Episode\", f\"{label} (moving avg)\", savepath_ma, legend=None, ylim=ylim)\n",
    "\n",
    "# ===== 3) 全報酬成分だけを重ねたプロット（見やすく色分け） =====\n",
    "reward_cols = [c for c in groups[\"reward_components\"] if c in df.columns]\n",
    "if reward_cols:\n",
    "    series = {}\n",
    "    for c in reward_cols:\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        series[f\"{label} [{unit}]\"] = df[c].values\n",
    "    savepath = os.path.join(plots_dir, \"all_reward_components_overlay.png\")\n",
    "    plot_series(episodes, series,  \"Episode\", \"Reward (arb.)\", savepath, legend=None)\n",
    "\n",
    "    # 全報酬成分の移動平均オーバーレイ\n",
    "    series_ma = {}\n",
    "    for c in reward_cols:\n",
    "        label, unit = label_unit_map.get(c, (c, \"\"))\n",
    "        series_ma[f\"{label} [{unit}]\"] = moving_average(df[c].values, MOVING_AVG_WINDOW)\n",
    "    savepath_ma = os.path.join(plots_dir, f\"all_reward_components_overlay_ma_w{MOVING_AVG_WINDOW}.png\")\n",
    "    plot_series(episodes, series_ma, \"Episode\", \"Reward (moving avg)\", savepath_ma, legend=True)\n",
    "\n",
    "print(\"[DONE] プロット作成完了。結果はフォルダに保存されています:\", plots_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
